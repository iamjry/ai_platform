#!/bin/bash

echo "ğŸ”§ è‡ªå‹•å‰µå»ºæ‰€æœ‰æœå‹™ä»£ç¢¼..."

# ============================================
# 1. MCP Server
# ============================================
echo "å‰µå»º MCP Server..."

# Dockerfile
cat > services/mcp-server/Dockerfile << 'DOCKERFILE'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
DOCKERFILE

# requirements.txt
cat > services/mcp-server/requirements.txt << 'REQUIREMENTS'
fastapi==0.110.0
uvicorn[standard]==0.29.0
pydantic==2.7.0
pydantic-settings==2.2.1
asyncpg==0.29.0
qdrant-client==1.9.0
redis==5.0.3
aiohttp==3.9.5
python-multipart==0.0.9
REQUIREMENTS

# main.py
cat > services/mcp-server/main.py << 'MAINPY'
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import asyncpg
from qdrant_client import QdrantClient
import redis.asyncio as redis
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="MCP Server", version="1.0.0")

# å…¨å±€è®Šé‡
db_pool = None
vector_db = None
redis_client = None

class SearchRequest(BaseModel):
    query: str
    collection: str = "documents"
    limit: int = 5

class SearchResult(BaseModel):
    content: str
    score: float
    metadata: Dict

class ToolResponse(BaseModel):
    tools: List[Dict]

@app.on_event("startup")
async def startup():
    global db_pool, vector_db, redis_client
    
    try:
        # åˆå§‹åŒ–PostgreSQL
        postgres_url = os.getenv("POSTGRES_URL", "postgresql://admin:password@postgres:5432/ai_platform")
        db_pool = await asyncpg.create_pool(postgres_url, min_size=2, max_size=10)
        logger.info("âœ“ PostgreSQL connected")
        
        # åˆå§‹åŒ–Qdrant
        qdrant_url = os.getenv("QDRANT_URL", "http://qdrant:6333")
        vector_db = QdrantClient(url=qdrant_url)
        logger.info("âœ“ Qdrant connected")
        
        # åˆå§‹åŒ–Redis
        redis_url = os.getenv("REDIS_URL", "redis://:password@redis:6379")
        redis_client = await redis.from_url(redis_url, decode_responses=True)
        logger.info("âœ“ Redis connected")
        
    except Exception as e:
        logger.error(f"Startup error: {e}")

@app.on_event("shutdown")
async def shutdown():
    if db_pool:
        await db_pool.close()
    if redis_client:
        await redis_client.close()

@app.get("/health")
async def health_check():
    health_status = {
        "status": "healthy",
        "services": {}
    }
    
    # æª¢æŸ¥PostgreSQL
    try:
        if db_pool:
            async with db_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            health_status["services"]["postgres"] = "connected"
        else:
            health_status["services"]["postgres"] = "not initialized"
    except Exception as e:
        health_status["services"]["postgres"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # æª¢æŸ¥Redis
    try:
        if redis_client:
            await redis_client.ping()
            health_status["services"]["redis"] = "connected"
        else:
            health_status["services"]["redis"] = "not initialized"
    except Exception as e:
        health_status["services"]["redis"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    return health_status

@app.get("/tools/list", response_model=ToolResponse)
async def list_tools():
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return {
        "tools": [
            {
                "name": "search_knowledge_base",
                "description": "æœå°‹ä¼æ¥­çŸ¥è­˜åº«",
                "parameters": {
                    "query": "string",
                    "collection": "string",
                    "limit": "integer"
                }
            },
            {
                "name": "query_database",
                "description": "æŸ¥è©¢ä¼æ¥­è³‡æ–™åº«",
                "parameters": {
                    "query_type": "string",
                    "parameters": "object"
                }
            },
            {
                "name": "get_document",
                "description": "ç²å–æ–‡ä»¶å…§å®¹",
                "parameters": {
                    "document_id": "string"
                }
            }
        ]
    }

@app.post("/tools/search", response_model=List[Dict])
async def search_knowledge_base(request: SearchRequest):
    """æœå°‹çŸ¥è­˜åº«"""
    try:
        # æª¢æŸ¥å¿«å–
        cache_key = f"search:{request.collection}:{request.query}"
        cached = await redis_client.get(cache_key)
        
        if cached:
            import json
            logger.info(f"Cache hit for query: {request.query}")
            return json.loads(cached)
        
        # å¾è³‡æ–™åº«æœå°‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        results = []
        if db_pool:
            async with db_pool.acquire() as conn:
                rows = await conn.fetch(
                    "SELECT title, content, metadata FROM documents WHERE content ILIKE $1 LIMIT $2",
                    f"%{request.query}%",
                    request.limit
                )
                
                for row in rows:
                    results.append({
                        "content": row["content"],
                        "title": row["title"],
                        "score": 0.8,
                        "metadata": row["metadata"]
                    })
        
        # å¿«å–çµæœ
        if results:
            import json
            await redis_client.setex(cache_key, 300, json.dumps(results))
        
        return results
        
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/resources/document/{doc_id}")
async def get_document(doc_id: int):
    """ç²å–æ–‡ä»¶"""
    try:
        if not db_pool:
            raise HTTPException(status_code=503, detail="Database not available")
            
        async with db_pool.acquire() as conn:
            doc = await conn.fetchrow(
                "SELECT * FROM documents WHERE id = $1", 
                doc_id
            )
            
            if not doc:
                raise HTTPException(status_code=404, detail="Document not found")
            
            return {
                "id": doc["id"],
                "title": doc["title"],
                "content": doc["content"],
                "metadata": doc["metadata"],
                "created_at": doc["created_at"].isoformat() if doc["created_at"] else None
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get document error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {
        "service": "MCP Server",
        "version": "1.0.0",
        "status": "running"
    }
MAINPY

echo "âœ“ MCP Server å‰µå»ºå®Œæˆ"

# ============================================
# 2. Agent Service
# ============================================
echo "å‰µå»º Agent Service..."

# Dockerfile
cat > services/agent-service/Dockerfile << 'DOCKERFILE'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
DOCKERFILE

# requirements.txt
cat > services/agent-service/requirements.txt << 'REQUIREMENTS'
fastapi==0.110.0
uvicorn[standard]==0.29.0
langchain==0.2.0
langchain-openai==0.1.7
langchain-community==0.2.0
aiohttp==3.9.5
redis==5.0.3
pydantic==2.7.0
httpx==0.27.0
REQUIREMENTS

# main.py
cat > services/agent-service/main.py << 'MAINPY'
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import aiohttp
import os
import logging
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Agent Service", version="1.0.0")

# é…ç½®
LLM_PROXY_URL = os.getenv("LLM_PROXY_URL", "http://litellm:4000")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")

class AgentRequest(BaseModel):
    task: str
    context: Optional[Dict] = None
    agent_type: str = "general"

class AgentResponse(BaseModel):
    result: str
    steps: List[Dict]
    metadata: Dict

class ChatRequest(BaseModel):
    message: str
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7

class ChatResponse(BaseModel):
    response: str
    model: str

@app.get("/health")
async def health_check():
    health_status = {
        "status": "healthy",
        "services": {}
    }
    
    # æª¢æŸ¥LLMæœå‹™
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{LLM_PROXY_URL}/health", timeout=5.0)
            if resp.status_code == 200:
                health_status["services"]["llm"] = "connected"
            else:
                health_status["services"]["llm"] = "unavailable"
                health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["llm"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # æª¢æŸ¥MCPæœå‹™
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{MCP_SERVER_URL}/health", timeout=5.0)
            if resp.status_code == 200:
                health_status["services"]["mcp"] = "connected"
            else:
                health_status["services"]["mcp"] = "unavailable"
                health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["mcp"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    return health_status

@app.post("/agent/execute", response_model=AgentResponse)
async def execute_agent(request: AgentRequest):
    """åŸ·è¡ŒAgentä»»å‹™"""
    try:
        steps = []
        
        # Step 1: å‘¼å«MCPç²å–å·¥å…·
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(f"{MCP_SERVER_URL}/tools/list", timeout=10.0)
                tools = resp.json()
                steps.append({
                    "step": "fetch_tools",
                    "result": f"Found {len(tools.get('tools', []))} tools",
                    "status": "success"
                })
        except Exception as e:
            steps.append({
                "step": "fetch_tools",
                "result": f"Failed: {str(e)}",
                "status": "failed"
            })
        
        # Step 2: å‘¼å«LLMè™•ç†ä»»å‹™
        try:
            async with httpx.AsyncClient() as client:
                llm_response = await client.post(
                    f"{LLM_PROXY_URL}/v1/chat/completions",
                    json={
                        "model": "gpt-3.5-turbo",
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ä¼æ¥­AIåŠ©æ‰‹ï¼Œå¹«åŠ©ç”¨æˆ¶å®Œæˆå„ç¨®ä»»å‹™ã€‚"},
                            {"role": "user", "content": request.task}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1000
                    },
                    timeout=30.0
                )
                
                llm_data = llm_response.json()
                result = llm_data["choices"][0]["message"]["content"]
                
                steps.append({
                    "step": "llm_processing",
                    "result": "Task processed successfully",
                    "status": "success"
                })
                
                return AgentResponse(
                    result=result,
                    steps=steps,
                    metadata={
                        "agent_type": request.agent_type,
                        "model_used": "gpt-3.5-turbo",
                        "tokens_used": llm_data.get("usage", {}).get("total_tokens", 0)
                    }
                )
                
        except Exception as e:
            logger.error(f"LLM processing error: {e}")
            steps.append({
                "step": "llm_processing",
                "result": f"Failed: {str(e)}",
                "status": "failed"
            })
            
            return AgentResponse(
                result=f"ä»»å‹™è™•ç†å¤±æ•—: {str(e)}",
                steps=steps,
                metadata={
                    "agent_type": request.agent_type,
                    "error": str(e)
                }
            )
        
    except Exception as e:
        logger.error(f"Agent execution error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/agent/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ç°¡å–®çš„èŠå¤©ä»‹é¢"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{LLM_PROXY_URL}/v1/chat/completions",
                json={
                    "model": request.model,
                    "messages": [
                        {"role": "user", "content": request.message}
                    ],
                    "temperature": request.temperature
                },
                timeout=30.0
            )
            
            data = response.json()
            
            return ChatResponse(
                response=data["choices"][0]["message"]["content"],
                model=request.model
            )
            
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LLMæœå‹™è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦")
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤±æ•—: {str(e)}")

@app.get("/")
async def root():
    return {
        "service": "Agent Service",
        "version": "1.0.0",
        "status": "running",
        "llm_proxy": LLM_PROXY_URL,
        "mcp_server": MCP_SERVER_URL
    }
MAINPY

echo "âœ“ Agent Service å‰µå»ºå®Œæˆ"

# ============================================
# 3. Web UI
# ============================================
echo "å‰µå»º Web UI..."

# Dockerfile
cat > services/web-ui/Dockerfile << 'DOCKERFILE'
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8501/_stcore/health || exit 1

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
DOCKERFILE

# requirements.txt
cat > services/web-ui/requirements.txt << 'REQUIREMENTS'
streamlit==1.33.0
requests==2.31.0
pandas==2.2.1
plotly==5.20.0
REQUIREMENTS

# app.py
cat > services/web-ui/app.py << 'APPPY'
import streamlit as st
import requests
import os
import time

st.set_page_config(
    page_title="ä¼æ¥­AIå¹³å° MVP",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

AGENT_SERVICE_URL = os.getenv("AGENT_SERVICE_URL", "http://agent-service:8000")
LITELLM_URL = os.getenv("LITELLM_URL", "http://litellm:4000")

# è‡ªå®šç¾©CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 1rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .status-healthy {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
    }
    .status-unhealthy {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<p class="main-header">ğŸ¤– ä¼æ¥­AIå…±ç”¨å¹³å° MVP</p>', unsafe_allow_html=True)

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    model_choice = st.selectbox(
        "é¸æ“‡æ¨¡å‹",
        ["gpt-3.5-turbo", "gpt-4", "claude-3-sonnet", "llama3"],
        help="é¸æ“‡è¦ä½¿ç”¨çš„LLMæ¨¡å‹"
    )
    
    temperature = st.slider(
        "Temperature", 
        0.0, 1.0, 0.7,
        help="æ§åˆ¶å›ç­”çš„å‰µé€ æ€§ã€‚è¼ƒé«˜çš„å€¼æœƒç”¢ç”Ÿæ›´å¤šæ¨£åŒ–çš„å›ç­”"
    )
    
    st.divider()
    
    # ç³»çµ±ç‹€æ…‹
    st.header("ğŸ“Š ç³»çµ±ç‹€æ…‹")
    
    with st.spinner("æª¢æŸ¥æœå‹™ç‹€æ…‹..."):
        # æª¢æŸ¥Agentæœå‹™
        try:
            resp = requests.get(f"{AGENT_SERVICE_URL}/health", timeout=3)
            if resp.ok:
                st.success("âœ… Agentæœå‹™æ­£å¸¸")
                health_data = resp.json()
                if "services" in health_data:
                    for service, status in health_data["services"].items():
                        if "connected" in str(status):
                            st.text(f"  â””â”€ {service}: âœ“")
                        else:
                            st.text(f"  â””â”€ {service}: âœ—")
            else:
                st.error("âŒ Agentæœå‹™ç•°å¸¸")
        except Exception as e:
            st.error(f"âŒ Agentæœå‹™é›¢ç·š")
            st.caption(f"éŒ¯èª¤: {str(e)}")
    
    st.divider()
    
    # å¿«é€Ÿæ“ä½œ
    st.header("ğŸš€ å¿«é€Ÿæ“ä½œ")
    if st.button("ğŸ”„ æ¸…é™¤å°è©±è¨˜éŒ„"):
        st.session_state.messages = []
        st.rerun()
    
    if st.button("ğŸ“¥ å°å‡ºå°è©±"):
        if "messages" in st.session_state and st.session_state.messages:
            conversation = "\n\n".join([
                f"{msg['role'].upper()}: {msg['content']}" 
                for msg in st.session_state.messages
            ])
            st.download_button(
                "ä¸‹è¼‰å°è©±è¨˜éŒ„",
                conversation,
                file_name="conversation.txt",
                mime="text/plain"
            )
        else:
            st.info("æš«ç„¡å°è©±è¨˜éŒ„")

# ä¸»è¦å…§å®¹
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ å°è©±", "ğŸ¤– Agentä»»å‹™", "ğŸ“Š ç›£æ§", "â„¹ï¸ é—œæ–¼"])

with tab1:
    st.header("ğŸ’¬ AIå°è©±ä»‹é¢")
    st.caption("èˆ‡AIåŠ©æ‰‹é€²è¡Œè‡ªç„¶èªè¨€å°è©±")
    
    # åˆå§‹åŒ–å°è©±è¨˜éŒ„
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # é¡¯ç¤ºå°è©±è¨˜éŒ„
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # è¼¸å…¥æ¡†
    if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ç”Ÿæˆå›è¦†
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                try:
                    start_time = time.time()
                    
                    response = requests.post(
                        f"{AGENT_SERVICE_URL}/agent/chat",
                        json={
                            "message": prompt,
                            "model": model_choice,
                            "temperature": temperature
                        },
                        timeout=60
                    )
                    
                    elapsed_time = time.time() - start_time
                    
                    if response.ok:
                        result = response.json()
                        answer = result["response"]
                        st.markdown(answer)
                        
                        # é¡¯ç¤ºå…ƒæ•¸æ“š
                        with st.expander("æŸ¥çœ‹è©³ç´°è³‡è¨Š"):
                            st.json({
                                "model": result["model"],
                                "response_time": f"{elapsed_time:.2f}ç§’"
                            })
                        
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": answer
                        })
                    else:
                        error_msg = f"âŒ éŒ¯èª¤: {response.text}"
                        st.error(error_msg)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": error_msg
                        })
                        
                except requests.exceptions.Timeout:
                    error_msg = "â±ï¸ è«‹æ±‚è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
                except Exception as e:
                    error_msg = f"âŒ è«‹æ±‚å¤±æ•—: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })

with tab2:
    st.header("ğŸ¤– Agentä»»å‹™åŸ·è¡Œ")
    st.caption("åŸ·è¡Œè¤‡é›œçš„å¤šæ­¥é©ŸAIä»»å‹™")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        task = st.text_area(
            "æè¿°æ‚¨çš„ä»»å‹™",
            height=150,
            placeholder="ä¾‹å¦‚ï¼šåˆ†ææˆ‘å€‘å…¬å¸çš„å­£åº¦éŠ·å”®æ•¸æ“šä¸¦ç”Ÿæˆå ±å‘Š"
        )
    
    with col2:
        agent_type = st.selectbox(
            "Agenté¡å‹",
            ["general", "research", "analysis"],
            help="é¸æ“‡é©åˆä»»å‹™é¡å‹çš„Agent"
        )
        
        execute_button = st.button("â–¶ï¸ åŸ·è¡Œä»»å‹™", use_container_width=True)
    
    if execute_button and task:
        with st.spinner("åŸ·è¡Œä¸­ï¼Œè«‹ç¨å€™..."):
            try:
                start_time = time.time()
                
                response = requests.post(
                    f"{AGENT_SERVICE_URL}/agent/execute",
                    json={
                        "task": task,
                        "agent_type": agent_type
                    },
                    timeout=120
                )
                
                elapsed_time = time.time() - start_time
                
                if response.ok:
                    result = response.json()
                    
                    st.success(f"âœ… ä»»å‹™å®Œæˆï¼(è€—æ™‚: {elapsed_time:.2f}ç§’)")
                    
                    # é¡¯ç¤ºçµæœ
                    st.subheader("ğŸ“„ åŸ·è¡Œçµæœ")
                    st.write(result["result"])
                    
                    # é¡¯ç¤ºåŸ·è¡Œæ­¥é©Ÿ
                    with st.expander("ğŸ” æŸ¥çœ‹åŸ·è¡Œæ­¥é©Ÿ", expanded=True):
                        for i, step in enumerate(result["steps"], 1):
                            status_icon = "âœ…" if step.get("status") == "success" else "âŒ"
                            st.write(f"{status_icon} **æ­¥é©Ÿ {i}: {step['step']}**")
                            st.caption(step["result"])
                    
                    # é¡¯ç¤ºå…ƒæ•¸æ“š
                    with st.expander("â„¹ï¸ ä»»å‹™è©³æƒ…"):
                        st.json(result["metadata"])
                else:
                    st.error(f"âŒ ä»»å‹™åŸ·è¡Œå¤±æ•—: {response.text}")
                    
            except requests.exceptions.Timeout:
                st.error("â±ï¸ ä»»å‹™åŸ·è¡Œè¶…æ™‚ï¼Œè«‹å˜—è©¦ç°¡åŒ–ä»»å‹™æˆ–ç¨å¾Œå†è©¦")
            except Exception as e:
                st.error(f"âŒ åŸ·è¡Œå¤±æ•—: {str(e)}")
    
    elif execute_button:
        st.warning("âš ï¸ è«‹è¼¸å…¥ä»»å‹™æè¿°")
    
    # ç¯„ä¾‹ä»»å‹™
    st.divider()
    st.subheader("ğŸ’¡ ç¯„ä¾‹ä»»å‹™")
    
    examples = [
        "ç¸½çµä»Šå¤©çš„é‡è¦æ–°è",
        "åˆ†æé›»å•†ç¶²ç«™çš„ç”¨æˆ¶è¡Œç‚ºæ•¸æ“š",
        "ç”Ÿæˆä¸€ä»½å¸‚å ´èª¿ç ”å ±å‘Š",
        "æ¯”è¼ƒä¸‰ç¨®ç”¢å“çš„ç‰¹æ€§å’Œåƒ¹æ ¼"
    ]
    
    cols = st.columns(2)
    for i, example in enumerate(examples):
        with cols[i % 2]:
            if st.button(f"ğŸ“‹ {example}", key=f"example_{i}"):
                st.rerun()

with tab3:
    st.header("ğŸ“Š ç³»çµ±ç›£æ§")
    st.caption("å¯¦æ™‚ç›£æ§ç³»çµ±é‹è¡Œç‹€æ…‹")
    
    col1, col2, col3 = st.columns(3)
    
    # æ¨¡æ“¬æŒ‡æ¨™ï¼ˆå¯¦éš›æ‡‰è©²å¾Prometheusç²å–ï¼‰
    with col1:
        st.metric(
            label="Agentæœå‹™",
            value="é‹è¡Œä¸­",
            delta="æ­£å¸¸"
        )
    
    with col2:
        st.metric(
            label="LLMæœå‹™",
            value="é‹è¡Œä¸­",
            delta="æ­£å¸¸"
        )
    
    with col3:
        st.metric(
            label="MCPæœå‹™",
            value="é‹è¡Œä¸­",
            delta="æ­£å¸¸"
        )
    
    st.divider()
    
    # ç›£æ§éˆæ¥
    st.subheader("ğŸ”— ç›£æ§å·¥å…·")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Grafana å„€è¡¨æ¿**
        - è¨ªå•åœ°å€: http://localhost:3000
        - å¸³è™Ÿ: admin
        - å¯†ç¢¼: admin
        - åŠŸèƒ½: è¦–è¦ºåŒ–ç›£æ§ã€å‘Šè­¦è¨­ç½®
        """)
    
    with col2:
        st.markdown("""
        **Prometheus**
        - è¨ªå•åœ°å€: http://localhost:9090
        - åŠŸèƒ½: æŒ‡æ¨™æŸ¥è©¢ã€æ™‚é–“åºåˆ—åˆ†æ
        """)
    
    st.info("ğŸ’¡ æç¤º: å®Œæ•´çš„ç›£æ§åŠŸèƒ½è«‹è¨ªå• Grafana å„€è¡¨æ¿")

with tab4:
    st.header("â„¹ï¸ é—œæ–¼æœ¬ç³»çµ±")
    
    st.markdown("""
    ### ğŸš€ ä¼æ¥­AIå…±ç”¨å¹³å° MVP
    
    é€™æ˜¯ä¸€å€‹åŸºæ–¼é–‹æºæŠ€è¡“æ§‹å»ºçš„ä¼æ¥­ç´šAIå¹³å°ï¼Œæä¾›çµ±ä¸€çš„LLMæœå‹™ä»‹é¢ã€‚
    
    #### æ ¸å¿ƒç‰¹æ€§
    - ğŸ”„ **æ··åˆéƒ¨ç½²**: æ”¯æ´é›²ç«¯å’Œåœ°ç«¯LLM
    - ğŸ¤– **Agentæ¡†æ¶**: åŸºæ–¼MCPå’ŒA2Aå”è­°
    - ğŸ“Š **å®Œæ•´ç›£æ§**: Prometheus + Grafana
    - ğŸ”’ **å®‰å…¨å¯é **: å¤šå±¤å®‰å…¨é˜²è­·
    
    #### æŠ€è¡“æ¶æ§‹
    - **å‰ç«¯**: Streamlit
    - **å¾Œç«¯**: FastAPI
    - **LLMé–˜é“**: LiteLLM
    - **æœ¬åœ°æ¨ç†**: Ollama
    - **è³‡æ–™åº«**: PostgreSQL, Redis, Qdrant
    - **ç›£æ§**: Prometheus, Grafana
    
    #### æ”¯æ´çš„æ¨¡å‹
    - OpenAI GPTç³»åˆ—
    - Anthropic Claudeç³»åˆ—
    - æœ¬åœ°Llamaæ¨¡å‹
    - å…¶ä»–é–‹æºæ¨¡å‹
    
    #### ä½¿ç”¨å»ºè­°
    1. é¦–æ¬¡ä½¿ç”¨å»ºè­°é¸æ“‡ gpt-3.5-turbo å¿«é€Ÿæ¸¬è©¦
    2. è¤‡é›œä»»å‹™å¯é¸æ“‡ gpt-4 æˆ– claude-3-opus
    3. æ³¨é‡éš±ç§å¯ä½¿ç”¨æœ¬åœ° llama3 æ¨¡å‹
    
    #### ç‰ˆæœ¬è³‡è¨Š
    - ç‰ˆæœ¬: 1.0.0 (MVP)
    - æ›´æ–°æ—¥æœŸ: 2024-10-15
    - æˆæ¬Š: MIT License
    """)
    
    st.divider()
    
    st.markdown("""
    ### ğŸ“ æŠ€è¡“æ”¯æ´
    
    å¦‚é‡å•é¡Œè«‹æŸ¥çœ‹ï¼š
    - ğŸ“– æ–‡æª”: æŸ¥çœ‹ `scripts/README.md`
    - ğŸ”§ æ•…éšœæ’æŸ¥: é‹è¡Œ `./scripts/troubleshoot.sh`
    - ğŸ“ æ—¥èªŒåˆ†æ: é‹è¡Œ `./scripts/analyze-logs.sh`
    """)
APPPY

echo "âœ“ Web UI å‰µå»ºå®Œæˆ"

echo ""
echo "ğŸ‰ æ‰€æœ‰æœå‹™ä»£ç¢¼å‰µå»ºå®Œæˆï¼"
echo ""
echo "æ–‡ä»¶æ¸…å–®:"
echo "  services/"
echo "  â”œâ”€â”€ mcp-server/"
echo "  â”‚   â”œâ”€â”€ Dockerfile"
echo "  â”‚   â”œâ”€â”€ requirements.txt"
echo "  â”‚   â””â”€â”€ main.py"
echo "  â”œâ”€â”€ agent-service/"
echo "  â”‚   â”œâ”€â”€ Dockerfile"
echo "  â”‚   â”œâ”€â”€ requirements.txt"
echo "  â”‚   â””â”€â”€ main.py"
echo "  â””â”€â”€ web-ui/"
echo "      â”œâ”€â”€ Dockerfile"
echo "      â”œâ”€â”€ requirements.txt"
echo "      â””â”€â”€ app.py"
echo ""
