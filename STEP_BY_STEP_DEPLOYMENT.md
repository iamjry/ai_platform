# AI Platform Production éƒ¨ç½²æŒ‡å— - Step by Step

**ç›®æ¨™ç’°å¢ƒ:** Red Hat Enterprise Linux 9.4 + 2x NVIDIA H100 GPU
**é è¨ˆéƒ¨ç½²æ™‚é–“:** 45-60 åˆ†é˜
**ç‰ˆæœ¬:** 2.0.0

---

## ç›®éŒ„

1. [å‰ç½®æº–å‚™](#æ­¥é©Ÿ-1-å‰ç½®æº–å‚™)
2. [ç³»çµ±ç’°å¢ƒæª¢æŸ¥](#æ­¥é©Ÿ-2-ç³»çµ±ç’°å¢ƒæª¢æŸ¥)
3. [ä¸Šå‚³éƒ¨ç½²æ–‡ä»¶](#æ­¥é©Ÿ-3-ä¸Šå‚³éƒ¨ç½²æ–‡ä»¶)
4. [å®‰è£ NVIDIA é©…å‹•](#æ­¥é©Ÿ-4-å®‰è£-nvidia-é©…å‹•)
5. [å®‰è£ Docker](#æ­¥é©Ÿ-5-å®‰è£-docker)
6. [å®‰è£ NVIDIA Container Toolkit](#æ­¥é©Ÿ-6-å®‰è£-nvidia-container-toolkit)
7. [é…ç½®ç’°å¢ƒè®Šæ•¸](#æ­¥é©Ÿ-7-é…ç½®ç’°å¢ƒè®Šæ•¸)
8. [æº–å‚™ SSL æ†‘è­‰](#æ­¥é©Ÿ-8-æº–å‚™-ssl-æ†‘è­‰)
9. [åŸ·è¡Œè‡ªå‹•éƒ¨ç½²è…³æœ¬](#æ­¥é©Ÿ-9-åŸ·è¡Œè‡ªå‹•éƒ¨ç½²è…³æœ¬)
10. [é…ç½® Systemd è‡ªå‹•å•Ÿå‹•](#æ­¥é©Ÿ-10-é…ç½®-systemd-è‡ªå‹•å•Ÿå‹•)
11. [é©—è­‰éƒ¨ç½²ç‹€æ…‹](#æ­¥é©Ÿ-11-é©—è­‰éƒ¨ç½²ç‹€æ…‹)
12. [è¨­ç½®ç›£æ§å‘Šè­¦](#æ­¥é©Ÿ-12-è¨­ç½®ç›£æ§å‘Šè­¦)
13. [åŸ·è¡Œè² è¼‰æ¸¬è©¦](#æ­¥é©Ÿ-13-åŸ·è¡Œè² è¼‰æ¸¬è©¦)
14. [é…ç½®é˜²ç«ç‰†](#æ­¥é©Ÿ-14-é…ç½®é˜²ç«ç‰†)
15. [è¨­ç½®å‚™ä»½è¨ˆç•«](#æ­¥é©Ÿ-15-è¨­ç½®å‚™ä»½è¨ˆç•«)

---

## æ­¥é©Ÿ 1: å‰ç½®æº–å‚™

### 1.1 æº–å‚™è³‡è¨Šæ¸…å–®

åœ¨é–‹å§‹éƒ¨ç½²å‰ï¼Œè«‹æº–å‚™ä»¥ä¸‹è³‡è¨Šï¼š

```bash
# ä¼ºæœå™¨è³‡è¨Š
ç”Ÿç”¢ä¼ºæœå™¨ IP: _________________
SSH ç”¨æˆ¶åç¨±: _________________
SSH å¯†é‘°è·¯å¾‘: _________________
åŸŸå (å¯é¸): _________________

# API é‡‘é‘°
OpenAI API Key: _________________
Anthropic API Key: _________________
Google Gemini API Key: _________________

# è³‡æ–™åº«å¯†ç¢¼ (å»ºè­°ä½¿ç”¨å¼·å¯†ç¢¼)
PostgreSQL å¯†ç¢¼: _________________
Redis å¯†ç¢¼: _________________
RabbitMQ å¯†ç¢¼: _________________

# SSL æ†‘è­‰ (å¦‚æœä½¿ç”¨ HTTPS)
æ†‘è­‰æ–‡ä»¶è·¯å¾‘: _________________
ç§é‘°æ–‡ä»¶è·¯å¾‘: _________________
```

### 1.2 ç¢ºèªç¡¬é«”éœ€æ±‚

```bash
æœ€ä½éœ€æ±‚:
âœ“ CPU: 32 cores
âœ“ RAM: 128 GB
âœ“ Storage: 500 GB SSD
âœ“ GPU: 2x NVIDIA H100 (80GB)
âœ“ Network: 10 Gbps
```

### 1.3 æº–å‚™æœ¬åœ°å·¥ä½œç’°å¢ƒ

```bash
# åœ¨æœ¬åœ°é–‹ç™¼æ©Ÿå™¨ä¸Š
cd /path/to/your/ai_platform

# ç¢ºèªæ‰€æœ‰æ–‡ä»¶éƒ½å­˜åœ¨
ls -la docker-compose.production.yml
ls -la .env.production.example
ls -la deploy-rhel-production.sh
ls -la systemd/
ls -la config/
ls -la load-tests/
```

**å®Œæˆç¢ºèª:** âœ“ æ‰€æœ‰è³‡è¨Šå·²æº–å‚™
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## æ­¥é©Ÿ 2: ç³»çµ±ç’°å¢ƒæª¢æŸ¥

### 2.1 SSH é€£ç·šåˆ°ç”Ÿç”¢ä¼ºæœå™¨

```bash
# å¾æœ¬åœ°é€£ç·šåˆ°ç”Ÿç”¢ä¼ºæœå™¨
ssh -i ~/.ssh/your-key.pem root@your-production-server-ip

# æˆ–ä½¿ç”¨å¯†ç¢¼ç™»å…¥
ssh root@your-production-server-ip
```

### 2.2 ç¢ºèªä½œæ¥­ç³»çµ±ç‰ˆæœ¬

```bash
# æª¢æŸ¥ RHEL ç‰ˆæœ¬ (æ‡‰è©²æ˜¯ 9.4)
cat /etc/redhat-release

# é æœŸè¼¸å‡ºï¼š
# Red Hat Enterprise Linux release 9.4 (Plow)
```

### 2.3 æª¢æŸ¥ç¡¬é«”è³‡æº

```bash
# æª¢æŸ¥ CPU
lscpu | grep -E "^CPU\(s\)|^Model name"

# æª¢æŸ¥è¨˜æ†¶é«”
free -h

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
df -h /

# æª¢æŸ¥ GPU (å¦‚æœé©…å‹•å·²å®‰è£)
nvidia-smi || echo "NVIDIA é©…å‹•å°šæœªå®‰è£"
```

### 2.4 æ›´æ–°ç³»çµ±å¥—ä»¶

```bash
# æ›´æ–°å¥—ä»¶åˆ—è¡¨
sudo dnf update -y

# å®‰è£åŸºæœ¬å·¥å…·
sudo dnf install -y \
    wget curl git vim \
    tar unzip gcc make \
    kernel-devel kernel-headers
```

**å®Œæˆç¢ºèª:** âœ“ ç³»çµ±ç’°å¢ƒç¬¦åˆéœ€æ±‚
**é è¨ˆæ™‚é–“:** 5 åˆ†é˜

---

## æ­¥é©Ÿ 3: ä¸Šå‚³éƒ¨ç½²æ–‡ä»¶

### 3.1 åœ¨ç”Ÿç”¢ä¼ºæœå™¨ä¸Šå»ºç«‹ç›®éŒ„

```bash
# åœ¨ç”Ÿç”¢ä¼ºæœå™¨ä¸ŠåŸ·è¡Œ
sudo mkdir -p /opt/ai_platform
sudo chown $USER:$USER /opt/ai_platform
cd /opt/ai_platform
```

### 3.2 å¾æœ¬åœ°ä¸Šå‚³æ–‡ä»¶

```bash
# åœ¨æœ¬åœ°é–‹ç™¼æ©Ÿå™¨ä¸ŠåŸ·è¡Œ
cd /path/to/your/ai_platform

# æ–¹æ³• 1: ä½¿ç”¨ SCP ä¸Šå‚³æ•´å€‹å°ˆæ¡ˆ
scp -r \
    docker-compose.production.yml \
    .env.production.example \
    deploy-rhel-production.sh \
    systemd \
    config \
    load-tests \
    services \
    PRODUCTION_DEPLOYMENT.md \
    root@your-production-server-ip:/opt/ai_platform/

# æ–¹æ³• 2: ä½¿ç”¨ rsync (æ›´å¿«ï¼Œæ”¯æ´æ–·é»çºŒå‚³)
rsync -avz --progress \
    --exclude='node_modules' \
    --exclude='__pycache__' \
    --exclude='*.pyc' \
    --exclude='.git' \
    . root@your-production-server-ip:/opt/ai_platform/

# æ–¹æ³• 3: æ‰“åŒ…å¾Œä¸Šå‚³ (é©åˆç¶²è·¯ä¸ç©©å®š)
tar -czf ai_platform_deployment.tar.gz \
    docker-compose.production.yml \
    .env.production.example \
    deploy-rhel-production.sh \
    systemd \
    config \
    load-tests \
    services \
    PRODUCTION_DEPLOYMENT.md

scp ai_platform_deployment.tar.gz root@your-production-server-ip:/tmp/

# åœ¨ç”Ÿç”¢ä¼ºæœå™¨ä¸Šè§£å£“
ssh root@your-production-server-ip
cd /opt/ai_platform
tar -xzf /tmp/ai_platform_deployment.tar.gz
rm /tmp/ai_platform_deployment.tar.gz
```

### 3.3 é©—è­‰æ–‡ä»¶å®Œæ•´æ€§

```bash
# åœ¨ç”Ÿç”¢ä¼ºæœå™¨ä¸ŠåŸ·è¡Œ
cd /opt/ai_platform

# æª¢æŸ¥ä¸»è¦æ–‡ä»¶
ls -lh docker-compose.production.yml
ls -lh deploy-rhel-production.sh
ls -lh .env.production.example

# æª¢æŸ¥ç›®éŒ„çµæ§‹
tree -L 2 || ls -R
```

### 3.4 è¨­ç½®åŸ·è¡Œæ¬Šé™

```bash
# è¨­ç½®è…³æœ¬åŸ·è¡Œæ¬Šé™
chmod +x deploy-rhel-production.sh
chmod +x systemd/*.sh
chmod +x load-tests/*.sh
```

**å®Œæˆç¢ºèª:** âœ“ æ‰€æœ‰æ–‡ä»¶å·²ä¸Šå‚³ä¸¦é©—è­‰
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## æ­¥é©Ÿ 4: å®‰è£ NVIDIA é©…å‹•

### 4.1 æª¢æŸ¥ GPU æ˜¯å¦è¢«åµæ¸¬

```bash
# æª¢æŸ¥ PCI è¨­å‚™
lspci | grep -i nvidia

# é æœŸè¼¸å‡ºæ‡‰åŒ…å«å…©å€‹ H100
# ä¾‹å¦‚ï¼š
# 17:00.0 3D controller: NVIDIA Corporation Device 2330 (rev a1)
# 65:00.0 3D controller: NVIDIA Corporation Device 2330 (rev a1)
```

### 4.2 å®‰è£ NVIDIA å®˜æ–¹é©…å‹•åº«

```bash
# æ·»åŠ  NVIDIA é©…å‹•åº«
sudo dnf config-manager --add-repo \
    https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

# æ›´æ–°å¥—ä»¶ç´¢å¼•
sudo dnf clean all
sudo dnf makecache
```

### 4.3 å®‰è£ NVIDIA é©…å‹•

```bash
# å®‰è£ NVIDIA é©…å‹• (550+ ç‰ˆæœ¬æ”¯æ´ H100)
sudo dnf install -y nvidia-driver-latest-dkms

# æˆ–æŒ‡å®šç‰ˆæœ¬
sudo dnf install -y nvidia-driver-550-dkms

# å®‰è£ CUDA å·¥å…·åŒ… (12.0+)
sudo dnf install -y cuda-toolkit-12-4
```

### 4.4 é‡æ–°å•Ÿå‹•ç³»çµ±

```bash
# é‡å•Ÿä»¥è¼‰å…¥é©…å‹•
sudo reboot

# ç­‰å¾… 2-3 åˆ†é˜å¾Œé‡æ–°é€£ç·š
# ssh root@your-production-server-ip
```

### 4.5 é©—è­‰ GPU é©…å‹•

```bash
# æª¢æŸ¥é©…å‹•ç‰ˆæœ¬
nvidia-smi

# é æœŸè¼¸å‡ºæ‡‰é¡¯ç¤º:
# - Driver Version: 550.x æˆ–æ›´é«˜
# - CUDA Version: 12.4 æˆ–æ›´é«˜
# - 2x NVIDIA H100 80GB GPU
# - GPU æº«åº¦ã€åŠŸè€—ã€è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version
```

**å®Œæˆç¢ºèª:** âœ“ NVIDIA é©…å‹•å·²å®‰è£ï¼Œnvidia-smi æ­£å¸¸é‹ä½œ
**é è¨ˆæ™‚é–“:** 15 åˆ†é˜ (å«é‡å•Ÿ)

---

## æ­¥é©Ÿ 5: å®‰è£ Docker

### 5.1 ç§»é™¤èˆŠç‰ˆæœ¬ Docker (å¦‚æœå­˜åœ¨)

```bash
sudo dnf remove -y \
    docker \
    docker-client \
    docker-client-latest \
    docker-common \
    docker-latest \
    docker-latest-logrotate \
    docker-logrotate \
    docker-engine \
    podman \
    runc
```

### 5.2 å®‰è£ Docker CE

```bash
# æ·»åŠ  Docker å®˜æ–¹åº«
sudo dnf config-manager --add-repo \
    https://download.docker.com/linux/rhel/docker-ce.repo

# å®‰è£ Docker
sudo dnf install -y \
    docker-ce \
    docker-ce-cli \
    containerd.io \
    docker-buildx-plugin \
    docker-compose-plugin
```

### 5.3 å•Ÿå‹• Docker æœå‹™

```bash
# å•Ÿå‹•ä¸¦è¨­ç½®é–‹æ©Ÿè‡ªå‹•å•Ÿå‹•
sudo systemctl start docker
sudo systemctl enable docker

# æª¢æŸ¥ Docker ç‹€æ…‹
sudo systemctl status docker

# é©—è­‰ Docker å®‰è£
docker --version
docker compose version
```

### 5.4 æ¸¬è©¦ Docker é‹ä½œ

```bash
# åŸ·è¡Œæ¸¬è©¦å®¹å™¨
sudo docker run hello-world

# é æœŸè¼¸å‡ºï¼š
# Hello from Docker!
# This message shows that your installation appears to be working correctly.
```

### 5.5 å°‡ç•¶å‰ç”¨æˆ¶åŠ å…¥ docker ç¾¤çµ„ (å¯é¸)

```bash
# åŠ å…¥ docker ç¾¤çµ„ (é¿å…æ¯æ¬¡éƒ½è¦ sudo)
sudo usermod -aG docker $USER

# é‡æ–°ç™»å…¥ä»¥ç”Ÿæ•ˆ
exit
ssh root@your-production-server-ip
```

**å®Œæˆç¢ºèª:** âœ“ Docker å·²å®‰è£ä¸¦æ­£å¸¸é‹ä½œ
**é è¨ˆæ™‚é–“:** 5 åˆ†é˜

---

## æ­¥é©Ÿ 6: å®‰è£ NVIDIA Container Toolkit

### 6.1 æ·»åŠ  NVIDIA Container Toolkit åº«

```bash
# æ·»åŠ  NVIDIA å®¹å™¨å·¥å…·åº«
curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
    sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# æ›´æ–°å¥—ä»¶ç´¢å¼•
sudo dnf makecache
```

### 6.2 å®‰è£ NVIDIA Container Toolkit

```bash
# å®‰è£å·¥å…·åŒ…
sudo dnf install -y nvidia-container-toolkit
```

### 6.3 é…ç½® Docker ä½¿ç”¨ NVIDIA Runtime

```bash
# é…ç½® Docker daemon
sudo nvidia-ctk runtime configure --runtime=docker

# é‡å•Ÿ Docker æœå‹™
sudo systemctl restart docker
```

### 6.4 é©—è­‰ GPU å®¹å™¨è¨ªå•

```bash
# æ¸¬è©¦ GPU å®¹å™¨
sudo docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi

# é æœŸè¼¸å‡ºæ‡‰é¡¯ç¤ºå…©å€‹ H100 GPU

# æ¸¬è©¦æŒ‡å®šå–®å€‹ GPU
sudo docker run --rm --gpus '"device=0"' nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi

# æ¸¬è©¦æŒ‡å®šå…©å€‹ GPU
sudo docker run --rm --gpus '"device=0,1"' nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

**å®Œæˆç¢ºèª:** âœ“ Docker å®¹å™¨å¯ä»¥è¨ªå• GPU
**é è¨ˆæ™‚é–“:** 5 åˆ†é˜

---

## æ­¥é©Ÿ 7: é…ç½®ç’°å¢ƒè®Šæ•¸

### 7.1 è¤‡è£½ç’°å¢ƒè®Šæ•¸æ¨¡æ¿

```bash
cd /opt/ai_platform

# è¤‡è£½æ¨¡æ¿æ–‡ä»¶
cp .env.production.example .env
```

### 7.2 ç·¨è¼¯ç’°å¢ƒè®Šæ•¸

```bash
# ä½¿ç”¨ vim æˆ– nano ç·¨è¼¯
vim .env
# æˆ–
nano .env
```

### 7.3 å¿…å¡«é…ç½®é …ç›®

**é‡è¦ï¼šè«‹å‹™å¿…ä¿®æ”¹ä»¥ä¸‹é …ç›®**

```bash
# === API é‡‘é‘° (å¿…é ˆå¡«å¯«) ===
OPENAI_API_KEY=sk-your-actual-openai-api-key
ANTHROPIC_API_KEY=sk-ant-your-actual-anthropic-key
GOOGLE_API_KEY=your-actual-gemini-api-key

# === è³‡æ–™åº«å¯†ç¢¼ (å¿…é ˆä¿®æ”¹ï¼Œä½¿ç”¨å¼·å¯†ç¢¼) ===
POSTGRES_PASSWORD=YourSuperSecurePassword123!
REDIS_PASSWORD=YourRedisSecurePassword456!
RABBITMQ_DEFAULT_PASS=YourRabbitMQSecurePassword789!

# === åŸŸåè¨­å®š ===
# å¦‚æœæœ‰åŸŸåï¼Œä¿®æ”¹é€™å€‹
DOMAIN=your-domain.com
# å¦‚æœåªä½¿ç”¨ IPï¼Œè¨­ç‚º localhost
DOMAIN=localhost

# === SSL/TLS è¨­å®š ===
# å¦‚æœä½¿ç”¨ HTTPSï¼Œè¨­ç‚º true
ENABLE_SSL=true
# å¦‚æœåªç”¨ HTTPï¼Œè¨­ç‚º false
ENABLE_SSL=false

# === GPU è¨­å®š (å·²é è¨­ç‚ºé›™ GPU) ===
ENABLE_GPU=true
CUDA_VISIBLE_DEVICES=0,1
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_LOADED_MODELS=2

# === æ•ˆèƒ½èª¿æ•´ ===
MAX_CONCURRENT_REQUESTS=200
RATE_LIMIT_PER_MINUTE=60
```

### 7.4 ç”Ÿæˆå®‰å…¨å¯†ç¢¼å·¥å…·

```bash
# ç”Ÿæˆå¼·å¯†ç¢¼ (32 å­—å…ƒ)
openssl rand -base64 32

# æˆ–ä½¿ç”¨ Python
python3 -c "import secrets; print(secrets.token_urlsafe(32))"

# ç”Ÿæˆå¤šçµ„å¯†ç¢¼
for i in {1..3}; do
    echo "Password $i: $(openssl rand -base64 32)"
done
```

### 7.5 é©—è­‰ç’°å¢ƒè®Šæ•¸

```bash
# æª¢æŸ¥å¿…è¦è®Šæ•¸æ˜¯å¦å·²è¨­å®š
grep -E "^(OPENAI_API_KEY|ANTHROPIC_API_KEY|POSTGRES_PASSWORD)" .env

# ç¢ºä¿æ²’æœ‰ä½¿ç”¨é è¨­å€¼
if grep -q "your-openai-api-key" .env; then
    echo "è­¦å‘Š: è«‹ä¿®æ”¹ OPENAI_API_KEY!"
fi

if grep -q "changeme" .env; then
    echo "è­¦å‘Š: è«‹ä¿®æ”¹è³‡æ–™åº«å¯†ç¢¼!"
fi
```

### 7.6 ä¿è­·ç’°å¢ƒè®Šæ•¸æ–‡ä»¶

```bash
# è¨­ç½®åªæœ‰ root å¯è®€
sudo chmod 600 .env
sudo chown root:root .env

# é©—è­‰æ¬Šé™
ls -la .env
# æ‡‰é¡¯ç¤º: -rw------- 1 root root
```

**å®Œæˆç¢ºèª:** âœ“ ç’°å¢ƒè®Šæ•¸å·²é…ç½®ä¸¦ä¿è­·
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## æ­¥é©Ÿ 8: æº–å‚™ SSL æ†‘è­‰

### 8.1 é¸æ“‡ SSL æ†‘è­‰æ–¹æ¡ˆ

**æ–¹æ¡ˆ A: ä½¿ç”¨ Let's Encrypt å…è²»æ†‘è­‰ (æ¨è–¦)**

```bash
# å®‰è£ Certbot
sudo dnf install -y certbot

# å–å¾—æ†‘è­‰ (éœ€è¦åœæ­¢å…¶ä»–ä½”ç”¨ 80/443 port çš„æœå‹™)
sudo certbot certonly --standalone -d your-domain.com

# æ†‘è­‰ä½ç½®:
# /etc/letsencrypt/live/your-domain.com/fullchain.pem
# /etc/letsencrypt/live/your-domain.com/privkey.pem
```

**æ–¹æ¡ˆ B: ä½¿ç”¨è‡ªç°½æ†‘è­‰ (åƒ…æ¸¬è©¦ç”¨)**

```bash
# å»ºç«‹æ†‘è­‰ç›®éŒ„
sudo mkdir -p /opt/ai_platform/certs

# ç”Ÿæˆè‡ªç°½æ†‘è­‰
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
    -keyout /opt/ai_platform/certs/privkey.pem \
    -out /opt/ai_platform/certs/fullchain.pem \
    -subj "/C=TW/ST=Taipei/L=Taipei/O=AI Platform/CN=your-domain.com"
```

**æ–¹æ¡ˆ C: ä¸Šå‚³ç¾æœ‰æ†‘è­‰**

```bash
# åœ¨æœ¬åœ°æ©Ÿå™¨ä¸Šå‚³æ†‘è­‰
scp /path/to/your/fullchain.pem root@your-server:/opt/ai_platform/certs/
scp /path/to/your/privkey.pem root@your-server:/opt/ai_platform/certs/
```

### 8.2 è¨­ç½®æ†‘è­‰æ¬Šé™

```bash
# è¨­ç½®æ†‘è­‰æ¬Šé™
sudo chmod 644 /opt/ai_platform/certs/fullchain.pem
sudo chmod 600 /opt/ai_platform/certs/privkey.pem
sudo chown root:root /opt/ai_platform/certs/*
```

### 8.3 æ›´æ–° nginx é…ç½®

```bash
# å¦‚æœä½¿ç”¨è‡ªè¨‚æ†‘è­‰è·¯å¾‘ï¼Œç·¨è¼¯ nginx.conf
vim /opt/ai_platform/config/nginx/nginx.conf

# ä¿®æ”¹æ†‘è­‰è·¯å¾‘ (æ‰¾åˆ°é€™å…©è¡Œä¸¦ä¿®æ”¹)
ssl_certificate /opt/ai_platform/certs/fullchain.pem;
ssl_certificate_key /opt/ai_platform/certs/privkey.pem;
```

### 8.4 ä¸ä½¿ç”¨ SSL (HTTP only)

```bash
# å¦‚æœä¸ä½¿ç”¨ SSLï¼Œç·¨è¼¯ .env
vim /opt/ai_platform/.env

# è¨­ç½®ç‚º false
ENABLE_SSL=false

# ä¸¦è¨»è§£æ‰ nginx.conf ä¸­çš„ SSL ç›¸é—œè¨­å®š
```

**å®Œæˆç¢ºèª:** âœ“ SSL æ†‘è­‰å·²æº–å‚™ (æˆ–å·²é¸æ“‡ HTTP only)
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## æ­¥é©Ÿ 9: åŸ·è¡Œè‡ªå‹•éƒ¨ç½²è…³æœ¬

### 9.1 æª¢æŸ¥éƒ¨ç½²è…³æœ¬

```bash
cd /opt/ai_platform

# æŸ¥çœ‹è…³æœ¬å…§å®¹ (å¯é¸)
less deploy-rhel-production.sh

# ç¢ºèªè…³æœ¬æœ‰åŸ·è¡Œæ¬Šé™
ls -l deploy-rhel-production.sh
```

### 9.2 åŸ·è¡Œéƒ¨ç½²è…³æœ¬

```bash
# åŸ·è¡Œè‡ªå‹•éƒ¨ç½² (æœƒè‡ªå‹•æª¢æŸ¥æ‰€æœ‰ä¾è³´)
sudo ./deploy-rhel-production.sh

# è…³æœ¬æœƒè‡ªå‹•åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿ:
# 1. æª¢æŸ¥ OS ç‰ˆæœ¬
# 2. æª¢æŸ¥ç¡¬é«”è³‡æº (CPU, RAM, Disk)
# 3. æª¢æŸ¥ NVIDIA é©…å‹•
# 4. æª¢æŸ¥ CUDA
# 5. æª¢æŸ¥ Docker
# 6. æª¢æŸ¥ NVIDIA Container Toolkit
# 7. é…ç½®é˜²ç«ç‰†
# 8. æ‹‰å– Docker æ˜ åƒ
# 9. å•Ÿå‹•æœå‹™ (ä¾åº: åŸºç¤è¨­æ–½ â†’ LLM â†’ æ‡‰ç”¨ â†’ ç›£æ§)
# 10. é©—è­‰æœå‹™ç‹€æ…‹
```

### 9.3 ç›£æ§éƒ¨ç½²éç¨‹

```bash
# éƒ¨ç½²éç¨‹æœƒé¡¯ç¤ºé€²åº¦ï¼Œä¾‹å¦‚:
# âœ“ OS version check passed
# âœ“ Hardware resources check passed
# âœ“ NVIDIA drivers found (2 GPUs)
# âœ“ CUDA toolkit found
# âœ“ Docker running
# âœ“ NVIDIA Container Toolkit found
# â³ Pulling Docker images...
# â³ Starting infrastructure services...
# â³ Starting LLM services...
# â³ Starting application services...
# â³ Starting monitoring services...
# âœ“ Deployment completed successfully!
```

### 9.4 å¦‚æœéƒ¨ç½²è…³æœ¬å¤±æ•—

```bash
# æŸ¥çœ‹è©³ç´°éŒ¯èª¤æ—¥èªŒ
cat deploy.log

# æ‰‹å‹•æª¢æŸ¥å„é …ä¾è³´
./deploy-rhel-production.sh --check-only

# é‡è©¦éƒ¨ç½²
sudo ./deploy-rhel-production.sh --force
```

### 9.5 æ‰‹å‹•éƒ¨ç½² (å¦‚æœè‡ªå‹•è…³æœ¬å¤±æ•—)

```bash
# 1. æ‹‰å–æ‰€æœ‰æ˜ åƒ
sudo docker compose -f docker-compose.production.yml pull

# 2. å•Ÿå‹•åŸºç¤è¨­æ–½æœå‹™
sudo docker compose -f docker-compose.production.yml up -d \
    postgres redis qdrant rabbitmq

# ç­‰å¾… 15 ç§’è®“è³‡æ–™åº«åˆå§‹åŒ–
sleep 15

# 3. å•Ÿå‹• LLM æœå‹™
sudo docker compose -f docker-compose.production.yml up -d \
    ollama litellm

# ç­‰å¾… 30 ç§’è®“ LLM æœå‹™å°±ç·’
sleep 30

# 4. å•Ÿå‹•æ‡‰ç”¨æœå‹™
sudo docker compose -f docker-compose.production.yml up -d \
    mcp-server agent-service web-ui

# 5. å•Ÿå‹•ç›£æ§æœå‹™
sudo docker compose -f docker-compose.production.yml up -d \
    prometheus grafana nginx
```

**å®Œæˆç¢ºèª:** âœ“ æ‰€æœ‰æœå‹™å·²å•Ÿå‹•
**é è¨ˆæ™‚é–“:** 15-20 åˆ†é˜ (å«ä¸‹è¼‰æ˜ åƒ)

---

## æ­¥é©Ÿ 10: é…ç½® Systemd è‡ªå‹•å•Ÿå‹•

### 10.1 å®‰è£ Systemd æœå‹™

```bash
cd /opt/ai_platform/systemd

# åŸ·è¡Œå®‰è£è…³æœ¬
sudo ./install-systemd.sh

# è…³æœ¬æœƒ:
# 1. è¤‡è£½æœå‹™æ–‡ä»¶åˆ° /etc/systemd/system/
# 2. é‡æ–°è¼‰å…¥ systemd
# 3. å•Ÿç”¨æ‰€æœ‰æœå‹™å’Œå®šæ™‚å™¨
# 4. é¡¯ç¤ºä½¿ç”¨èªªæ˜
```

### 10.2 é©—è­‰ Systemd æœå‹™

```bash
# æª¢æŸ¥ä¸»æœå‹™ç‹€æ…‹
sudo systemctl status ai-platform

# æª¢æŸ¥å‚™ä»½å®šæ™‚å™¨
sudo systemctl status ai-platform-backup.timer

# æª¢æŸ¥å¥åº·æª¢æŸ¥å®šæ™‚å™¨
sudo systemctl status ai-platform-healthcheck.timer

# åˆ—å‡ºæ‰€æœ‰ AI Platform ç›¸é—œæœå‹™
sudo systemctl list-units "ai-platform*"
```

### 10.3 æ¸¬è©¦è‡ªå‹•é‡å•ŸåŠŸèƒ½

```bash
# åœæ­¢å¹³å°
sudo systemctl stop ai-platform

# ç­‰å¾…å¹¾ç§’å¾Œæª¢æŸ¥ (æ‡‰è‡ªå‹•é‡å•Ÿ)
sleep 10
sudo systemctl status ai-platform

# æª¢æŸ¥å®¹å™¨ç‹€æ…‹
sudo docker ps
```

### 10.4 æŸ¥çœ‹æœå‹™æ—¥èªŒ

```bash
# æŸ¥çœ‹ä¸»æœå‹™æ—¥èªŒ
sudo journalctl -u ai-platform -f

# æŸ¥çœ‹å‚™ä»½æ—¥èªŒ
sudo journalctl -u ai-platform-backup -n 50

# æŸ¥çœ‹å¥åº·æª¢æŸ¥æ—¥èªŒ
sudo journalctl -u ai-platform-healthcheck -n 50
```

**å®Œæˆç¢ºèª:** âœ“ Systemd æœå‹™å·²å•Ÿç”¨ï¼Œé‡å•Ÿå¾Œè‡ªå‹•å•Ÿå‹•
**é è¨ˆæ™‚é–“:** 5 åˆ†é˜

---

## æ­¥é©Ÿ 11: é©—è­‰éƒ¨ç½²ç‹€æ…‹

### 11.1 æª¢æŸ¥æ‰€æœ‰å®¹å™¨ç‹€æ…‹

```bash
# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨
sudo docker ps -a

# æ‡‰è©²çœ‹åˆ°ä»¥ä¸‹æœå‹™éƒ½æ˜¯ Up ç‹€æ…‹:
# - postgres
# - redis
# - qdrant
# - rabbitmq
# - ollama
# - litellm
# - mcp-server (3 å€‹å‰¯æœ¬)
# - agent-service (3 å€‹å‰¯æœ¬)
# - web-ui (2 å€‹å‰¯æœ¬)
# - prometheus
# - grafana
# - nginx
```

### 11.2 æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹

```bash
# å¥åº·æª¢æŸ¥ç«¯é»
curl -s http://localhost:8001/health | jq .

# é æœŸè¼¸å‡º:
# {
#   "status": "healthy",
#   "services": {
#     "postgres": "up",
#     "redis": "up",
#     "qdrant": "up",
#     "ollama": "up"
#   }
# }

# æª¢æŸ¥ MCP Server
curl -s http://localhost:8002/health

# æª¢æŸ¥ Agent Service
curl -s http://localhost:8000/health

# æª¢æŸ¥ LiteLLM
curl -s http://localhost:4000/health
```

### 11.3 æª¢æŸ¥ GPU ä½¿ç”¨æƒ…æ³

```bash
# æŸ¥çœ‹ GPU ç‹€æ…‹
nvidia-smi

# æŒçºŒç›£æ§ GPU
watch -n 1 nvidia-smi

# æª¢æŸ¥ Ollama å®¹å™¨çš„ GPU è¨ªå•
sudo docker exec ai-ollama-prod nvidia-smi
```

### 11.4 æ¸¬è©¦ Ollama æ¨¡å‹

```bash
# é€²å…¥ Ollama å®¹å™¨
sudo docker exec -it ai-ollama-prod bash

# æ‹‰å–æ¸¬è©¦æ¨¡å‹ (qwen2.5:7b)
ollama pull qwen2.5:7b

# æ¸¬è©¦æ¨¡å‹æ¨ç†
ollama run qwen2.5:7b "Hello, how are you?"

# é›¢é–‹å®¹å™¨
exit
```

### 11.5 æª¢æŸ¥ç¶²è·¯é€£ç·š

```bash
# æ¸¬è©¦å…§éƒ¨ç¶²è·¯
curl http://localhost:8501  # Web UI
curl http://localhost:8000  # Agent Service
curl http://localhost:8001  # MCP Server
curl http://localhost:3000  # Grafana

# å¦‚æœä½¿ç”¨ NGINX (HTTPS)
curl https://localhost/health
# æˆ–
curl https://your-domain.com/health
```

### 11.6 æª¢æŸ¥æ—¥èªŒ

```bash
# æŸ¥çœ‹æ‰€æœ‰æœå‹™æ—¥èªŒ
sudo docker compose -f /opt/ai_platform/docker-compose.production.yml logs --tail=50

# æŸ¥çœ‹ç‰¹å®šæœå‹™æ—¥èªŒ
sudo docker compose -f /opt/ai_platform/docker-compose.production.yml logs agent-service --tail=100 -f

# æŸ¥çœ‹ Ollama æ—¥èªŒ
sudo docker logs ai-ollama-prod --tail=50 -f
```

**å®Œæˆç¢ºèª:** âœ“ æ‰€æœ‰æœå‹™å¥åº·ï¼ŒGPU å¯è¨ªå•ï¼Œç«¯é»æ­£å¸¸å›æ‡‰
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## æ­¥é©Ÿ 12: è¨­ç½®ç›£æ§å‘Šè­¦

### 12.1 è¨ªå• Grafana

```bash
# 1. å–å¾— Grafana ç®¡ç†å“¡å¯†ç¢¼
grep GRAFANA_ADMIN_PASSWORD /opt/ai_platform/.env

# 2. åœ¨ç€è¦½å™¨è¨ªå• Grafana
http://localhost:3000
# æˆ–
https://your-domain.com/grafana

# 3. ç™»å…¥
# ç”¨æˆ¶å: admin
# å¯†ç¢¼: (å¾ä¸Šé¢å–å¾—)
```

### 12.2 é©—è­‰å„€è¡¨æ¿

```bash
# Grafana æ‡‰è‡ªå‹•è¼‰å…¥å…©å€‹å„€è¡¨æ¿:
# 1. AI Platform Overview (ç³»çµ±ç¸½è¦½)
# 2. GPU Monitoring (GPU ç›£æ§)

# å°èˆª: Dashboards â†’ Browse â†’ AI Platform
```

### 12.3 é…ç½®å‘Šè­¦è¦å‰‡

åœ¨ Grafana ä¸­è¨­ç½®å‘Šè­¦:

**é«˜ GPU æº«åº¦å‘Šè­¦:**
```
æ¢ä»¶: GPU æº«åº¦ > 85Â°C æŒçºŒ 5 åˆ†é˜
åš´é‡åº¦: è­¦å‘Š
å‹•ä½œ: ç™¼é€é€šçŸ¥
```

**é«˜éŒ¯èª¤ç‡å‘Šè­¦:**
```
æ¢ä»¶: éŒ¯èª¤ç‡ > 5% æŒçºŒ 5 åˆ†é˜
åš´é‡åº¦: åš´é‡
å‹•ä½œ: ç™¼é€é€šçŸ¥ + è‡ªå‹•é‡å•Ÿ
```

**ä½ GPU åˆ©ç”¨ç‡å‘Šè­¦:**
```
æ¢ä»¶: GPU åˆ©ç”¨ç‡ < 10% æŒçºŒ 30 åˆ†é˜
åš´é‡åº¦: è³‡è¨Š
å‹•ä½œ: è¨˜éŒ„æ—¥èªŒ
```

### 12.4 é…ç½®é€šçŸ¥æ¸ é“

```bash
# åœ¨ Grafana ä¸­è¨­ç½®é€šçŸ¥:
# Settings â†’ Alerting â†’ Contact points

# æ”¯æ´çš„é€šçŸ¥æ–¹å¼:
# - Email
# - Slack
# - Discord
# - Webhook
# - PagerDuty
```

### 12.5 æ¸¬è©¦å‘Šè­¦

```bash
# æ‰‹å‹•è§¸ç™¼æ¸¬è©¦å‘Šè­¦
# åœ¨ Grafana Alert è¦å‰‡ä¸­é»æ“Š "Test"

# æˆ–æ‰‹å‹•è£½é€ é«˜è² è¼‰
stress --cpu 16 --timeout 60s
```

**å®Œæˆç¢ºèª:** âœ“ Grafana å¯è¨ªå•ï¼Œå„€è¡¨æ¿é¡¯ç¤ºæ•¸æ“šï¼Œå‘Šè­¦å·²é…ç½®
**é è¨ˆæ™‚é–“:** 15 åˆ†é˜

---

## æ­¥é©Ÿ 13: åŸ·è¡Œè² è¼‰æ¸¬è©¦

### 13.1 å®‰è£æ¸¬è©¦å·¥å…·

```bash
cd /opt/ai_platform/load-tests

# å®‰è£ Apache Bench
sudo dnf install -y httpd-tools

# å®‰è£ Python ä¾è³´
pip3 install -r requirements.txt

# é©—è­‰å®‰è£
ab -V
locust --version
```

### 13.2 åŸ·è¡Œç…™éœ§æ¸¬è©¦ (Smoke Test)

```bash
# å¿«é€Ÿé©—è­‰æ¸¬è©¦ (5 ç”¨æˆ¶, 100 è«‹æ±‚)
BASE_URL=http://localhost:8001 \
CONCURRENT_USERS=5 \
TOTAL_REQUESTS=100 \
./test-api-endpoints.sh

# æŸ¥çœ‹çµæœ
cat results_*/SUMMARY.txt
```

### 13.3 åŸ·è¡Œè² è¼‰æ¸¬è©¦ (Load Test)

```bash
# æ¨¡æ“¬æ­£å¸¸æµé‡ (50 ç”¨æˆ¶, 10 åˆ†é˜)
locust -f locustfile.py \
    --host=http://localhost:8001 \
    --users 50 \
    --spawn-rate 5 \
    --run-time 10m \
    --headless \
    --csv=results/load_test

# æŸ¥çœ‹çµæœ
cat results/load_test_stats.csv
```

### 13.4 åŸ·è¡Œå£“åŠ›æ¸¬è©¦ (Stress Test)

```bash
# æ¨¡æ“¬é«˜å³°æµé‡ (200 ç”¨æˆ¶, 15 åˆ†é˜)
locust -f locustfile.py \
    --host=http://localhost:8001 \
    --users 200 \
    --spawn-rate 20 \
    --run-time 15m \
    --headless \
    --csv=results/stress_test
```

### 13.5 åˆ†ææ¸¬è©¦çµæœ

```bash
# æª¢æŸ¥é—œéµæŒ‡æ¨™:
# - Response Time p95 < 1s âœ“
# - Error Rate < 2% âœ“
# - Throughput > 500 RPS âœ“

# å¦‚æœæ¸¬è©¦å¤±æ•—ï¼Œæª¢æŸ¥:
# 1. æœå‹™æ˜¯å¦å…¨éƒ¨å•Ÿå‹•
sudo docker ps

# 2. è³‡æºä½¿ç”¨æƒ…æ³
htop
nvidia-smi

# 3. æœå‹™æ—¥èªŒ
sudo docker compose logs --tail=100
```

### 13.6 ç›£æ§æ¸¬è©¦æœŸé–“çš„æŒ‡æ¨™

```bash
# åœ¨å¦ä¸€å€‹çµ‚ç«¯è¦–çª—ç›£æ§:

# GPU ä½¿ç”¨ç‡
watch -n 1 nvidia-smi

# å®¹å™¨è³‡æºä½¿ç”¨
watch -n 1 'docker stats --no-stream'

# æœå‹™æ—¥èªŒ
sudo docker compose logs -f agent-service
```

**å®Œæˆç¢ºèª:** âœ“ è² è¼‰æ¸¬è©¦é€šéï¼Œæ•ˆèƒ½ç¬¦åˆé æœŸ
**é è¨ˆæ™‚é–“:** 20 åˆ†é˜

---

## æ­¥é©Ÿ 14: é…ç½®é˜²ç«ç‰†

### 14.1 é–‹æ”¾å¿…è¦ç«¯å£

```bash
# å¦‚æœä½¿ç”¨ firewalld
sudo systemctl start firewalld
sudo systemctl enable firewalld

# é–‹æ”¾ HTTP (80)
sudo firewall-cmd --permanent --add-service=http

# é–‹æ”¾ HTTPS (443)
sudo firewall-cmd --permanent --add-service=https

# é–‹æ”¾ SSH (22) - ç¢ºä¿å·²é–‹æ”¾
sudo firewall-cmd --permanent --add-service=ssh

# é–‹æ”¾ Grafana (3000) - åƒ…é™ä¿¡ä»» IP
sudo firewall-cmd --permanent --add-rich-rule='
  rule family="ipv4"
  source address="YOUR_OFFICE_IP/32"
  port protocol="tcp" port="3000" accept'

# é‡æ–°è¼‰å…¥é˜²ç«ç‰†
sudo firewall-cmd --reload

# æŸ¥çœ‹é–‹æ”¾çš„ç«¯å£
sudo firewall-cmd --list-all
```

### 14.2 ä½¿ç”¨ iptables (æ›¿ä»£æ–¹æ¡ˆ)

```bash
# å¦‚æœä¸ä½¿ç”¨ firewalldï¼Œä½¿ç”¨ iptables
sudo systemctl stop firewalld
sudo systemctl disable firewalld

# é–‹æ”¾ç«¯å£
sudo iptables -A INPUT -p tcp --dport 80 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 443 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT

# ä¿å­˜è¦å‰‡
sudo iptables-save | sudo tee /etc/sysconfig/iptables
```

### 14.3 é…ç½® SELinux

```bash
# æª¢æŸ¥ SELinux ç‹€æ…‹
getenforce

# å¦‚æœæ˜¯ Enforcingï¼Œè¨­ç½® Docker ç›¸é—œæ¬Šé™
sudo setsebool -P container_manage_cgroup on
sudo setsebool -P docker_connect_any on

# æˆ–æš«æ™‚è¨­ç‚º Permissive (ä¸å»ºè­°ç”Ÿç”¢ç’°å¢ƒ)
sudo setenforce 0

# æ°¸ä¹…ä¿®æ”¹ (ç·¨è¼¯ /etc/selinux/config)
sudo vim /etc/selinux/config
# æ”¹ç‚º: SELINUX=permissive
```

### 14.4 é™åˆ¶ä¾†æº IP (å®‰å…¨åŠ å›º)

```bash
# åªå…è¨±ç‰¹å®š IP è¨ªå• (è¾¦å…¬å®¤/VPN)
ALLOWED_IP="203.0.113.10"  # æ›¿æ›ç‚ºæ‚¨çš„ IP

sudo firewall-cmd --permanent --add-rich-rule="
  rule family='ipv4'
  source address='${ALLOWED_IP}/32'
  port protocol='tcp' port='443' accept"

# æ‹’çµ•å…¶ä»–æ‰€æœ‰ IP
sudo firewall-cmd --permanent --add-rich-rule="
  rule family='ipv4'
  port protocol='tcp' port='443' reject"

sudo firewall-cmd --reload
```

**å®Œæˆç¢ºèª:** âœ“ é˜²ç«ç‰†å·²é…ç½®ï¼Œåªé–‹æ”¾å¿…è¦ç«¯å£
**é è¨ˆæ™‚é–“:** 5 åˆ†é˜

---

## æ­¥é©Ÿ 15: è¨­ç½®å‚™ä»½è¨ˆç•«

### 15.1 é©—è­‰è‡ªå‹•å‚™ä»½

```bash
# æª¢æŸ¥å‚™ä»½å®šæ™‚å™¨ç‹€æ…‹
sudo systemctl status ai-platform-backup.timer

# æŸ¥çœ‹ä¸‹æ¬¡å‚™ä»½æ™‚é–“
sudo systemctl list-timers ai-platform-backup.timer

# æ‰‹å‹•åŸ·è¡Œå‚™ä»½æ¸¬è©¦
sudo systemctl start ai-platform-backup.service

# æŸ¥çœ‹å‚™ä»½æ—¥èªŒ
sudo journalctl -u ai-platform-backup.service -n 50
```

### 15.2 æª¢æŸ¥å‚™ä»½æ–‡ä»¶

```bash
# å‚™ä»½ä½ç½®
ls -lh /opt/ai_platform/backups/

# æ‡‰è©²åŒ…å«:
# - backup_YYYYMMDD_HHMMSS.tar.gz (å®Œæ•´å‚™ä»½)
# - postgres_YYYYMMDD_HHMMSS.sql (è³‡æ–™åº«å‚™ä»½)

# æª¢æŸ¥å‚™ä»½å¤§å°
du -sh /opt/ai_platform/backups/
```

### 15.3 æ¸¬è©¦å‚™ä»½é‚„åŸ

```bash
# è§£å£“å‚™ä»½æ¸¬è©¦
cd /tmp
sudo tar -xzf /opt/ai_platform/backups/backup_*.tar.gz

# æ¸¬è©¦è³‡æ–™åº«é‚„åŸ (åœ¨æ¸¬è©¦ç’°å¢ƒ)
# è­¦å‘Š: ä¸è¦åœ¨ç”Ÿç”¢ç’°å¢ƒç›´æ¥åŸ·è¡Œ
sudo docker exec -i ai-postgres-prod psql -U ai_platform < /opt/ai_platform/backups/postgres_*.sql
```

### 15.4 è¨­ç½®é ç«¯å‚™ä»½

```bash
# æ–¹æ¡ˆ A: è¤‡è£½åˆ°é ç«¯ä¼ºæœå™¨ (rsync)
BACKUP_SERVER="backup-server.example.com"
BACKUP_PATH="/mnt/backups/ai_platform/"

# è¨­ç½® SSH å…å¯†ç™»å…¥å¾ŒåŸ·è¡Œ
rsync -avz --delete \
    /opt/ai_platform/backups/ \
    ${BACKUP_SERVER}:${BACKUP_PATH}

# æ–¹æ¡ˆ B: ä¸Šå‚³åˆ° S3
# å®‰è£ AWS CLI
pip3 install awscli

# é…ç½® AWS æ†‘è­‰
aws configure

# åŒæ­¥åˆ° S3
aws s3 sync /opt/ai_platform/backups/ s3://your-bucket/ai-platform-backups/

# æ–¹æ¡ˆ C: ä½¿ç”¨ Google Cloud Storage
# å®‰è£ gsutil
curl https://sdk.cloud.google.com | bash

# åŒæ­¥åˆ° GCS
gsutil -m rsync -r /opt/ai_platform/backups/ gs://your-bucket/ai-platform-backups/
```

### 15.5 é…ç½®å‚™ä»½ä¿ç•™ç­–ç•¥

```bash
# ç·¨è¼¯å‚™ä»½è…³æœ¬ï¼Œæ·»åŠ ä¿ç•™ç­–ç•¥
sudo vim /etc/systemd/system/ai-platform-backup.service

# åœ¨ ExecStart ä¸­æ·»åŠ æ¸…ç†èˆŠå‚™ä»½ (ä¿ç•™ 30 å¤©)
find /opt/ai_platform/backups/ -type f -mtime +30 -delete
```

### 15.6 è¨­ç½®å‚™ä»½ç›£æ§

```bash
# å‰µå»ºå‚™ä»½ç›£æ§è…³æœ¬
cat > /opt/ai_platform/scripts/check-backup.sh << 'EOF'
#!/bin/bash
BACKUP_DIR="/opt/ai_platform/backups"
LATEST_BACKUP=$(ls -t ${BACKUP_DIR}/backup_*.tar.gz | head -1)
BACKUP_AGE=$(find ${LATEST_BACKUP} -mtime +1 | wc -l)

if [ ${BACKUP_AGE} -gt 0 ]; then
    echo "WARNING: Latest backup is older than 24 hours!"
    exit 1
else
    echo "OK: Backup is current"
    exit 0
fi
EOF

sudo chmod +x /opt/ai_platform/scripts/check-backup.sh

# æ¸¬è©¦åŸ·è¡Œ
sudo /opt/ai_platform/scripts/check-backup.sh
```

**å®Œæˆç¢ºèª:** âœ“ è‡ªå‹•å‚™ä»½å·²å•Ÿç”¨ï¼Œå‚™ä»½æ¸¬è©¦æˆåŠŸ
**é è¨ˆæ™‚é–“:** 10 åˆ†é˜

---

## éƒ¨ç½²å®Œæˆæª¢æŸ¥æ¸…å–®

### æ ¸å¿ƒæœå‹™æª¢æŸ¥

- [ ] æ‰€æœ‰ Docker å®¹å™¨éƒ½åœ¨é‹è¡Œ (`docker ps`)
- [ ] GPU é©…å‹•æ­£å¸¸ (`nvidia-smi`)
- [ ] GPU åœ¨å®¹å™¨ä¸­å¯è¨ªå• (`docker exec ollama nvidia-smi`)
- [ ] æ‰€æœ‰å¥åº·æª¢æŸ¥ç«¯é»è¿”å› 200 OK
- [ ] Ollama æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†

### ç¶²è·¯èˆ‡å®‰å…¨æª¢æŸ¥

- [ ] HTTPS æ­£å¸¸é‹ä½œ (å¦‚æœå•Ÿç”¨)
- [ ] é˜²ç«ç‰†è¦å‰‡å·²é…ç½®
- [ ] åªé–‹æ”¾å¿…è¦ç«¯å£ (80, 443, 22)
- [ ] å…§éƒ¨æœå‹™ç¶å®šåˆ° localhost
- [ ] ç’°å¢ƒè®Šæ•¸æ–‡ä»¶æ¬Šé™æ­£ç¢º (600)

### è‡ªå‹•åŒ–èˆ‡ç›£æ§æª¢æŸ¥

- [ ] Systemd æœå‹™å·²å•Ÿç”¨ (`systemctl list-units "ai-platform*"`)
- [ ] é‡å•Ÿå¾Œæœå‹™è‡ªå‹•å•Ÿå‹•
- [ ] å‚™ä»½å®šæ™‚å™¨æ­£å¸¸é‹ä½œ
- [ ] å¥åº·æª¢æŸ¥å®šæ™‚å™¨æ­£å¸¸é‹ä½œ
- [ ] Grafana å¯è¨ªå•ä¸¦é¡¯ç¤ºæ•¸æ“š
- [ ] å‘Šè­¦è¦å‰‡å·²é…ç½®

### æ•ˆèƒ½èˆ‡æ¸¬è©¦æª¢æŸ¥

- [ ] ç…™éœ§æ¸¬è©¦é€šé (100 è«‹æ±‚, éŒ¯èª¤ç‡ < 1%)
- [ ] è² è¼‰æ¸¬è©¦é€šé (50 ç”¨æˆ¶, p95 < 1s)
- [ ] å£“åŠ›æ¸¬è©¦é€šé (200 ç”¨æˆ¶, ç„¡å´©æ½°)
- [ ] GPU åˆ©ç”¨ç‡æ­£å¸¸ (> 70% åœ¨è² è¼‰æ™‚)
- [ ] è¨˜æ†¶é«”ä½¿ç”¨æ­£å¸¸ (< 85%)

### å‚™ä»½èˆ‡æ¢å¾©æª¢æŸ¥

- [ ] è‡ªå‹•å‚™ä»½å·²åŸ·è¡Œ
- [ ] å‚™ä»½æ–‡ä»¶å®Œæ•´
- [ ] å‚™ä»½é‚„åŸæ¸¬è©¦æˆåŠŸ (åœ¨æ¸¬è©¦ç’°å¢ƒ)
- [ ] é ç«¯å‚™ä»½å·²é…ç½® (å¦‚æœéœ€è¦)

---

## å¸¸è¦‹å•é¡Œæ’æŸ¥

### Q1: Docker å®¹å™¨ç„¡æ³•å•Ÿå‹•

```bash
# æª¢æŸ¥æ—¥èªŒ
sudo docker compose logs service-name

# æª¢æŸ¥è³‡æº
docker stats

# æª¢æŸ¥ç¶²è·¯
docker network inspect ai_platform_network

# é‡æ–°å»ºç«‹
sudo docker compose down
sudo docker compose up -d
```

### Q2: GPU ç„¡æ³•åœ¨å®¹å™¨ä¸­è¨ªå•

```bash
# æª¢æŸ¥é©…å‹•
nvidia-smi

# æª¢æŸ¥ NVIDIA Container Toolkit
sudo docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi

# é‡æ–°é…ç½®
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Q3: æœå‹™å•Ÿå‹•å¾Œç„¡æ³•è¨ªå•

```bash
# æª¢æŸ¥ç«¯å£ç¶å®š
sudo netstat -tlnp | grep -E ':(8000|8001|8501|3000|443)'

# æª¢æŸ¥é˜²ç«ç‰†
sudo firewall-cmd --list-all

# æª¢æŸ¥ SELinux
sudo setenforce 0  # æš«æ™‚é—œé–‰æ¸¬è©¦
```

### Q4: Ollama æ¨¡å‹æ¨ç†å¤±æ•—

```bash
# æª¢æŸ¥ GPU è¨˜æ†¶é«”
nvidia-smi

# æª¢æŸ¥ Ollama æ—¥èªŒ
sudo docker logs ai-ollama-prod -f

# é‡å•Ÿ Ollama
sudo docker restart ai-ollama-prod

# é‡æ–°æ‹‰å–æ¨¡å‹
sudo docker exec -it ai-ollama-prod ollama pull qwen2.5:7b
```

### Q5: è³‡æ–™åº«é€£ç·šå¤±æ•—

```bash
# æª¢æŸ¥ PostgreSQL ç‹€æ…‹
sudo docker exec ai-postgres-prod pg_isready

# æª¢æŸ¥é€£ç·š
sudo docker exec -it ai-postgres-prod psql -U ai_platform -c "SELECT version();"

# æŸ¥çœ‹æ—¥èªŒ
sudo docker logs ai-postgres-prod --tail=100
```

### Q6: è² è¼‰æ¸¬è©¦å¤±æ•—

```bash
# èª¿æ•´è³‡æºé™åˆ¶
vim docker-compose.production.yml
# å¢åŠ  CPU/Memory limits

# èª¿æ•´ä¸¦ç™¼æ•¸
vim .env
MAX_CONCURRENT_REQUESTS=100  # é™ä½

# é‡æ–°éƒ¨ç½²
sudo docker compose down
sudo docker compose up -d
```

---

## ç”Ÿç”¢ç’°å¢ƒç¶­è­·å»ºè­°

### æ—¥å¸¸ç¶­è­· (æ¯å¤©)

```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ai-platform

# æª¢æŸ¥ GPU ç‹€æ…‹
nvidia-smi

# æŸ¥çœ‹æœ€æ–°æ—¥èªŒ
sudo docker compose logs --tail=100 --since 24h
```

### æ¯é€±ç¶­è­·

```bash
# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
df -h

# æ¸…ç†èˆŠæ—¥èªŒ
sudo docker system prune -f

# æª¢æŸ¥å‚™ä»½
ls -lh /opt/ai_platform/backups/

# æŸ¥çœ‹ Grafana å„€è¡¨æ¿
# æª¢æŸ¥æ•ˆèƒ½è¶¨å‹¢
```

### æ¯æœˆç¶­è­·

```bash
# æ›´æ–° Docker æ˜ åƒ
sudo docker compose pull
sudo docker compose up -d

# åŸ·è¡Œè² è¼‰æ¸¬è©¦
cd /opt/ai_platform/load-tests
./test-api-endpoints.sh

# æª¢æŸ¥å®‰å…¨æ›´æ–°
sudo dnf check-update

# å¯©æŸ¥å‘Šè­¦æ—¥èªŒ
sudo journalctl -u ai-platform --since "30 days ago" | grep -i error
```

### å®‰å…¨æ›´æ–°

```bash
# å®šæœŸæ›´æ–°ç³»çµ±
sudo dnf update -y

# æ›´æ–° NVIDIA é©…å‹• (è¬¹æ…åŸ·è¡Œ)
sudo dnf update nvidia-driver-latest-dkms

# æ›´æ–° Docker
sudo dnf update docker-ce docker-compose-plugin
```

---

## ç·Šæ€¥å›å¾©ç¨‹åº

### å®Œå…¨é‡æ–°éƒ¨ç½²

```bash
# 1. åœæ­¢æ‰€æœ‰æœå‹™
sudo systemctl stop ai-platform
sudo docker compose down

# 2. æ¸…é™¤æ‰€æœ‰å®¹å™¨å’Œå· (è­¦å‘Š: æœƒåˆªé™¤æ•¸æ“š)
sudo docker compose down -v
sudo docker system prune -a -f

# 3. å¾å‚™ä»½é‚„åŸé…ç½®
sudo tar -xzf /opt/ai_platform/backups/backup_YYYYMMDD.tar.gz -C /opt/ai_platform/

# 4. é‚„åŸè³‡æ–™åº«
sudo docker compose up -d postgres
sleep 10
sudo docker exec -i ai-postgres-prod psql -U ai_platform < backups/postgres_YYYYMMDD.sql

# 5. é‡æ–°éƒ¨ç½²
sudo ./deploy-rhel-production.sh
```

---

## è¯çµ¡è³‡è¨Šèˆ‡æ”¯æ´

### éƒ¨ç½²æ”¯æ´

- **æŠ€è¡“æ–‡æª”:** `/opt/ai_platform/PRODUCTION_DEPLOYMENT.md`
- **æ•…éšœæ’é™¤:** `/opt/ai_platform/TROUBLESHOOTING_GUIDE.md`
- **è² è¼‰æ¸¬è©¦:** `/opt/ai_platform/load-tests/README.md`

### ç³»çµ±ç®¡ç†

```bash
# å¿«é€Ÿç‹€æ…‹æª¢æŸ¥è…³æœ¬
cat > /usr/local/bin/ai-platform-status << 'EOF'
#!/bin/bash
echo "=== AI Platform Status ==="
echo ""
echo "System Services:"
systemctl is-active ai-platform
echo ""
echo "Docker Containers:"
docker ps --format "table {{.Names}}\t{{.Status}}" | grep ai-
echo ""
echo "GPU Status:"
nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,memory.used --format=csv,noheader
echo ""
echo "Disk Usage:"
df -h /opt/ai_platform
EOF

sudo chmod +x /usr/local/bin/ai-platform-status

# ä½¿ç”¨: ai-platform-status
```

---

## ç¸½çµ

æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆ AI Platform çš„ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²ï¼

### å·²å®Œæˆçš„è¨­ç½®

âœ… NVIDIA H100 GPU é©…å‹•èˆ‡ CUDA
âœ… Docker èˆ‡ NVIDIA Container Toolkit
âœ… é›™ GPU åŠ é€Ÿçš„ AI æœå‹™
âœ… é«˜å¯ç”¨æ€§æœå‹™å‰¯æœ¬
âœ… Systemd è‡ªå‹•å•Ÿå‹•
âœ… è‡ªå‹•å‚™ä»½èˆ‡å¥åº·æª¢æŸ¥
âœ… Grafana ç›£æ§å„€è¡¨æ¿
âœ… è² è¼‰æ¸¬è©¦é©—è­‰
âœ… é˜²ç«ç‰†èˆ‡å®‰å…¨åŠ å›º
âœ… SSL/TLS åŠ å¯† (å¦‚æœå•Ÿç”¨)

### ä¸‹ä¸€æ­¥å»ºè­°

1. **ç›£æ§é‹è¡Œ 7 å¤©**ï¼Œè§€å¯Ÿç©©å®šæ€§
2. **é…ç½®å‘Šè­¦é€šçŸ¥**åˆ° Slack/Email
3. **åŸ·è¡Œå®šæœŸè² è¼‰æ¸¬è©¦**
4. **å»ºç«‹é‹ç¶­æ–‡æª”**è¨˜éŒ„å¸¸è¦‹å•é¡Œ
5. **è¨­ç½®ç•°åœ°å‚™ä»½**
6. **è¦åŠƒæ“´å±•æ–¹æ¡ˆ** (éœ€è¦æ™‚å¢åŠ ç¯€é»)

### é‡è¦æé†’

âš ï¸ å®šæœŸæª¢æŸ¥ GPU æº«åº¦ (< 85Â°C)
âš ï¸ ç›£æ§ç£ç¢Ÿç©ºé–“ (ä¿æŒ > 20% å¯ç”¨)
âš ï¸ æ¯é€±æª¢æŸ¥å‚™ä»½å®Œæ•´æ€§
âš ï¸ ä¿æŒ NVIDIA é©…å‹•æ›´æ–°
âš ï¸ å®šæœŸå¯©æŸ¥å®‰å…¨æ—¥èªŒ

---

**éƒ¨ç½²å®Œæˆï¼ç³»çµ±å·²æº–å‚™å¥½è™•ç†ç”Ÿç”¢æµé‡ï¼** ğŸš€

**ç‰ˆæœ¬:** 2.0.0
**æœ€å¾Œæ›´æ–°:** 2025-10-29
**æ–‡æª”ç¶­è­·:** AI Platform DevOps Team
