# Red Hat Enterprise Linux v9 éƒ¨ç½²æŒ‡å—
## AI Platform with 2x Nvidia H100 GPUs

**ç›®æ¨™ç’°å¢ƒ**:
- Red Hat Enterprise Linux v9
- 2x Nvidia H100 80GB GPUs
- CUDA 12.2+
- Docker with GPU support

---

## ğŸ“‹ ç›®éŒ„

1. [ç³»çµ±éœ€æ±‚](#ç³»çµ±éœ€æ±‚)
2. [å‰ç½®æº–å‚™](#å‰ç½®æº–å‚™)
3. [å®‰è£æ­¥é©Ÿ](#å®‰è£æ­¥é©Ÿ)
4. [GPU é…ç½®é©—è­‰](#gpu-é…ç½®é©—è­‰)
5. [éƒ¨ç½²å¹³å°](#éƒ¨ç½²å¹³å°)
6. [ç›£æ§èˆ‡ç¶­è­·](#ç›£æ§èˆ‡ç¶­è­·)
7. [ç–‘é›£æ’è§£](#ç–‘é›£æ’è§£)
8. [å®‰å…¨å»ºè­°](#å®‰å…¨å»ºè­°)

---

## ç³»çµ±éœ€æ±‚

### ç¡¬é«”éœ€æ±‚

- **CPU**: 16+ cores (å»ºè­° 32+)
- **RAM**: 128GB+ (å»ºè­° 256GB)
- **GPU**: 2x Nvidia H100 80GB
- **å„²å­˜**:
  - ç³»çµ±ç¢Ÿ: 500GB+ SSD
  - è³‡æ–™ç¢Ÿ: 2TB+ NVMe SSD
  - æ¨¡å‹å¿«å–: 500GB+ (å¯é¸)

### è»Ÿé«”éœ€æ±‚

- **ä½œæ¥­ç³»çµ±**: Red Hat Enterprise Linux 9.x
- **Kernel**: 5.14+
- **Docker**: 24.0+
- **Docker Compose**: 2.20+
- **NVIDIA Driver**: 535+ (æ”¯æ´ CUDA 12.2)
- **NVIDIA Container Toolkit**: Latest
- **CUDA**: 12.2+

---

## å‰ç½®æº–å‚™

### 1. æ›´æ–°ç³»çµ±

```bash
# ä»¥ root æˆ– sudo åŸ·è¡Œ
sudo dnf update -y
sudo dnf install -y epel-release
```

### 2. å®‰è£åŸºæœ¬å·¥å…·

```bash
sudo dnf install -y \
    curl \
    wget \
    git \
    vim \
    htop \
    iotop \
    nethogs \
    tmux \
    jq \
    python3 \
    python3-pip
```

### 3. å®‰è£ NVIDIA Driver

```bash
# æ·»åŠ  NVIDIA å®˜æ–¹ repo
sudo dnf config-manager --add-repo \
    https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

# å®‰è£ NVIDIA driver
sudo dnf module install -y nvidia-driver:latest-dkms

# å®‰è£ CUDA toolkit
sudo dnf install -y cuda-toolkit-12-2

# é‡å•Ÿç³»çµ±
sudo reboot
```

### 4. é©—è­‰ GPU é©…å‹•

```bash
# æª¢æŸ¥ GPU ç‹€æ…‹
nvidia-smi

# é æœŸè¼¸å‡ºæ‡‰é¡¯ç¤º 2 å¼µ H100 GPU
# GPU 0: NVIDIA H100 80GB
# GPU 1: NVIDIA H100 80GB

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version
# æ‡‰é¡¯ç¤º CUDA 12.2 æˆ–æ›´é«˜
```

### 5. å®‰è£ Docker

```bash
# æ·»åŠ  Docker CE repository
sudo dnf config-manager --add-repo \
    https://download.docker.com/linux/rhel/docker-ce.repo

# å®‰è£ Docker
sudo dnf install -y \
    docker-ce \
    docker-ce-cli \
    containerd.io \
    docker-buildx-plugin \
    docker-compose-plugin

# å•Ÿå‹• Docker æœå‹™
sudo systemctl start docker
sudo systemctl enable docker

# é©—è­‰ Docker å®‰è£
docker --version
docker compose version

# å°‡ç•¶å‰ç”¨æˆ¶åŠ å…¥ docker ç¾¤çµ„ï¼ˆé¿å…æ¯æ¬¡ sudoï¼‰
sudo usermod -aG docker $USER
# é‡æ–°ç™»å…¥æˆ–åŸ·è¡Œ: newgrp docker
```

### 6. å®‰è£ NVIDIA Container Toolkit

```bash
# è¨­å®š repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
    sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

# å®‰è£ NVIDIA Container Toolkit
sudo dnf install -y nvidia-container-toolkit

# é…ç½® Docker runtime
sudo nvidia-ctk runtime configure --runtime=docker

# é‡å•Ÿ Docker
sudo systemctl restart docker
```

---

## GPU é…ç½®é©—è­‰

### æ¸¬è©¦ GPU åœ¨ Docker ä¸­çš„å¯ç”¨æ€§

```bash
# æ¸¬è©¦ GPU è¨ªå•
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

# æ¸¬è©¦æŒ‡å®š GPU
docker run --rm --gpus '"device=0"' nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
docker run --rm --gpus '"device=1"' nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

# æ¸¬è©¦ CUDA é‹ç®—
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 \
    /usr/local/cuda/samples/bin/x86_64/linux/release/deviceQuery
```

**é æœŸçµæœ**:
- æ‡‰è©²çœ‹åˆ°å…©å¼µ H100 GPU
- CUDA Version æ‡‰ç‚º 12.2 æˆ–æ›´é«˜
- æ‰€æœ‰æ¸¬è©¦æ‡‰è©² PASS

---

## éƒ¨ç½²å¹³å°

### 1. å…‹éš†å°ˆæ¡ˆ

```bash
# å‰µå»ºéƒ¨ç½²ç›®éŒ„
sudo mkdir -p /opt/ai-platform
sudo chown $USER:$USER /opt/ai-platform
cd /opt/ai-platform

# å…‹éš†å°ˆæ¡ˆ (å‡è¨­ä½¿ç”¨ Git)
git clone <YOUR_REPO_URL> .

# æˆ–ç›´æ¥è¤‡è£½æª”æ¡ˆåˆ°æ­¤ç›®éŒ„
```

### 2. é…ç½®ç’°å¢ƒè®Šæ•¸

```bash
# è¤‡è£½ç’°å¢ƒè®Šæ•¸ç¯„æœ¬
cp .env.prod.template .env.prod

# ç·¨è¼¯ç’°å¢ƒè®Šæ•¸
vim .env.prod

# å¿…é ˆé…ç½®çš„é …ç›®ï¼š
# - POSTGRES_PASSWORD
# - REDIS_PASSWORD
# - RABBITMQ_DEFAULT_PASS
# - OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY
# - LITELLM_MASTER_KEY
# - GRAFANA_ADMIN_PASSWORD
```

**å®‰å…¨å»ºè­°**:
```bash
# è¨­å®šé©ç•¶çš„æª”æ¡ˆæ¬Šé™
chmod 600 .env.prod
chown $USER:$USER .env.prod

# ç¢ºä¿ .env.prod ä¸åœ¨ç‰ˆæœ¬æ§åˆ¶ä¸­
echo ".env.prod" >> .gitignore
```

### 3. å»ºæ§‹ Docker æ˜ åƒæª”

```bash
# ä½¿ç”¨ç”Ÿç”¢ç’°å¢ƒé…ç½®
export COMPOSE_FILE=docker-compose.prod.yml

# å»ºæ§‹æ‰€æœ‰æœå‹™
docker compose build --no-cache

# æŸ¥çœ‹å»ºæ§‹çš„æ˜ åƒæª”
docker images | grep ai_platform
```

**é æœŸå»ºæ§‹æ™‚é–“**:
- MCP Server (å« GPU): 20-30 åˆ†é˜ (é¦–æ¬¡ï¼Œéœ€ä¸‹è¼‰ CUDA ç›¸é—œå¥—ä»¶)
- Agent Service: 5-10 åˆ†é˜
- Web UI: 5-10 åˆ†é˜

### 4. åˆå§‹åŒ–è³‡æ–™åº«

```bash
# å…ˆå•Ÿå‹•è³‡æ–™åº«æœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml up -d postgres redis qdrant

# ç­‰å¾…è³‡æ–™åº«å°±ç·’
sleep 10

# æª¢æŸ¥è³‡æ–™åº«ç‹€æ…‹
docker compose --env-file .env.prod -f docker-compose.prod.yml ps postgres
```

### 5. å•Ÿå‹•æ‰€æœ‰æœå‹™

```bash
# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml up -d

# æŸ¥çœ‹æœå‹™ç‹€æ…‹
docker compose --env-file .env.prod -f docker-compose.prod.yml ps

# æŸ¥çœ‹æ—¥èªŒ
docker compose --env-file .env.prod -f docker-compose.prod.yml logs -f

# æŸ¥çœ‹ç‰¹å®šæœå‹™æ—¥èªŒ
docker compose --env-file .env.prod -f docker-compose.prod.yml logs -f mcp-server
```

### 6. é©—è­‰ GPU ä½¿ç”¨æƒ…æ³

```bash
# ç›£æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# é æœŸçœ‹åˆ°ï¼š
# GPU 0: Ollama (è™•ç†æœ¬åœ° LLM æ¨ç†)
# GPU 1: MCP Server (è™•ç† DeepSeek-OCR)

# æª¢æŸ¥å®¹å™¨ GPU é…ç½®
docker inspect ai-mcp-server-prod | jq '.[0].HostConfig.DeviceRequests'
docker inspect ai-ollama-prod | jq '.[0].HostConfig.DeviceRequests'
```

### 7. é©—è­‰æœå‹™å¥åº·ç‹€æ…‹

```bash
# æª¢æŸ¥æ‰€æœ‰æœå‹™å¥åº·ç‹€æ…‹
docker compose --env-file .env.prod -f docker-compose.prod.yml ps

# ä½¿ç”¨å¥åº·æª¢æŸ¥è…³æœ¬ï¼ˆå‰µå»ºä»¥ä¸‹è…³æœ¬ï¼‰
cat > check_health.sh << 'EOF'
#!/bin/bash
echo "=== AI Platform Health Check ==="
echo ""

services=(
    "http://localhost:8501/_stcore/health|Web UI"
    "http://localhost:8002/health|Agent Service"
    "http://localhost:8001/health|MCP Server"
    "http://localhost:4000/health|LiteLLM"
    "http://localhost:9090/-/healthy|Prometheus"
    "http://localhost:3000/api/health|Grafana"
)

for service in "${services[@]}"; do
    IFS='|' read -r url name <<< "$service"
    if curl -sf "$url" > /dev/null 2>&1; then
        echo "âœ… $name: Healthy"
    else
        echo "âŒ $name: Unhealthy"
    fi
done

echo ""
echo "=== GPU Status ==="
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv
EOF

chmod +x check_health.sh
./check_health.sh
```

### 8. æ¸¬è©¦ OCR åŠŸèƒ½

```bash
# æ¸¬è©¦ OCR ç‹€æ…‹
curl -s http://localhost:8001/tools/ocr_get_status | jq .

# é æœŸè¼¸å‡ºæ‡‰åŒ…å«ï¼š
# {
#   "ocr_available": true,
#   "backends": [
#     {
#       "name": "EasyOCR",
#       "type": "cpu",
#       "available": true
#     },
#     {
#       "name": "DeepSeek-OCR",
#       "type": "gpu",
#       "available": true,    <-- æ‡‰ç‚º true
#       "cuda_available": true <-- æ‡‰ç‚º true
#     }
#   ]
# }
```

---

## ç›£æ§èˆ‡ç¶­è­·

### 1. ç³»çµ±ç›£æ§

**Prometheus + Grafana**:
```bash
# è¨ªå• Grafana
http://<SERVER_IP>:3000

# é è¨­å¸³è™Ÿ: åœ¨ .env.prod ä¸­è¨­å®šçš„ GRAFANA_ADMIN_USER/PASSWORD
```

**GPU ç›£æ§**:
```bash
# æŒçºŒç›£æ§ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹ GPU è©³ç´°è³‡è¨Š
nvidia-smi -l 1

# è¨˜éŒ„ GPU ä½¿ç”¨åˆ°æª”æ¡ˆ
nvidia-smi --query-gpu=timestamp,name,pci.bus_id,driver_version,pstate,pcie.link.gen.max,pcie.link.gen.current,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 5 > gpu_log.csv
```

### 2. æ—¥èªŒç®¡ç†

```bash
# æŸ¥çœ‹æœå‹™æ—¥èªŒ
docker compose --env-file .env.prod -f docker-compose.prod.yml logs -f --tail=100 <service_name>

# æ—¥èªŒä½ç½®ï¼ˆJSON æ ¼å¼ï¼‰
# æ‰€æœ‰å®¹å™¨æ—¥èªŒåœ¨: /var/lib/docker/containers/<container_id>/<container_id>-json.log

# æ¸…ç†èˆŠæ—¥èªŒï¼ˆè¬¹æ…ä½¿ç”¨ï¼‰
docker system prune -a --volumes
```

### 3. å‚™ä»½ç­–ç•¥

```bash
# å‚™ä»½è³‡æ–™åº«
docker exec ai-postgres-prod pg_dump -U ai_platform_user ai_platform_prod > backup_$(date +%Y%m%d).sql

# å‚™ä»½ Docker Volumes
docker run --rm \
    -v ai_platform_postgres_data:/data \
    -v $(pwd)/backups:/backup \
    alpine tar czf /backup/postgres_data_$(date +%Y%m%d).tar.gz /data

# å‚™ä»½é…ç½®æª”æ¡ˆ
tar czf config_backup_$(date +%Y%m%d).tar.gz \
    .env.prod \
    config/ \
    docker-compose.prod.yml
```

### 4. å®šæœŸç¶­è­·

**æ¯æ—¥**:
- æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹: `./check_health.sh`
- ç›£æ§ GPU ä½¿ç”¨ç‡
- æª¢æŸ¥æ—¥èªŒéŒ¯èª¤

**æ¯é€±**:
- æª¢æŸ¥ç£ç¢Ÿç©ºé–“: `df -h`
- æª¢æŸ¥ Docker æ˜ åƒæª”å¤§å°: `docker system df`
- æ¸…ç†æœªä½¿ç”¨çš„æ˜ åƒæª”: `docker image prune -a`

**æ¯æœˆ**:
- æ›´æ–°ç³»çµ±å¥—ä»¶: `sudo dnf update -y`
- è¼ªæ›¿å¯†é‘°å’Œæ†‘è­‰
- å®Œæ•´å‚™ä»½

---

## ç–‘é›£æ’è§£

### å•é¡Œ 1: GPU ç„¡æ³•è¢« Docker è­˜åˆ¥

**ç—‡ç‹€**:
```bash
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# é‡æ–°é…ç½® NVIDIA Container Toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# é©—è­‰é…ç½®
cat /etc/docker/daemon.json
# æ‡‰åŒ…å« "nvidia" runtime
```

### å•é¡Œ 2: DeepSeek-OCR æœªä½¿ç”¨ GPU

**æª¢æŸ¥**:
```bash
# æŸ¥çœ‹ MCP Server æ—¥èªŒ
docker compose logs mcp-server | grep -i "cuda\|gpu"

# é€²å…¥å®¹å™¨æª¢æŸ¥
docker exec -it ai-mcp-server-prod bash
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python3 -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# ç¢ºèªç’°å¢ƒè®Šæ•¸
docker exec ai-mcp-server-prod env | grep -i cuda

# é‡å•Ÿæœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml restart mcp-server
```

### å•é¡Œ 3: è¨˜æ†¶é«”ä¸è¶³ (OOM)

**ç—‡ç‹€**:
```bash
# å®¹å™¨è¢« killed
docker logs ai-mcp-server-prod | tail -20
# çœ‹åˆ°: Killed
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”
free -h

# èª¿æ•´ Docker Compose è³‡æºé™åˆ¶
# ç·¨è¼¯ docker-compose.prod.yml ä¸­çš„ deploy.resources.limits
```

### å•é¡Œ 4: SELinux é˜»æ­¢å®¹å™¨å•Ÿå‹•

**ç—‡ç‹€**:
```bash
# å®¹å™¨ç„¡æ³•è¨ªå•æ›è¼‰çš„ volumes
Permission denied
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ 1: æš«æ™‚é—œé–‰ SELinuxï¼ˆä¸å»ºè­°ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼‰
sudo setenforce 0

# æ–¹æ¡ˆ 2: é…ç½® SELinux contextï¼ˆå»ºè­°ï¼‰
sudo chcon -R -t container_file_t /opt/ai-platform/config
sudo chcon -R -t container_file_t /var/lib/docker/volumes

# æ–¹æ¡ˆ 3: ä½¿ç”¨ :z æˆ– :Z volume æ¨™è¨˜
# åœ¨ docker-compose.prod.yml ä¸­ï¼š
# volumes:
#   - ./config:/app/config:z
```

### å•é¡Œ 5: ç¶²è·¯é€£æ¥å•é¡Œ

**æª¢æŸ¥**:
```bash
# æª¢æŸ¥ Docker ç¶²è·¯
docker network ls
docker network inspect ai_platform_ai-platform

# æ¸¬è©¦å®¹å™¨é–“é€£æ¥
docker exec ai-web-ui-prod curl -v http://agent-service:8000/health
```

---

## å®‰å…¨å»ºè­°

### 1. é˜²ç«ç‰†é…ç½®

```bash
# ä½¿ç”¨ firewalld (RHEL 9 é è¨­)
sudo systemctl start firewalld
sudo systemctl enable firewalld

# åªé–‹æ”¾å¿…è¦çš„ç«¯å£ï¼ˆæ ¹æ“šéœ€æ±‚èª¿æ•´ï¼‰
sudo firewall-cmd --permanent --add-port=8501/tcp  # Web UI
sudo firewall-cmd --permanent --add-port=8001/tcp  # MCP Server (å¦‚éœ€å¤–éƒ¨è¨ªå•)
sudo firewall-cmd --permanent --add-port=8002/tcp  # Agent Service (å¦‚éœ€å¤–éƒ¨è¨ªå•)
sudo firewall-cmd --permanent --add-port=22/tcp    # SSH

# é‡æ–°è¼‰å…¥é…ç½®
sudo firewall-cmd --reload

# æŸ¥çœ‹é–‹æ”¾çš„ç«¯å£
sudo firewall-cmd --list-all
```

### 2. SSL/TLS é…ç½®

**å»ºè­°ä½¿ç”¨ Nginx åå‘ä»£ç†**:

```bash
# å®‰è£ Nginx
sudo dnf install -y nginx certbot python3-certbot-nginx

# é…ç½®åå‘ä»£ç†
sudo vim /etc/nginx/conf.d/ai-platform.conf
```

ç¯„ä¾‹ Nginx é…ç½®:
```nginx
upstream web_ui {
    server localhost:8501;
}

server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;

    ssl_certificate /etc/letsencrypt/live/your-domain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/your-domain.com/privkey.pem;

    location / {
        proxy_pass http://web_ui;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 3. å®šæœŸå®‰å…¨æ›´æ–°

```bash
# è¨­å®šè‡ªå‹•å®‰å…¨æ›´æ–°
sudo dnf install -y dnf-automatic
sudo systemctl enable --now dnf-automatic.timer

# é…ç½®æ›´æ–°ç­–ç•¥
sudo vim /etc/dnf/automatic.conf
# è¨­å®š: apply_updates = yes (åƒ…å®‰å…¨æ›´æ–°)
```

### 4. å¯©è¨ˆèˆ‡æ—¥èªŒ

```bash
# å•Ÿç”¨ auditd
sudo systemctl start auditd
sudo systemctl enable auditd

# ç›£æ§ Docker äº‹ä»¶
docker events &

# è¨­å®šæ—¥èªŒè¼ªæ›¿
sudo vim /etc/logrotate.d/ai-platform
```

---

## æ•ˆèƒ½èª¿æ ¡

### 1. CUDA æœ€ä½³åŒ–

```bash
# è¨­å®š CUDA å¿«å–
export CUDA_CACHE_PATH=/var/cache/cuda
sudo mkdir -p $CUDA_CACHE_PATH
sudo chmod 777 $CUDA_CACHE_PATH

# è¨­å®šæŒä¹…åŒ–æ¨¡å¼ (æå‡ GPU æ•ˆèƒ½)
sudo nvidia-smi -pm 1

# è¨­å®š GPU æ™‚è„ˆ (H100 é è¨­å·²å„ªåŒ–ï¼Œå¯é¸)
sudo nvidia-smi -lgc 1980  # è¨­å®šæœ€å¤§æ™‚è„ˆ
```

### 2. Docker æ•ˆèƒ½èª¿æ ¡

```bash
# ç·¨è¼¯ Docker daemon é…ç½®
sudo vim /etc/docker/daemon.json
```

```json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "50m",
    "max-file": "5"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "default-ulimits": {
    "nofile": {
      "Name": "nofile",
      "Hard": 64000,
      "Soft": 64000
    }
  }
}
```

### 3. ç³»çµ±åƒæ•¸èª¿æ ¡

```bash
# ç·¨è¼¯ sysctl
sudo vim /etc/sysctl.d/99-ai-platform.conf
```

```conf
# ç¶²è·¯å„ªåŒ–
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 8192
net.core.netdev_max_backlog = 5000

# è¨˜æ†¶é«”å„ªåŒ–
vm.swappiness = 10
vm.dirty_ratio = 40
vm.dirty_background_ratio = 10

# æª”æ¡ˆæè¿°ç¬¦é™åˆ¶
fs.file-max = 2097152
```

æ‡‰ç”¨é…ç½®:
```bash
sudo sysctl -p /etc/sysctl.d/99-ai-platform.conf
```

---

## å¿«é€Ÿåƒè€ƒæŒ‡ä»¤

### æœå‹™ç®¡ç†

```bash
# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml up -d

# åœæ­¢æ‰€æœ‰æœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml down

# é‡å•Ÿç‰¹å®šæœå‹™
docker compose --env-file .env.prod -f docker-compose.prod.yml restart <service>

# æŸ¥çœ‹æ—¥èªŒ
docker compose --env-file .env.prod -f docker-compose.prod.yml logs -f <service>

# é€²å…¥å®¹å™¨
docker exec -it <container_name> bash
```

### ç›£æ§æŒ‡ä»¤

```bash
# GPU ç›£æ§
nvidia-smi -l 1

# å®¹å™¨è³‡æºä½¿ç”¨
docker stats

# ç³»çµ±è³‡æº
htop
iotop
nethogs
```

### ç¶­è­·æŒ‡ä»¤

```bash
# æ›´æ–°æ˜ åƒæª”
docker compose --env-file .env.prod -f docker-compose.prod.yml pull
docker compose --env-file .env.prod -f docker-compose.prod.yml up -d

# æ¸…ç†
docker system prune -a
docker volume prune

# å‚™ä»½
./backup.sh

# å¥åº·æª¢æŸ¥
./check_health.sh
```

---

## é™„éŒ„

### A. ç³»çµ±éœ€æ±‚æª¢æŸ¥æ¸…å–®

- [ ] RHEL 9 å·²å®‰è£ä¸¦æ›´æ–°
- [ ] 2x Nvidia H100 GPU å·²è­˜åˆ¥
- [ ] NVIDIA Driver 535+ å·²å®‰è£
- [ ] CUDA 12.2+ å·²å®‰è£
- [ ] Docker 24.0+ å·²å®‰è£
- [ ] Docker Compose 2.20+ å·²å®‰è£
- [ ] NVIDIA Container Toolkit å·²å®‰è£
- [ ] GPU åœ¨ Docker ä¸­å¯ç”¨
- [ ] ç’°å¢ƒè®Šæ•¸å·²é…ç½®
- [ ] é˜²ç«ç‰†å·²é…ç½®
- [ ] SSL æ†‘è­‰å·²è¨­å®šï¼ˆå¦‚éœ€ï¼‰

### B. æ•ˆèƒ½åŸºæº–

**H100 GPU æ•ˆèƒ½**:
- DeepSeek-OCR: ~0.5-1 ç§’/é  (GPU)
- è¨˜æ†¶é«”ä½¿ç”¨: ~10-15 GB VRAM
- ä¸¦ç™¼è™•ç†: æ”¯æ´å¤šå€‹è«‹æ±‚

**ç³»çµ±æ•´é«”æ•ˆèƒ½**:
- Web UI å›æ‡‰æ™‚é–“: < 200ms
- API å›æ‡‰æ™‚é–“: < 500ms
- OCR è™•ç†æ™‚é–“: 0.5-2 ç§’/é 
- Contract Review: 5-10 ç§’/å¥‘ç´„

### C. æ”¯æ´è³‡æº

- **å°ˆæ¡ˆæ–‡æª”**: PROJECT_OVERVIEW.md, AI_ASSISTANT_GUIDE.md
- **OCR æ–‡æª”**: OCR_TESTING_GUIDE.md, AGENT_OCR_USAGE.md
- **å¹³å°ç‹€æ…‹**: PLATFORM_STATUS.md
- **NVIDIA æ–‡æª”**: https://docs.nvidia.com/datacenter/tesla/
- **Docker GPU æ–‡æª”**: https://docs.docker.com/config/containers/resource_constraints/#gpu

---

**éƒ¨ç½²å®Œæˆå¾Œï¼Œè«‹è¨ªå•**: http://<SERVER_IP>:8501

**é è¨­æœå‹™ç«¯å£**:
- Web UI: 8501
- Agent Service: 8002
- MCP Server: 8001
- LiteLLM: 4000
- Grafana: 3000
- Prometheus: 9090

**ç¥éƒ¨ç½²é †åˆ©ï¼** ğŸš€
