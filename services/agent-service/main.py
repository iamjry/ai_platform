from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional
import aiohttp
import os
import logging
import httpx
import json
from prometheus_fastapi_instrumentator import Instrumentator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Agent Service", version="1.0.0")

# Setup Prometheus metrics
Instrumentator().instrument(app).expose(app)

# é…ç½®
LLM_PROXY_URL = os.getenv("LLM_PROXY_URL", "http://litellm:4000")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")
LITELLM_API_KEY = os.getenv("LITELLM_API_KEY", "sk-1234")

class AgentRequest(BaseModel):
    task: str
    context: Optional[Dict] = None
    agent_type: str = "general"
    model: str = "qwen2.5"  # Default to local model
    conversation_history: Optional[List[Dict]] = None  # For multi-stage conversations
    images: Optional[List[Dict]] = None  # For vision models (base64 encoded images)
    temperature: float = 0.7  # Sampling temperature
    top_p: float = 0.9  # Nucleus sampling
    top_k: int = 40  # Top-k sampling

class AgentResponse(BaseModel):
    result: str
    steps: List[Dict]
    metadata: Dict
    needs_more_info: bool = False  # Indicates if agent needs more information
    missing_parameters: Optional[List[str]] = None  # What information is missing

class ChatRequest(BaseModel):
    message: str
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7

class ChatResponse(BaseModel):
    response: str
    model: str

@app.get("/health")
async def health_check():
    health_status = {
        "status": "healthy",
        "services": {}
    }
    
    # æª¢æŸ¥LLMæœå‹™
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{LLM_PROXY_URL}/health/readiness", timeout=5.0)
            if resp.status_code == 200:
                health_status["services"]["llm"] = "connected"
            else:
                health_status["services"]["llm"] = "unavailable"
                health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["llm"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    # æª¢æŸ¥MCPæœå‹™
    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{MCP_SERVER_URL}/health", timeout=5.0)
            if resp.status_code == 200:
                health_status["services"]["mcp"] = "connected"
            else:
                health_status["services"]["mcp"] = "unavailable"
                health_status["status"] = "degraded"
    except Exception as e:
        health_status["services"]["mcp"] = f"error: {str(e)}"
        health_status["status"] = "degraded"
    
    return health_status

def convert_tools_to_functions(mcp_tools: List[Dict]) -> List[Dict]:
    """Convert MCP tools to OpenAI function calling format"""
    functions = []
    for tool in mcp_tools:
        # Create properties from parameters
        properties = {}
        required = []

        # Map of parameter types to valid JSON Schema types
        type_mapping = {
            "datetime": "string",  # Claude doesn't support datetime type
            "float": "number",      # Map float to number for consistency
        }

        for param_name, param_type in tool.get("parameters", {}).items():
            # Convert invalid types to valid JSON Schema types
            mapped_type = type_mapping.get(param_type, param_type)
            properties[param_name] = {"type": mapped_type}

            # Add description for datetime fields
            if param_type == "datetime":
                properties[param_name]["description"] = "ISO 8601 datetime string"

            # Make certain parameters required
            if param_name in ["query", "to", "subject", "body", "title", "message"]:
                required.append(param_name)

        function_def = {
            "name": tool["name"],
            "description": tool["description"],
            "parameters": {
                "type": "object",
                "properties": properties,
                "required": required
            }
        }
        functions.append(function_def)

    return functions

async def call_mcp_tool(tool_name: str, arguments: Dict) -> Dict:
    """Call an MCP server tool"""
    # Map tool names to MCP endpoints
    endpoint_map = {
        "search_knowledge_base": "/tools/search",
        "get_document": lambda args: f"/resources/document/{args.get('document_id')}",
        "analyze_data": "/tools/analyze_data",
        "generate_chart": "/tools/generate_chart",
        "process_csv": "/tools/process_csv",
        "semantic_search": "/tools/semantic_search",
        "web_search": "/tools/web_search",
        "find_similar_documents": lambda args: f"/tools/find_similar_documents/{args.get('document_id')}",
        "summarize_document": "/tools/summarize_document",
        "translate_text": "/tools/translate_text",
        "generate_report": "/tools/generate_report",
        "check_permissions": "/tools/check_permissions",
        "audit_log": "/tools/audit_log",
        "scan_sensitive_data": "/tools/scan_sensitive_data",
        "create_task": "/tools/create_task",
        "send_notification": "/tools/send_notification",
        "schedule_meeting": "/tools/schedule_meeting",
        "call_api": "/tools/call_api",
        "execute_sql": "/tools/execute_sql",
        "run_script": "/tools/run_script",
        "send_email": "/tools/send_email",
        "create_slack_message": "/tools/create_slack_message",
        "upload_file": "/tools/upload_file",
        "download_file": "/tools/download_file",
        "list_files": "/tools/list_files",
        "calculate_metrics": "/tools/calculate_metrics",
        "financial_calculator": "/tools/financial_calculator",
    }

    endpoint = endpoint_map.get(tool_name)
    if not endpoint:
        raise ValueError(f"Unknown tool: {tool_name}")

    # Handle dynamic endpoints (lambdas)
    if callable(endpoint):
        endpoint = endpoint(arguments)
        method = "GET"
    else:
        method = "POST" if not endpoint.startswith("/resources/") else "GET"

    async with httpx.AsyncClient() as client:
        if method == "GET":
            response = await client.get(f"{MCP_SERVER_URL}{endpoint}", timeout=30.0)
        else:
            response = await client.post(f"{MCP_SERVER_URL}{endpoint}", json=arguments, timeout=30.0)

        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=response.text)

        return response.json()

def detect_tool_intent(task: str) -> Optional[tuple]:
    """Fallback: Detect tool intent from user message when function calling not supported"""
    import re
    task_lower = task.lower()

    # First, check if there's an email address in the text
    # This helps with context-based detection like "contact John at jerry@email.com"
    emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', task)

    # Email sending patterns - now includes context-based detection
    email_keywords = [
        "ç™¼é€éƒµä»¶", "å‘é€é‚®ä»¶", "send email", "å¯„ä¿¡", "å‚³é€email",
        "å¯«ä¸€å°ä¿¡", "å†™ä¸€å°ä¿¡", "write email", "email to", "mail to",
        "å¯„email", "é€ä¿¡", "å¹«æˆ‘å¯«ä¿¡", "å¸®æˆ‘å†™ä¿¡", "send mail",
        "ç™¼email", "å‘email"
    ]

    # Context-based email indicators (even without direct "send" words)
    context_indicators = [
        "contact", "è¯çµ¡", "è”ç»œ", "é€šçŸ¥", "å‘ŠçŸ¥", "å‘Šè¯‰", "inform",
        "reach out", "get in touch", "let them know", "å‘Šè¨´",
        "å•å€™", "é—®å€™", "ç¥ç¦", "é—œå¿ƒ", "å…³å¿ƒ"
    ]

    has_email_keyword = any(keyword in task_lower for keyword in email_keywords)
    has_context_indicator = any(indicator in task_lower for indicator in context_indicators)

    # Trigger email if: explicit keyword OR (email address + context indicator)
    if has_email_keyword or (emails and has_context_indicator):
        if emails:
            # Try to extract subject and body
            subject = "ä¾†è‡ªAIåŠ©æ‰‹çš„è¨Šæ¯"
            body = task

            # Try to extract the message content after "è¡¨é”" or similar keywords
            content_keywords = ["è¡¨é”", "è¡¨è¾¾", "å‘Šè¨´", "å‘Šè¯‰", "èªª", "è¯´", "å…§å®¹", "å†…å®¹", "message", "tell them"]
            for keyword in content_keywords:
                if keyword in task:
                    parts = task.split(keyword, 1)
                    if len(parts) > 1:
                        content = parts[1].strip()
                        # Remove trailing sender info like "æˆ‘æ˜¯XXX"
                        content = re.sub(r',?\s*æˆ‘æ˜¯.*$', '', content)
                        if content:
                            body = content

            # Look for subject keywords
            for keyword in ["ä¸»æ—¨", "ä¸»é¡Œ", "æ¨™é¡Œ", "subject", "é¡Œç›®"]:
                if keyword in task_lower:
                    parts = task.split(keyword, 1)
                    if len(parts) > 1:
                        # Extract text between quotes or until next keyword
                        subject_match = re.search(r'[æ˜¯:ï¼š]?\s*[ã€Œã€"]?([^ã€ã€"ï¼Œ,ã€‚]+)', parts[1])
                        if subject_match:
                            subject = subject_match.group(1).strip()

            # If no explicit subject, try to infer from context
            if subject == "ä¾†è‡ªAIåŠ©æ‰‹çš„è¨Šæ¯" and "é—œå¿ƒ" in task:
                subject = "å•å€™èˆ‡ç¥ç¦"

            # Look for body keywords explicitly
            for keyword in ["å…§å®¹æ˜¯", "æ­£æ–‡æ˜¯", "å†…å®¹æ˜¯", "body is", "content is"]:
                if keyword in task_lower:
                    parts = task.split(keyword, 1)
                    if len(parts) > 1:
                        body_match = re.search(r':?\s*[ã€Œã€"]?([^ã€ã€"]+)', parts[1])
                        if body_match:
                            body = body_match.group(1).strip()

            return ("send_email", {
                "to": emails,
                "subject": subject,
                "body": body
            })

    # Task creation patterns
    if any(keyword in task_lower for keyword in ["å‰µå»ºä»»å‹™", "å»ºç«‹ä»»å‹™", "create task", "æ–°å¢ä»»å‹™", "add task"]):
        return ("create_task", {
            "title": task[:100],
            "description": task,
            "assignee": "system"
        })

    # Search patterns - extract the actual search term
    search_keywords = ["æœç´¢", "æœå°‹", "search", "æŸ¥æ‰¾", "find", "æœ", "æ‰¾", "å°‹æ‰¾", "å¯»æ‰¾"]
    for keyword in search_keywords:
        if keyword in task_lower:
            # Extract search query by removing the search keyword and common connecting words
            query = task
            # Remove search keywords with common patterns - updated to handle more cases
            query = re.sub(r'(æœç´¢|æœå°‹|search\s+for|search|æŸ¥æ‰¾|find|æœ|æ‰¾|å°‹æ‰¾|å¯»æ‰¾)\s*(é—œæ–¼|å…³äº|about|for)?\s*', '', query, flags=re.IGNORECASE)
            # Remove common Chinese article/connecting words at the end
            query = re.sub(r'[çš„ä¹‹]?(æ–‡æª”|æ–‡æ¡£|æª”æ¡ˆ|æ¡£æ¡ˆ|è³‡æ–™|èµ„æ–™|å…§å®¹|å†…å®¹|ä¿¡æ¯|è³‡è¨Š|èµ„è®¯)$', '', query)
            # Clean up
            query = query.strip().lstrip('çš„ä¹‹').rstrip('çš„ä¹‹').strip()
            # If query is empty or too short, use original task
            if len(query) < 2:
                query = task
            logger.info(f"Search detected - Original: '{task}', Extracted query: '{query}'")
            return ("search_knowledge_base", {
                "query": query,
                "limit": 5
            })

    return None

@app.post("/agent/execute", response_model=AgentResponse)
async def execute_agent(request: AgentRequest):
    """åŸ·è¡ŒAgentä»»å‹™ - æ”¯æŒå·¥å…·èª¿ç”¨"""
    try:
        steps = []

        # Track MCP usage
        mcp_usage = {
            "tools_used": [],  # List of tools that were called
            "resources_accessed": [],  # List of resources accessed
            "system_prompt": "",  # The system prompt used
            "sampling_parameters": {
                "temperature": request.temperature,
                "top_p": request.top_p,
                "top_k": request.top_k
            }
        }

        # Map model aliases to actual model names for LiteLLM
        # LiteLLM handles provider prefixes, we just pass the model name configured in litellm-config.yaml
        model_name_map = {
            "claude-3-sonnet": "claude-3-sonnet",
            "claude-3-5-sonnet": "claude-3-5-sonnet",
            "claude-3-opus": "claude-3-opus",
            "claude-3-haiku": "claude-3-haiku",
            "gpt-3.5-turbo": "gpt-3.5-turbo",
            "gpt-4": "gpt-4",
            "gpt-4o": "gpt-4o",
            "gpt-4o-mini": "gpt-4o-mini",
            "gpt-4-turbo": "gpt-4-turbo",
            "gemini-1.5-pro": "gemini-1.5-pro",
            "gemini-1.5-flash": "gemini-1.5-flash",
            "qwen2.5": "qwen2.5",
            "qwen2.5-7b": "qwen2.5-7b",
            # Taiwan Government LLM API models
            "llama31-taidelx-8b-32k": "llama31-taidelx-8b-32k",
            "llama3-taiwan-70b-8k": "llama3-taiwan-70b-8k",
            "llama31-foxbrain-70b-32k": "llama31-foxbrain-70b-32k",
            "llama33-ffm-70b-32k": "llama33-ffm-70b-32k",
            "phi4-reasoning-plus-32k": "phi4-reasoning-plus-32k",
            "magistral-small-2506-32k": "magistral-small-2506-32k",
            "google-gemma-3-27b-32k": "google-gemma-3-27b-32k",
            "llama4-scout-17b-16e-instruct-32k": "llama4-scout-17b-16e-instruct-32k",
            "gpt-oss-20b-32k": "gpt-oss-20b-32k",
            "gpt-oss-120b-32k": "gpt-oss-120b-32k"
        }

        # Get actual model name for API calls
        actual_model = model_name_map.get(request.model, request.model)

        # Step 1: ç²å–å¯ç”¨å·¥å…·
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.get(f"{MCP_SERVER_URL}/tools/list", timeout=10.0)
                tools_data = resp.json()
                tools = tools_data.get('tools', [])
                steps.append({
                    "step": "fetch_tools",
                    "result": f"Found {len(tools)} tools",
                    "status": "success"
                })

                # Convert to function calling format
                functions = convert_tools_to_functions(tools)
        except Exception as e:
            logger.error(f"Failed to fetch tools: {e}")
            steps.append({
                "step": "fetch_tools",
                "result": f"Failed: {str(e)}",
                "status": "failed"
            })
            functions = []

        # List of models that support function calling
        # Note: Ollama models (qwen2.5) don't support OpenAI-style function calling
        # They use the fallback pattern matching approach in detect_tool_intent()
        function_calling_models = [
            "gpt-3.5-turbo", "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4-turbo",
            "claude-3-sonnet", "claude-3-5-sonnet", "claude-3-opus", "claude-3-haiku"
        ]

        # Check if this is a simple tool action that can be handled directly
        # This is a fallback for models that don't support function calling
        tool_intent = detect_tool_intent(request.task)
        if tool_intent and request.model not in function_calling_models:
            tool_name, tool_args = tool_intent

            steps.append({
                "step": "intent_detection",
                "tool": tool_name,
                "arguments": tool_args,
                "status": "detected"
            })

            try:
                # Log tool call for debugging
                logger.info(f"Calling tool in fallback mode: {tool_name} with args: {tool_args}")

                # Call the tool directly
                tool_result = await call_mcp_tool(tool_name, tool_args)

                logger.info(f"Tool result: {tool_result}")

                steps.append({
                    "step": "tool_execution",
                    "tool": tool_name,
                    "result": tool_result,
                    "status": "success"
                })

                # Format a nice response
                if tool_name == "send_email":
                    result = f"âœ… éƒµä»¶å·²æˆåŠŸç™¼é€ï¼\n\næ”¶ä»¶äºº: {', '.join(tool_args['to'])}\nä¸»æ—¨: {tool_args['subject']}\néƒµä»¶ID: {tool_result.get('email_id')}\nç™¼é€æ™‚é–“: {tool_result.get('sent_at')}"
                elif tool_name == "create_task":
                    result = f"âœ… ä»»å‹™å·²å‰µå»ºï¼\n\nä»»å‹™ID: {tool_result.get('id')}\næ¨™é¡Œ: {tool_args['title']}\nç‹€æ…‹: {tool_result.get('status')}"
                elif tool_name == "search_knowledge_base":
                    results = tool_result.get('results', [])
                    if len(results) > 0:
                        result_items = []
                        for i, r in enumerate(results[:5], 1):
                            title = r.get('title', 'N/A')
                            content = r.get('content', '')[:200]  # First 200 chars
                            result_items.append(f"{i}. **{title}**\n   {content}...")
                        result = f"âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ° {len(results)} å€‹çµæœã€‚\n\n" + "\n\n".join(result_items)
                    else:
                        result = f"âœ… æœç´¢å®Œæˆï¼æ‰¾åˆ° 0 å€‹çµæœã€‚\n\næœå°‹è©: \"{tool_args.get('query', 'N/A')}\"\n\nè³‡æ–™åº«ä¸­å¯èƒ½æ²’æœ‰ç›¸é—œæ–‡æª”ï¼Œè«‹å˜—è©¦å…¶ä»–æœå°‹è©ã€‚"
                else:
                    result = f"âœ… å·¥å…·åŸ·è¡ŒæˆåŠŸï¼\n\n{json.dumps(tool_result, ensure_ascii=False, indent=2)}"

                # Track fallback tool usage
                mcp_usage["tools_used"].append({
                    "name": tool_name,
                    "arguments": tool_args,
                    "result_summary": str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result)
                })

                return AgentResponse(
                    result=result,
                    steps=steps,
                    metadata={
                        "agent_type": request.agent_type,
                        "model_used": request.model,
                        "tool_called": tool_name,
                        "fallback_mode": True,
                        "mcp_usage": mcp_usage
                    }
                )

            except Exception as tool_error:
                logger.error(f"Tool execution error: {tool_error}")
                steps.append({
                    "step": "tool_execution",
                    "tool": tool_name,
                    "error": str(tool_error),
                    "status": "failed"
                })

                return AgentResponse(
                    result=f"âŒ å·¥å…·åŸ·è¡Œå¤±æ•—: {str(tool_error)}",
                    steps=steps,
                    metadata={
                        "agent_type": request.agent_type,
                        "error": str(tool_error),
                        "fallback_mode": True,
                        "mcp_usage": mcp_usage
                    }
                )

        # Step 2: å‘¼å«LLM with function calling (for supported models)
        # Enhanced system prompt for multi-stage conversations - specialized by agent type
        agent_prompts = {
            "general": """ä½ æ˜¯ä¸€å€‹ä¼æ¥­AIåŠ©æ‰‹ï¼Œå¯ä»¥ç›´æ¥å›ç­”å•é¡Œæˆ–ä½¿ç”¨å„ç¨®å·¥å…·ä¾†å¹«åŠ©ç”¨æˆ¶å®Œæˆä»»å‹™ã€‚

é‡è¦æŒ‡å—ï¼š

ğŸ“„ **æ–‡ä»¶åˆ†ææ¨¡å¼**ï¼š
- å¦‚æœç”¨æˆ¶ä¸Šå‚³äº†æ–‡ä»¶ï¼ˆPDFã€æ–‡æœ¬ç­‰ï¼‰ä¸¦è©¢å•å…§å®¹ï¼Œç›´æ¥åˆ†ææ–‡ä»¶ä¸¦å›ç­”å•é¡Œ
- ä¸éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥é–±è®€æä¾›çš„æ–‡ä»¶å…§å®¹ä¸¦é€²è¡Œåˆ†æ
- ç¤ºä¾‹ï¼šç”¨æˆ¶ä¸Šå‚³PDFä¸¦å•"æè¿°é€™ä»½æ–‡ä»¶" â†’ ç›´æ¥åˆ†ææ–‡ä»¶å…§å®¹ä¸¦è©³ç´°æè¿°

ğŸ› ï¸ **å·¥å…·ä½¿ç”¨æ¨¡å¼**ï¼š
1. ç•¶ç”¨æˆ¶è¦æ±‚åŸ·è¡ŒæŸå€‹æ“ä½œæ™‚ï¼ˆå¦‚ç™¼é€éƒµä»¶ã€å‰µå»ºä»»å‹™ã€æœç´¢ç­‰ï¼‰ï¼Œè«‹èª¿ç”¨ç›¸æ‡‰çš„å·¥å…·
2. åœ¨èª¿ç”¨å·¥å…·ä¹‹å‰ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰æ‰€æœ‰å¿…éœ€çš„åƒæ•¸
3. å¦‚æœç¼ºå°‘å¿…éœ€åƒæ•¸ï¼ˆå¦‚emailåœ°å€ã€subjectã€bodyç­‰ï¼‰ï¼Œä¸è¦çŒœæ¸¬æˆ–ä½¿ç”¨é»˜èªå€¼
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹ç¦®è²Œåœ°è©¢å•ç”¨æˆ¶æä¾›ç¼ºå°‘çš„ä¿¡æ¯
5. ä¸€æ¬¡åªè©¢å•ç¼ºå°‘çš„ä¿¡æ¯ï¼Œä¸è¦å•ä¸å¿…è¦çš„å•é¡Œ
6. æ”¶é›†åˆ°æ‰€æœ‰å¿…éœ€ä¿¡æ¯å¾Œï¼Œç«‹å³åŸ·è¡Œæ“ä½œ

ç¤ºä¾‹ï¼š
- ç”¨æˆ¶èªª"send email"ä½†æ²’æœ‰æä¾›æ”¶ä»¶äºº â†’ è©¢å•æ”¶ä»¶äººemailåœ°å€
- ç”¨æˆ¶èªª"send email to john@example.com"ä½†æ²’æœ‰ä¸»æ—¨å’Œå…§å®¹ â†’ è©¢å•éƒµä»¶ä¸»æ—¨å’Œå…§å®¹
- ç”¨æˆ¶æä¾›äº†æ‰€æœ‰ä¿¡æ¯ â†’ ç›´æ¥åŸ·è¡Œç™¼é€éƒµä»¶""",

            "research": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•·ä¿¡æ¯æ”¶é›†ã€åˆ†æå’Œæ•´ç†ã€‚

ä½ çš„å°ˆé•·ï¼š
1. ä½¿ç”¨æœç´¢å·¥å…·ï¼ˆsearch_knowledge_base, web_search, semantic_searchï¼‰æ·±å…¥ç ”ç©¶ä¸»é¡Œ
2. æ‰¾åˆ°ç›¸é—œæ–‡æª”ä¸¦æå–é—œéµä¿¡æ¯
3. æ•´åˆå¤šå€‹ä¾†æºçš„ä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„ç ”ç©¶å ±å‘Š
4. é©—è­‰ä¿¡æ¯çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§
5. æä¾›å¼•ç”¨å’Œä¾†æº

å·¥ä½œæ–¹å¼ï¼š
- æ”¶åˆ°ç ”ç©¶ä»»å‹™æ™‚ï¼Œå…ˆè¦åŠƒæœç´¢ç­–ç•¥
- ä½¿ç”¨å¤šå€‹æœç´¢å·¥å…·äº¤å‰é©—è­‰ä¿¡æ¯
- æ•´ç†ç™¼ç¾çš„ä¿¡æ¯ï¼Œä»¥çµæ§‹åŒ–æ–¹å¼å‘ˆç¾
- å¿…è¦æ™‚ä½¿ç”¨ summarize_document å·¥å…·ç¸½çµé•·æ–‡æª”
- æä¾›æ¸…æ™°çš„ç ”ç©¶çµè«–å’Œå»ºè­°

é‡é»ï¼šæ·±åº¦ã€æº–ç¢ºæ€§ã€ä¾†æºå¯é æ€§""",

            "analysis": """ä½ æ˜¯ä¸€å€‹æ•¸æ“šåˆ†æå°ˆå®¶ï¼Œå°ˆæ³¨æ–¼æ•¸æ“šè™•ç†ã€åˆ†æå’Œå¯è¦–åŒ–ã€‚

ä½ çš„å°ˆé•·ï¼š
1. ä½¿ç”¨ analyze_data å·¥å…·é€²è¡Œçµ±è¨ˆåˆ†æ
2. ä½¿ç”¨ process_csv è™•ç†å’Œæ¸…ç†æ•¸æ“š
3. ä½¿ç”¨ generate_chart å‰µå»ºæ•¸æ“šå¯è¦–åŒ–
4. ä½¿ç”¨ calculate_metrics è¨ˆç®—æ¥­å‹™æŒ‡æ¨™
5. ä½¿ç”¨ financial_calculator é€²è¡Œè²¡å‹™åˆ†æ

å·¥ä½œæµç¨‹ï¼š
- ç†è§£æ•¸æ“šåˆ†æéœ€æ±‚
- æª¢æŸ¥æ•¸æ“šè³ªé‡å’Œå®Œæ•´æ€§
- é¸æ“‡é©ç•¶çš„åˆ†ææ–¹æ³•
- ç”Ÿæˆæ¸…æ™°çš„åœ–è¡¨å’Œå ±è¡¨
- æä¾›æ•¸æ“šé©…å‹•çš„è¦‹è§£å’Œå»ºè­°

é‡é»ï¼šæ•¸æ“šæº–ç¢ºæ€§ã€åˆ†ææ·±åº¦ã€å¯è¦–åŒ–æ¸…æ™°åº¦ã€actionable insights"""
        }

        system_prompt = agent_prompts.get(request.agent_type, agent_prompts["general"])

        # Store system prompt in MCP usage
        mcp_usage["system_prompt"] = system_prompt

        # Check if document analysis is needed (documents are marked with special tags)
        has_document = "===== IMPORTANT: DOCUMENT ANALYSIS REQUIRED =====" in request.task or "---BEGIN DOCUMENT CONTENT---" in request.task

        if has_document:
            # PRIORITY: Document analysis mode - override other behaviors
            system_prompt = """ğŸ“„ ä½ æ­£åœ¨è™•ç†æ–‡ä»¶åˆ†æä»»å‹™ã€‚

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç”¨æˆ¶å·²ç¶“ä¸Šå‚³äº†æ–‡ä»¶å…§å®¹ï¼ˆPDFã€æ–‡æœ¬ç­‰ï¼‰ï¼Œå…§å®¹å·²ç¶“åŒ…å«åœ¨ç”¨æˆ¶çš„æ¶ˆæ¯ä¸­
2. ä½ çš„ä»»å‹™æ˜¯ç›´æ¥é–±è®€å’Œåˆ†ææä¾›çš„æ–‡ä»¶å…§å®¹
3. ä¸è¦ä½¿ç”¨ä»»ä½•å·¥å…·ï¼ˆweb_searchã€search_knowledge_baseç­‰ï¼‰
4. ä¸è¦èªª"æˆ‘ç„¡æ³•æ‰¾åˆ°"æˆ–"æˆ‘æœªèƒ½æ‰¾åˆ°" - æ–‡ä»¶å…§å®¹å°±åœ¨ç”¨æˆ¶æ¶ˆæ¯ä¸­
5. ç›´æ¥åˆ†ææ–‡ä»¶å…§å®¹ä¸¦è©³ç´°å›ç­”ç”¨æˆ¶çš„å•é¡Œ

**å·¥ä½œæµç¨‹**ï¼š
- ä»”ç´°é–±è®€ "---BEGIN DOCUMENT CONTENT---" å’Œ "---END DOCUMENT CONTENT---" ä¹‹é–“çš„å…§å®¹
- æ ¹æ“šæ–‡ä»¶å…§å®¹å›ç­”ç”¨æˆ¶çš„å•é¡Œ
- æä¾›å…·é«”ã€è©³ç´°çš„åˆ†æå’Œè¦‹è§£

è¨˜ä½ï¼šæ–‡ä»¶å…§å®¹å·²ç¶“æä¾›çµ¦ä½ äº†ï¼Œä¸éœ€è¦æœç´¢æˆ–ä½¿ç”¨å·¥å…·ï¼"""
            actual_task = request.task
        else:
            # Check if web search is enabled
            web_search_enabled = request.task.startswith("[WEB_SEARCH_ENABLED]")
            if web_search_enabled:
                # Remove the flag from the task
                actual_task = request.task.replace("[WEB_SEARCH_ENABLED]", "").strip()
                # Enhance system prompt to use web search
                system_prompt += "\n\nğŸŒ **WEB SEARCH MODE ENABLED**\né‡è¦: ç”¨æˆ¶è¦æ±‚ä½¿ç”¨ç¶²è·¯æœç´¢ã€‚è«‹å‹™å¿…ä½¿ç”¨ web_search å·¥å…·ä¾†ç²å–æœ€æ–°çš„å³æ™‚ä¿¡æ¯ã€‚æ­¥é©Ÿ:\n1. ä½¿ç”¨ web_search å·¥å…·æœç´¢ç›¸é—œè³‡è¨Š\n2. åˆ†ææœç´¢çµæœ\n3. åŸºæ–¼æœç´¢çµæœå›ç­”ç”¨æˆ¶çš„å•é¡Œ\n\nå¦‚æœæ²’æœ‰æœç´¢åˆ°ç›¸é—œçµæœï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥ç”¨æˆ¶ã€‚"
            else:
                actual_task = request.task

        # Build messages array with conversation history
        messages = [{"role": "system", "content": system_prompt}]

        # Add conversation history if provided
        if request.conversation_history:
            messages.extend(request.conversation_history)

        # Add current user message with optional images (use actual_task without flag)
        if request.images and len(request.images) > 0:
            # Vision models: format message with images
            content = []
            content.append({"type": "text", "text": actual_task})

            # Add images in proper format
            for img in request.images:
                # OpenAI/Claude format
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{img['mime_type']};base64,{img['data']}"
                    }
                })

            messages.append({"role": "user", "content": content})
        else:
            # Text-only message
            messages.append({"role": "user", "content": actual_task})

        max_iterations = 5  # Prevent infinite loops
        iteration = 0

        while iteration < max_iterations:
            iteration += 1

            try:
                # Call LLM with functions
                async with httpx.AsyncClient() as client:
                    llm_payload = {
                        "model": actual_model,  # Use actual model name for LiteLLM
                        "messages": messages,
                        "temperature": request.temperature,
                        "top_p": request.top_p,
                        "max_tokens": 2000
                    }

                    # Add top_k if supported (mainly for local models like qwen)
                    if request.model.startswith("qwen"):
                        llm_payload["top_k"] = request.top_k

                    # Add functions if model supports it
                    if functions and request.model in function_calling_models:
                        llm_payload["tools"] = [{"type": "function", "function": f} for f in functions]
                        # Claude doesn't need tool_choice parameter, LiteLLM handles it
                        if not request.model.startswith("claude"):
                            llm_payload["tool_choice"] = "auto"

                    llm_response = await client.post(
                        f"{LLM_PROXY_URL}/v1/chat/completions",
                        headers={"Authorization": f"Bearer {LITELLM_API_KEY}"},
                        json=llm_payload,
                        timeout=60.0
                    )

                    if llm_response.status_code != 200:
                        llm_data = llm_response.json()
                        error_detail = str(llm_data)
                        if isinstance(llm_data, dict) and "error" in llm_data:
                            error_detail = llm_data["error"].get("message", str(llm_data["error"]))

                        steps.append({
                            "step": f"llm_call_{iteration}",
                            "result": f"Failed: {error_detail}",
                            "status": "failed"
                        })

                        return AgentResponse(
                            result=f"LLMéŒ¯èª¤: {error_detail}",
                            steps=steps,
                            metadata={"agent_type": request.agent_type, "error": error_detail, "mcp_usage": mcp_usage}
                        )

                    llm_data = llm_response.json()
                    assistant_message = llm_data["choices"][0]["message"]

                    # Add assistant message to history
                    messages.append(assistant_message)

                    # Check if function was called
                    tool_calls = assistant_message.get("tool_calls", [])

                    if not tool_calls:
                        # No tool call - could be asking for more info or final answer
                        result = assistant_message.get("content", "")

                        # Detect if agent is asking for more information
                        asking_keywords = [
                            "è«‹æä¾›", "è¯·æä¾›", "please provide", "what is", "what's",
                            "éœ€è¦", "ç¼ºå°‘", "could you", "can you provide",
                            "è«‹å‘Šè¨´", "è¯·å‘Šè¯‰", "tell me", "who", "which",
                            "emailåœ°å€", "email address", "æ”¶ä»¶äºº", "recipient",
                            "ä¸»æ—¨", "subject", "å…§å®¹", "content", "body"
                        ]

                        is_asking = any(keyword in result.lower() for keyword in asking_keywords)
                        has_question = "?" in result or "ï¼Ÿ" in result

                        needs_more_info = is_asking or has_question

                        steps.append({
                            "step": f"llm_response_{iteration}",
                            "result": "Asking for more information" if needs_more_info else "Task completed",
                            "status": "success"
                        })

                        return AgentResponse(
                            result=result,
                            steps=steps,
                            metadata={
                                "agent_type": request.agent_type,
                                "model_used": request.model,
                                "iterations": iteration,
                                "tokens_used": llm_data.get("usage", {}).get("total_tokens", 0),
                                "conversation_active": needs_more_info,
                                "mcp_usage": mcp_usage
                            },
                            needs_more_info=needs_more_info
                        )

                    # Execute each tool call
                    for tool_call in tool_calls:
                        function_name = tool_call["function"]["name"]
                        function_args = json.loads(tool_call["function"]["arguments"])

                        steps.append({
                            "step": f"tool_call_{iteration}",
                            "tool": function_name,
                            "arguments": function_args,
                            "status": "executing"
                        })

                        try:
                            # Call the MCP tool
                            tool_result = await call_mcp_tool(function_name, function_args)

                            # Track tool usage
                            tool_usage_record = {
                                "name": function_name,
                                "arguments": function_args,
                                "result_summary": str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result)
                            }
                            mcp_usage["tools_used"].append(tool_usage_record)

                            # Track resource access
                            if function_name == "get_document" and "document_id" in function_args:
                                mcp_usage["resources_accessed"].append({
                                    "type": "document",
                                    "id": function_args["document_id"]
                                })
                            elif function_name in ["search_knowledge_base", "semantic_search", "web_search"]:
                                mcp_usage["resources_accessed"].append({
                                    "type": "search",
                                    "query": function_args.get("query", "N/A")
                                })

                            steps.append({
                                "step": f"tool_result_{iteration}",
                                "tool": function_name,
                                "result": tool_result,
                                "status": "success"
                            })

                            # Add function result to messages
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "content": json.dumps(tool_result)
                            })

                        except Exception as tool_error:
                            logger.error(f"Tool execution error for {function_name}: {tool_error}")
                            steps.append({
                                "step": f"tool_error_{iteration}",
                                "tool": function_name,
                                "error": str(tool_error),
                                "status": "failed"
                            })

                            # Add error to messages
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call["id"],
                                "content": json.dumps({"error": str(tool_error)})
                            })

                    # Continue loop to get LLM's response with tool results

            except Exception as e:
                logger.error(f"LLM processing error: {e}")
                steps.append({
                    "step": f"llm_error_{iteration}",
                    "result": f"Failed: {str(e)}",
                    "status": "failed"
                })

                return AgentResponse(
                    result=f"è™•ç†å¤±æ•—: {str(e)}",
                    steps=steps,
                    metadata={"agent_type": request.agent_type, "error": str(e), "mcp_usage": mcp_usage}
                )

        # Max iterations reached
        return AgentResponse(
            result="ä»»å‹™è™•ç†è¶…éæœ€å¤§è¿­ä»£æ¬¡æ•¸",
            steps=steps,
            metadata={"agent_type": request.agent_type, "max_iterations_reached": True, "mcp_usage": mcp_usage}
        )

    except Exception as e:
        logger.error(f"Agent execution error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/agent/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ç°¡å–®çš„èŠå¤©ä»‹é¢"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{LLM_PROXY_URL}/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {LITELLM_API_KEY}"
                },
                json={
                    "model": request.model,
                    "messages": [
                        {"role": "user", "content": request.message}
                    ],
                    "temperature": request.temperature,
                    "max_tokens": 2000
                },
                timeout=30.0
            )
            
            data = response.json()

            if response.status_code != 200:
                # Parse detailed error from LiteLLM response
                error_detail = str(data)
                original_error = ""

                # Try to extract meaningful error message from nested structures
                if isinstance(data, dict):
                    if "error" in data:
                        error_obj = data["error"]
                        if isinstance(error_obj, dict):
                            error_detail = error_obj.get("message", str(error_obj))
                            # Try to get the original error message if available
                            if "error" in error_obj and isinstance(error_obj["error"], dict):
                                original_error = error_obj["error"].get("message", "")
                        else:
                            error_detail = str(error_obj)
                    elif "detail" in data:
                        error_detail = data["detail"]

                # Combine all error text for matching
                full_error_text = f"{error_detail} {original_error}".lower()

                # Provide user-friendly error messages for common issues
                if "credit balance is too low" in full_error_text or "insufficient_quota" in full_error_text:
                    user_message = f"âŒ APIé…é¡ä¸è¶³\n\næ‚¨çš„ {request.model} APIå¸³æˆ¶é¤˜é¡ä¸è¶³æˆ–é…é¡å·²ç”¨å®Œã€‚\n\nè§£æ±ºæ–¹æ³•:\n1. å‰å¾€APIæä¾›å•†çš„æ§åˆ¶å°å……å€¼\n2. å‡ç´šæ‚¨çš„APIæ–¹æ¡ˆ\n3. æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹ 'qwen2.5' (ç„¡éœ€APIé‡‘é‘°)"
                elif "authentication" in full_error_text or "api key" in full_error_text or "invalid_api_key" in full_error_text:
                    user_message = f"âŒ èªè­‰å¤±æ•—\n\n{request.model} çš„APIé‡‘é‘°ç„¡æ•ˆæˆ–å·²éæœŸã€‚\n\nè§£æ±ºæ–¹æ³•:\n1. æª¢æŸ¥.envæª”æ¡ˆä¸­çš„APIé‡‘é‘°é…ç½®\n2. ç¢ºèªAPIé‡‘é‘°æœ‰æ•ˆä¸”æœªéæœŸ\n3. æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹ 'qwen2.5' (ç„¡éœ€APIé‡‘é‘°)"
                elif "rate limit" in full_error_text or "too many requests" in full_error_text or "429" in str(response.status_code):
                    user_message = f"âŒ è«‹æ±‚éæ–¼é »ç¹\n\n{request.model} APIå·²é”åˆ°é€Ÿç‡é™åˆ¶ã€‚\n\nè§£æ±ºæ–¹æ³•:\n1. ç¨å¾Œå†è©¦\n2. å‡ç´šæ‚¨çš„APIæ–¹æ¡ˆä»¥ç²å¾—æ›´é«˜é€Ÿç‡é™åˆ¶\n3. æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹ 'qwen2.5' (ç„¡é€Ÿç‡é™åˆ¶)"
                elif ("model" in full_error_text and "not found" in full_error_text) or "model_not_found" in full_error_text:
                    user_message = f"âŒ æ¨¡å‹ä¸å­˜åœ¨\n\næ¨¡å‹ '{request.model}' ä¸å¯ç”¨ã€‚\n\nè§£æ±ºæ–¹æ³•:\n1. æª¢æŸ¥æ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢º\n2. ç¢ºèªæ‚¨çš„APIå¸³æˆ¶æœ‰æ¬Šè¨ªå•è©²æ¨¡å‹\n3. ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹: qwen2.5 (æœ¬åœ°), gpt-3.5-turbo, gpt-4, claude-3-sonnet"
                elif "timeout" in full_error_text:
                    user_message = f"â±ï¸ è«‹æ±‚è¶…æ™‚\n\n{request.model} APIéŸ¿æ‡‰è¶…æ™‚ï¼Œè«‹ç¨å¾Œé‡è©¦ã€‚"
                else:
                    # Show truncated error for better readability
                    error_preview = error_detail[:200] + "..." if len(error_detail) > 200 else error_detail
                    user_message = f"âŒ APIéŒ¯èª¤ ({request.model})\n\n{error_preview}\n\næç¤º: å¯ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹ 'qwen2.5' é¿å…APIå•é¡Œ"

                logger.error(f"LLM API error for model {request.model}: {error_detail}")
                raise HTTPException(status_code=response.status_code, detail=user_message)

            return ChatResponse(
                response=data["choices"][0]["message"]["content"],
                model=request.model
            )

    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LLMæœå‹™è¶…æ™‚ï¼Œè«‹ç¨å¾Œå†è©¦")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=f"èŠå¤©å¤±æ•—: {str(e)}")

@app.get("/")
async def root():
    return {
        "service": "Agent Service",
        "version": "1.0.0",
        "status": "running",
        "llm_proxy": LLM_PROXY_URL,
        "mcp_server": MCP_SERVER_URL
    }
