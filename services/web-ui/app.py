import streamlit as st
import requests
import os
import time
import base64
import random
from io import BytesIO
from PyPDF2 import PdfReader
from i18n import LANGUAGES, get_text

# Initialize session state for language
if "language" not in st.session_state:
    st.session_state.language = "zh-TW"  # Default to Traditional Chinese

lang = st.session_state.language

st.set_page_config(
    page_title=get_text("page_title", lang),
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

AGENT_SERVICE_URL = os.getenv("AGENT_SERVICE_URL", "http://agent-service:8000")
LITELLM_URL = os.getenv("LITELLM_URL", "http://litellm:4000")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")

# Model context limits (conservative estimates to account for system prompt and tools)
MODEL_CONTEXT_LIMITS = {
    "qwen2.5": {
        "total_tokens": 32000,
        "safe_limit": 24000,
        "avg_tokens_per_message": 150
    },
    "qwen2.5-7b": {
        "total_tokens": 32000,
        "safe_limit": 24000,
        "avg_tokens_per_message": 150
    },
    "gpt-3.5-turbo": {
        "total_tokens": 16385,
        "safe_limit": 12000,
        "avg_tokens_per_message": 100
    },
    "gpt-4": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "gpt-4o": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "gpt-4o-mini": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "claude-3-opus": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-sonnet": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-5-sonnet": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-haiku": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "gemini-1.5-pro": {
        "total_tokens": 1000000,
        "safe_limit": 750000,
        "avg_tokens_per_message": 200
    },
    "gemini-1.5-flash": {
        "total_tokens": 1000000,
        "safe_limit": 750000,
        "avg_tokens_per_message": 150
    }
}

def estimate_tokens(text):
    """Rough estimate: ~4 characters per token"""
    return len(text) // 4

def truncate_conversation_history(history, model, max_messages=None):
    """Truncate conversation history to fit within model context limits"""
    if not history:
        return history

    limits = MODEL_CONTEXT_LIMITS.get(model, MODEL_CONTEXT_LIMITS["gpt-3.5-turbo"])

    # If max_messages specified, use that
    if max_messages:
        return history[-max_messages * 2:]  # Keep last N exchanges (user + assistant pairs)

    # Otherwise, estimate tokens and truncate
    estimated_tokens = sum(estimate_tokens(msg["content"]) for msg in history)

    if estimated_tokens <= limits["safe_limit"]:
        return history

    # Truncate from the beginning, keeping most recent messages
    # Always keep at least the last 4 messages (2 exchanges)
    min_messages = 4
    truncated = history[-min_messages:]

    # Add more messages if we have room
    for i in range(len(history) - min_messages - 1, -1, -1):
        msg = history[i]
        msg_tokens = estimate_tokens(msg["content"])
        current_tokens = sum(estimate_tokens(m["content"]) for m in truncated)

        if current_tokens + msg_tokens < limits["safe_limit"]:
            truncated.insert(0, msg)
        else:
            break

    return truncated

def get_context_usage_info(history, model):
    """Get context usage information"""
    if not history:
        return {"messages": 0, "estimated_tokens": 0, "percentage": 0}

    limits = MODEL_CONTEXT_LIMITS.get(model, MODEL_CONTEXT_LIMITS["gpt-3.5-turbo"])
    estimated_tokens = sum(estimate_tokens(msg["content"]) for msg in history)
    percentage = (estimated_tokens / limits["safe_limit"]) * 100

    return {
        "messages": len(history),
        "estimated_tokens": estimated_tokens,
        "safe_limit": limits["safe_limit"],
        "percentage": round(percentage, 1)
    }

# Custom CSS
st.markdown("""
<style>
    /* Main content area padding */
    .main > div {
        padding-top: 2rem !important;
    }

    .block-container {
        padding-top: 2rem !important;
        padding-bottom: 1rem !important;
    }

    /* Reduce sidebar padding aggressively */
    section[data-testid="stSidebar"] {
        padding-top: 0 !important;
    }

    section[data-testid="stSidebar"] > div {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] > div > div {
        padding-top: 0 !important;
    }

    /* Ensure first element in sidebar has no extra margin */
    section[data-testid="stSidebar"] .element-container:first-child {
        margin-top: 0 !important;
        padding-top: 0 !important;
    }

    section[data-testid="stSidebar"] .element-container {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] hr {
        margin-top: 0.5rem !important;
        margin-bottom: 0.5rem !important;
    }

    section[data-testid="stSidebar"] .stSelectbox {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] [data-testid="stMarkdownContainer"] {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] .stAlert {
        padding: 0.5rem !important;
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] .stColumns {
        gap: 0.3rem !important;
    }

    /* Reduce header spacing */
    h1, h2, h3 {
        margin-top: 0.3rem !important;
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] h1,
    section[data-testid="stSidebar"] h2,
    section[data-testid="stSidebar"] h3 {
        margin-top: 0.2rem !important;
        margin-bottom: 0.2rem !important;
        padding-top: 0 !important;
        padding-bottom: 0 !important;
    }

    /* Reduce caption spacing */
    .caption {
        margin-top: 0.2rem !important;
        margin-bottom: 0.2rem !important;
    }

    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 0.3rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .status-healthy {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
    }
    .status-unhealthy {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    # Logo and Title at top of sidebar
    st.markdown("""
    <div style="text-align: center; margin: 0; padding: 0;">
        <div style="font-size: 2rem; line-height: 1; margin: 0;">ü§ñ</div>
        <h3 style="margin: 0; padding: 0; color: #1f77b4; font-weight: 700; font-size: 1rem; line-height: 1;">AI Agents Platform</h3>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("---")

    st.markdown(f"### {get_text('settings', lang)}")

    # Language selector
    selected_lang = st.selectbox(
        get_text("language", lang),
        options=list(LANGUAGES.keys()),
        format_func=lambda x: LANGUAGES[x],
        index=list(LANGUAGES.keys()).index(st.session_state.language),
        key="lang_selector"
    )

    if selected_lang != st.session_state.language:
        st.session_state.language = selected_lang
        st.rerun()

    # Model options with display names
    # Load model options from litellm config
    import yaml

    def load_model_options():
        try:
            with open("/app/config/litellm-config.yaml", 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                model_options = {}
                if "model_list" in config:
                    for model in config["model_list"]:
                        # Only include models where visible is True (default to True if not specified)
                        is_visible = model.get("visible", True)
                        if not is_visible:
                            continue

                        model_name = model.get("model_name", "")
                        display_name = model.get("display_name", model_name)
                        if model_name and display_name:
                            model_options[display_name] = model_name
                return model_options
        except Exception as e:
            # Fallback to default if config can't be loaded
            return {
                "qwen2.5:7b (local - better for PDFs)": "qwen2.5-7b",
                "gpt-4o (OpenAI)": "gpt-4o"
            }

    model_options = load_model_options()

    model_display = st.selectbox(
        get_text("select_model", lang),
        options=list(model_options.keys()),
        help=get_text("model_help", lang)
    )

    model_choice = model_options[model_display]

    # Show model information
    st.markdown("---")
    st.markdown("### üìã " + ("Ê®°ÂûãË≥áË®ä" if lang == "zh-TW" else "Model Information"))

    # Get model info
    model_info = MODEL_CONTEXT_LIMITS.get(model_choice, {})

    # Display model details in a nice format
    col_info1, col_info2 = st.columns(2)

    with col_info1:
        # Model name and provider
        if model_choice.startswith("qwen"):
            st.info(f"**Provider:** Local (Ollama)\n\n**Status:** ‚úÖ No API key needed")
        elif model_choice.startswith("gpt"):
            st.info(f"**Provider:** OpenAI\n\n**Status:** ‚ö†Ô∏è API key required")
        elif model_choice.startswith("claude"):
            st.info(f"**Provider:** Anthropic\n\n**Status:** ‚ö†Ô∏è API key required")
        elif model_choice.startswith("gemini"):
            st.info(f"**Provider:** Google\n\n**Status:** ‚ö†Ô∏è API key required")
        elif model_choice.startswith("llama"):
            st.info(f"**Provider:** Taiwan Gov (AFSPOD)\n\n**Status:** ‚úÖ API key configured")

    with col_info2:
        # Context window info
        if model_info:
            st.info(f"**Context Window:** {model_info.get('total_tokens', 'N/A'):,} tokens\n\n**Safe Limit:** {model_info.get('safe_limit', 'N/A'):,} tokens")

    # Model capabilities
    vision_models = ["gpt-4o", "gpt-4o-mini", "claude-3-opus", "claude-3-5-sonnet", "claude-3-sonnet", "gemini-1.5-pro", "gemini-1.5-flash"]
    capabilities = []
    if model_choice in vision_models:
        capabilities.append("üñºÔ∏è Vision")
    if model_choice.startswith("qwen") or model_choice.startswith("claude"):
        capabilities.append("üìÑ PDF Analysis")
    if model_choice in ["gpt-4o", "gpt-4", "claude-3-opus", "claude-3-5-sonnet", "gemini-1.5-pro"]:
        capabilities.append("üß† Advanced Reasoning")

    if capabilities:
        st.caption("**Capabilities:** " + " ‚Ä¢ ".join(capabilities))

    st.markdown("---")

    # Sampling parameters
    st.subheader("üéõÔ∏è " + ("Êé°Ê®£ÂèÉÊï∏" if lang == "zh-TW" else "Sampling Parameters"))

    temperature = st.slider(
        get_text("temperature", lang),
        0.0, 1.0, 0.7,
        help=get_text("temperature_help", lang)
    )

    top_p = st.slider(
        "Top-P (nucleus sampling)",
        0.0, 1.0, 0.9,
        help="Controls diversity via nucleus sampling. Lower values make output more focused, higher values more diverse."
    )

    top_k = st.slider(
        "Top-K",
        0, 100, 40,
        help="Limits sampling to top K tokens. 0 means no limit. Lower values make output more focused."
    )

    st.divider()

    # Always show Context Information section
    st.subheader("üí¨ " + ("Â∞çË©±‰∏ä‰∏ãÊñá" if lang == "zh-TW" else "Context Info"))

    # Show model context limits
    if model_choice in MODEL_CONTEXT_LIMITS:
        limits = MODEL_CONTEXT_LIMITS[model_choice]
        st.info(f"üìä **{model_choice}**\n\nMax Context: {limits['total_tokens']:,} tokens\nSafe Limit: {limits['safe_limit']:,} tokens")

    # Show context usage if there's conversation history
    if "conversation_history" in st.session_state and st.session_state.conversation_history:
        usage = get_context_usage_info(st.session_state.conversation_history, model_choice)

        # Color based on usage
        if usage["percentage"] < 50:
            color = "üü¢"
        elif usage["percentage"] < 80:
            color = "üü°"
        else:
            color = "üî¥"

        st.metric(
            label="Current Usage",
            value=f"{usage['percentage']}%",
            delta=f"{usage['messages']} messages"
        )
        st.caption(f"{color} {usage['estimated_tokens']:,} / {usage['safe_limit']:,} tokens")

        if usage["percentage"] > 80:
            st.warning("‚ö†Ô∏è " + ("Êé•Ëøë‰∏ä‰∏ãÊñáÈôêÂà∂ÔºÅ" if lang == "zh-TW" else "Near context limit!"))
    else:
        st.caption("üí≠ " + ("ÈñãÂßãÂ∞çË©±ÂæåÊúÉÈ°ØÁ§∫‰ΩøÁî®ÊÉÖÊ≥Å" if lang == "zh-TW" else "Start a conversation to see usage"))

    st.divider()

    # System Status
    st.header(get_text("system_status", lang))

    with st.spinner(get_text("checking_status", lang)):
        # Check Agent service
        try:
            resp = requests.get(f"{AGENT_SERVICE_URL}/health", timeout=3)
            if resp.ok:
                st.success(get_text("agent_service_ok", lang))
                health_data = resp.json()
                if "services" in health_data:
                    for service, status in health_data["services"].items():
                        if "connected" in str(status):
                            st.text(f"  ‚îî‚îÄ {service}: ‚úì")
                        else:
                            st.text(f"  ‚îî‚îÄ {service}: ‚úó")
            else:
                st.error(get_text("agent_service_error", lang))
        except Exception as e:
            st.error(get_text("agent_service_offline", lang))
            st.caption(f"{get_text('error', lang)}: {str(e)}")

    st.divider()

    # Quick Actions
    st.header(get_text("quick_actions", lang))
    if st.button(get_text("clear_chat", lang)):
        st.session_state.messages = []
        st.session_state.conversation_history = []  # Also clear conversation history
        st.rerun()

    if st.button(get_text("export_chat", lang)):
        if "messages" in st.session_state and st.session_state.messages:
            conversation = "\n\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in st.session_state.messages
            ])
            st.download_button(
                get_text("download_chat", lang),
                conversation,
                file_name="conversation.txt",
                mime="text/plain"
            )
        else:
            st.info(get_text("no_chat_history", lang))

# Main Content
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
    get_text("tab_chat", lang),
    get_text("tab_agent", lang),
    get_text("tab_agents_catalog", lang),
    get_text("tab_models_config", lang),
    get_text("tab_monitor", lang),
    get_text("tab_rag", lang),
    "üìö Documentation",
    get_text("tab_about", lang)
])

with tab1:
    st.header(get_text("chat_header", lang))
    st.caption(get_text("chat_caption", lang))

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Initialize conversation history for multi-stage conversations
    if "conversation_history" not in st.session_state:
        st.session_state.conversation_history = []

    # Initialize uploaded files
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []

    # File upload and web search options (Claude.ai style)
    with st.expander("üìé Attachments & Options", expanded=False):
        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            uploaded_files = st.file_uploader(
                "Upload files (images, documents, etc.)",
                accept_multiple_files=True,
                type=['png', 'jpg', 'jpeg', 'gif', 'pdf', 'txt', 'md', 'csv', 'json', 'xml'],
                key="file_uploader"
            )

        with col2:
            web_search_enabled = st.checkbox(
                "üåê Web Search",
                value=False,
                help="Enable web search for real-time information"
            )

        with col3:
            rag_search_enabled = st.checkbox(
                "üìö RAG Knowledge",
                value=False,
                help="Search knowledge base for relevant context"
            )

        # Document selection for RAG
        if rag_search_enabled:
            st.write("**Select documents to search:**")
            try:
                response = requests.get(f"{MCP_SERVER_URL}/rag/documents?limit=100", timeout=5)
                if response.status_code == 200:
                    docs_data = response.json()
                    documents = docs_data.get('documents', [])

                    if documents:
                        doc_options = {f"{doc['id']} - {doc['title']}": doc['id'] for doc in documents}
                        selected_docs = st.multiselect(
                            "Choose specific documents (optional - leave empty to search all)",
                            options=list(doc_options.keys()),
                            default=[],
                            key="rag_doc_selection"
                        )
                        # Store selected doc IDs in session state
                        if "selected_doc_ids" not in st.session_state:
                            st.session_state.selected_doc_ids = []
                        st.session_state.selected_doc_ids = [doc_options[doc] for doc in selected_docs]
                    else:
                        st.info("No documents available. Upload documents in the Knowledge Base tab.")
                else:
                    st.warning("Could not load documents list")
            except Exception as e:
                st.warning(f"Error loading documents: {str(e)}")

        # Display uploaded files
        if uploaded_files:
            st.write("**Attached files:**")
            for file in uploaded_files:
                file_size = len(file.getvalue()) / 1024  # KB
                st.caption(f"üìÑ {file.name} ({file_size:.1f} KB)")

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # Show attached files if any
            if message.get("files"):
                with st.expander("üìé Attachments"):
                    for file_info in message["files"]:
                        st.caption(f"üìÑ {file_info['name']}")

            # Show conversation indicator if this was part of a multi-stage conversation
            if message.get("needs_more_info"):
                st.caption("üí¨ Waiting for more information...")

    # Chat input
    if prompt := st.chat_input(get_text("chat_input", lang)):
        # Process uploaded files
        file_contents = []
        file_info_list = []
        if uploaded_files:
            for file in uploaded_files:
                file_bytes = file.getvalue()
                file_name = file.name
                file_type = file.type

                # Handle different file types
                if file_type.startswith('image/'):
                    # Encode image as base64
                    base64_image = base64.b64encode(file_bytes).decode('utf-8')
                    file_contents.append({
                        "type": "image",
                        "name": file_name,
                        "data": base64_image,
                        "mime_type": file_type
                    })
                elif file_type == 'application/pdf':
                    # Extract PDF text content
                    try:
                        pdf_reader = PdfReader(BytesIO(file_bytes))
                        text_content = ""
                        for page in pdf_reader.pages:
                            text_content += page.extract_text() + "\n"
                        file_contents.append({
                            "type": "text",
                            "name": file_name,
                            "content": text_content,
                            "pages": len(pdf_reader.pages)
                        })
                    except Exception as e:
                        # If PDF extraction fails, provide error info
                        file_contents.append({
                            "type": "file",
                            "name": file_name,
                            "size": len(file_bytes),
                            "error": f"Failed to extract PDF: {str(e)}"
                        })
                elif file_type in ['text/plain', 'text/markdown', 'text/csv', 'application/json']:
                    # Extract text content
                    text_content = file_bytes.decode('utf-8')
                    file_contents.append({
                        "type": "text",
                        "name": file_name,
                        "content": text_content
                    })
                else:
                    # For other files, provide basic info
                    file_contents.append({
                        "type": "file",
                        "name": file_name,
                        "size": len(file_bytes)
                    })

                file_info_list.append({"name": file_name, "type": file_type})

        # Build enhanced prompt with file context
        enhanced_prompt = prompt
        has_images = False

        if file_contents:
            # Check if we have images
            has_images = any(fc["type"] == "image" for fc in file_contents)

            # Add text file contexts
            text_files = [fc for fc in file_contents if fc["type"] == "text"]
            if text_files:
                # Add explicit instruction for document analysis
                files_context = "\n\n===== IMPORTANT: DOCUMENT ANALYSIS REQUIRED =====\n"
                files_context += "User has uploaded document(s). Please analyze the content below and respond to the user's question based on this document.\n\n"

                for fc in text_files:
                    # For PDF files, include more content (up to 10000 chars for better understanding)
                    # For other text files, include full content or 5000 chars
                    if fc['name'].endswith('.pdf'):
                        max_chars = 10000
                        content_preview = fc['content'][:max_chars]
                        truncated = len(fc['content']) > max_chars
                        if truncated:
                            files_context += f"\n[DOCUMENT: {fc['name']} - {fc.get('pages', '?')} pages PDF - First {max_chars} characters shown]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT (TRUNCATED)---\n"
                        else:
                            files_context += f"\n[DOCUMENT: {fc['name']} - {fc.get('pages', '?')} pages PDF - Complete Content]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT---\n"
                    else:
                        max_chars = 5000
                        content_preview = fc['content'][:max_chars]
                        truncated = len(fc['content']) > max_chars
                        if truncated:
                            files_context += f"\n[DOCUMENT: {fc['name']} - First {max_chars} characters shown]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT (TRUNCATED)---\n"
                        else:
                            files_context += f"\n[DOCUMENT: {fc['name']} - Complete Content]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT---\n"

                files_context += "\n===== END OF DOCUMENT(S) =====\n\nUser's Question: "
                enhanced_prompt = files_context + prompt

            # For images, we'll add them separately to the API call
            vision_models = ["gpt-4o", "gpt-4o-mini", "claude-3-opus", "claude-3-5-sonnet", "claude-3-sonnet", "gemini-1.5-pro", "gemini-1.5-flash"]
            if has_images and model_choice not in vision_models:
                # Show error message with available vision models
                if lang == "zh-TW":
                    error_msg = """
                    ‚ö†Ô∏è **ÊâÄÈÅ∏Ê®°Âûã‰∏çÊîØÊè¥ÂúñÂÉèËº∏ÂÖ•**

                    ÊÇ®‰∏äÂÇ≥‰∫ÜÂúñÁâáÔºå‰ΩÜÁï∂ÂâçÈÅ∏ÊìáÁöÑÊ®°Âûã `{}` ÁÑ°Ê≥ïËôïÁêÜÂúñÂÉè„ÄÇ

                    **Ë´ãÈÅ∏Êìá‰ª•‰∏ãÊîØÊè¥Ë¶ñË¶∫ÂäüËÉΩÁöÑÊ®°Âûã‰πã‰∏ÄÔºö**
                    - gpt-4o (OpenAI)
                    - gpt-4o-mini (OpenAI)
                    - claude-3-5-sonnet (Anthropic)
                    - claude-3-opus (Anthropic)
                    - claude-3-sonnet (Anthropic)
                    - gemini-1.5-pro (Google)
                    - gemini-1.5-flash (Google)

                    Ë´ãÂæûÂ∑¶ÂÅ¥Ê¨ÑÊõ¥ÊèõÊ®°ÂûãÂæåÂÜçË©¶„ÄÇ
                    """.format(model_choice)
                else:
                    error_msg = """
                    ‚ö†Ô∏è **Selected model doesn't support vision**

                    You have uploaded image(s), but the current model `{}` cannot process images.

                    **Please select one of these vision-capable models:**
                    - gpt-4o (OpenAI)
                    - gpt-4o-mini (OpenAI)
                    - claude-3-5-sonnet (Anthropic)
                    - claude-3-opus (Anthropic)
                    - claude-3-sonnet (Anthropic)
                    - gemini-1.5-pro (Google)
                    - gemini-1.5-flash (Google)

                    Please change your model selection in the sidebar and try again.
                    """.format(model_choice)

                st.error(error_msg)
                st.stop()  # Prevent submission

        # RAG Knowledge Search
        rag_context = ""
        if rag_search_enabled:
            try:
                # Get selected doc IDs if any
                selected_doc_ids = st.session_state.get("selected_doc_ids", [])

                search_payload = {
                    "query": prompt,
                    "top_k": 3,
                    "similarity_threshold": 0.5
                }

                # Add doc_ids filter if specific documents are selected
                if selected_doc_ids:
                    search_payload["doc_ids"] = selected_doc_ids

                rag_response = requests.post(
                    f"{MCP_SERVER_URL}/rag/search",
                    json=search_payload,
                    timeout=10
                )

                if rag_response.status_code == 200:
                    rag_results = rag_response.json()
                    if rag_results.get('count', 0) > 0:
                        rag_context = "\n\n**[RAG Knowledge Context]**\n"
                        for result in rag_results['results']:
                            rag_context += f"\n- {result['title']} (similarity: {result['score']:.2f})\n"
                            rag_context += f"  {result['content'][:200]}...\n"

                        enhanced_prompt = f"{rag_context}\n\n**[User Question]**\n{enhanced_prompt}"
            except Exception as e:
                st.warning(f"RAG search failed: {str(e)}")

        if web_search_enabled:
            enhanced_prompt = f"[WEB_SEARCH_ENABLED] {enhanced_prompt}"

        # Add user message
        st.session_state.messages.append({
            "role": "user",
            "content": prompt,
            "files": file_info_list if file_info_list else None
        })

        with st.chat_message("user"):
            st.markdown(prompt)
            if file_info_list:
                with st.expander("üìé Attachments"):
                    for file_info in file_info_list:
                        st.caption(f"üìÑ {file_info['name']}")
            if rag_context:
                with st.expander("üìö RAG Knowledge Context"):
                    st.markdown(rag_context)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner(get_text("thinking", lang)):
                try:
                    start_time = time.time()

                    # Truncate conversation history to fit within context limits
                    truncated_history = truncate_conversation_history(
                        st.session_state.conversation_history,
                        model_choice
                    )

                    # Show truncation warning if needed
                    if len(truncated_history) < len(st.session_state.conversation_history):
                        removed = len(st.session_state.conversation_history) - len(truncated_history)
                        st.caption(f"‚ÑπÔ∏è {removed} " + ("Ê¢ùËàäÊ∂àÊÅØÂ∑≤ÁßªÈô§‰ª•ÁØÄÁúÅ‰∏ä‰∏ãÊñáÁ©∫Èñì" if lang == "zh-TW" else "old messages removed to save context"))

                    # Build request payload
                    request_payload = {
                        "task": enhanced_prompt,
                        "model": model_choice,
                        "conversation_history": truncated_history,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k
                    }

                    # Add image data for vision models
                    if has_images and file_contents:
                        image_files = [fc for fc in file_contents if fc["type"] == "image"]
                        if image_files:
                            # Add images to context
                            request_payload["images"] = []
                            for img in image_files:
                                request_payload["images"].append({
                                    "name": img["name"],
                                    "data": img["data"],
                                    "mime_type": img["mime_type"]
                                })

                    # Use /agent/execute endpoint with conversation history support
                    # Longer timeout for PDF processing with qwen2.5:7b (180 seconds)
                    response = requests.post(
                        f"{AGENT_SERVICE_URL}/agent/execute",
                        json=request_payload,
                        timeout=180
                    )

                    elapsed_time = time.time() - start_time

                    if response.ok:
                        result = response.json()
                        answer = result["result"]
                        needs_more_info = result.get("needs_more_info", False)
                        conversation_active = result.get("metadata", {}).get("conversation_active", False)

                        # Display the response
                        st.markdown(answer)

                        # Show conversation status
                        if needs_more_info or conversation_active:
                            st.info("üí¨ " + ("Ë´ãÁπºÁ∫åÊèê‰æõË≥áË®ä..." if lang == "zh-TW" else "Please provide more information..."))

                        # Show metadata
                        with st.expander(get_text("view_details", lang)):
                            metadata_display = {
                                get_text("model", lang): result.get("metadata", {}).get("model_used", model_choice),
                                get_text("response_time", lang): f"{elapsed_time:.2f}{get_text('seconds', lang)}",
                                "Tokens Used": result.get("metadata", {}).get("tokens_used", "N/A")
                            }

                            if needs_more_info:
                                metadata_display["Status"] = "Waiting for more info" if lang == "en" else "Á≠âÂæÖÊõ¥Â§öË≥áË®ä"

                            st.json(metadata_display)

                            # Show execution steps if available
                            if result.get("steps"):
                                st.markdown("**Execution Steps:**")
                                for step in result["steps"]:
                                    st.text(f"‚Ä¢ {step.get('step', 'Unknown')}: {step.get('status', 'unknown')}")

                        # Update conversation history for multi-stage conversations
                        if needs_more_info or conversation_active:
                            st.session_state.conversation_history.append({
                                "role": "user",
                                "content": prompt
                            })
                            st.session_state.conversation_history.append({
                                "role": "assistant",
                                "content": answer
                            })
                        else:
                            # Task completed, clear conversation history
                            st.session_state.conversation_history = []

                        # Add to display messages
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": answer,
                            "needs_more_info": needs_more_info
                        })
                    else:
                        error_msg = f"‚ùå {get_text('error', lang)}: {response.text}"
                        st.error(error_msg)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": error_msg
                        })
                        # Clear conversation history on error
                        st.session_state.conversation_history = []

                except requests.exceptions.Timeout:
                    error_msg = get_text("request_timeout", lang)
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
                    st.session_state.conversation_history = []
                except Exception as e:
                    error_msg = f"{get_text('request_failed', lang)}: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
                    st.session_state.conversation_history = []

with tab2:
    st.header(get_text("agent_header", lang))
    st.caption(get_text("agent_caption", lang))

    # Initialize selected_example in session state
    if "selected_example" not in st.session_state:
        st.session_state.selected_example = ""

    # Initialize agent tab conversation history
    if "agent_conversation_history" not in st.session_state:
        st.session_state.agent_conversation_history = []

    # Initialize agent conversation messages for display
    if "agent_messages" not in st.session_state:
        st.session_state.agent_messages = []

    # Initialize task input value
    if "task_input_value" not in st.session_state:
        st.session_state.task_input_value = ""

    # Show conversation history
    if st.session_state.agent_messages:
        st.subheader("üí¨ " + ("Â∞çË©±Ê≠∑Âè≤" if lang == "zh-TW" else "Conversation History"))
        for msg in st.session_state.agent_messages:
            if msg["role"] == "user":
                st.info(f"**You:** {msg['content']}")
            else:
                st.success(f"**Agent:** {msg['content']}")

        st.divider()

    col1, col2 = st.columns([3, 1])

    with col1:
        # Use task_input_value to control the text_area
        if st.session_state.selected_example:
            st.session_state.task_input_value = st.session_state.selected_example
            st.session_state.selected_example = ""

        task = st.text_area(
            get_text("describe_task", lang),
            height=150,
            placeholder=get_text("task_placeholder", lang),
            value=st.session_state.task_input_value,
            key="task_input"
        )

    with col2:
        agent_type = st.selectbox(
            get_text("agent_type", lang),
            ["general", "research", "analysis"],
            help=get_text("agent_type_help", lang)
        )

        execute_button = st.button(get_text("execute_task", lang), use_container_width=True)

        # Add clear conversation button
        if st.session_state.agent_conversation_history:
            if st.button("üîÑ " + ("ÈáçÁΩÆÂ∞çË©±" if lang == "zh-TW" else "Reset Conversation"), use_container_width=True):
                st.session_state.agent_conversation_history = []
                st.session_state.agent_messages = []
                st.rerun()

    if execute_button and task:
        # Clear the task input after execution starts
        st.session_state.selected_example = ""
        st.session_state.task_input_value = ""

        with st.spinner(get_text("executing", lang)):
            try:
                start_time = time.time()

                response = requests.post(
                    f"{AGENT_SERVICE_URL}/agent/execute",
                    json={
                        "task": task,
                        "agent_type": agent_type,
                        "model": model_choice,
                        "conversation_history": st.session_state.agent_conversation_history,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k
                    },
                    timeout=180
                )

                elapsed_time = time.time() - start_time

                if response.ok:
                    result = response.json()
                    needs_more_info = result.get("needs_more_info", False)
                    conversation_active = result.get("metadata", {}).get("conversation_active", False)

                    # Add user message to conversation display
                    st.session_state.agent_messages.append({
                        "role": "user",
                        "content": task
                    })

                    # Add agent response to conversation display
                    st.session_state.agent_messages.append({
                        "role": "assistant",
                        "content": result["result"]
                    })

                    # Update conversation history if more info needed
                    if needs_more_info or conversation_active:
                        st.session_state.agent_conversation_history.append({
                            "role": "user",
                            "content": task
                        })
                        st.session_state.agent_conversation_history.append({
                            "role": "assistant",
                            "content": result["result"]
                        })

                        # Show that conversation is active
                        st.info("üí¨ " + ("Ë´ãÂú®‰∏äÊñπÊñáÂ≠óÊ°ÜÁπºÁ∫åÊèê‰æõË≥áË®äÔºåÁÑ∂ÂæåÈªûÊìä„ÄåÂü∑Ë°å‰ªªÂãô„Äç" if lang == "zh-TW" else "Please provide more information in the text box above and click 'Execute Task'"))
                    else:
                        # Task completed, clear conversation history
                        st.session_state.agent_conversation_history = []
                        st.success(get_text("task_complete", lang, time=f"{elapsed_time:.2f}"))

                    # Show result
                    st.subheader(get_text("execution_result", lang))
                    st.write(result["result"])

                    # Show execution steps
                    with st.expander(get_text("view_steps", lang), expanded=True):
                        for i, step in enumerate(result["steps"], 1):
                            # Map status to icon
                            status = step.get("status", "unknown")
                            if status == "success":
                                status_icon = "‚úÖ"
                            elif status == "failed":
                                status_icon = "‚ùå"
                            elif status in ["detected", "executing"]:
                                status_icon = "üîç"
                            else:
                                status_icon = "‚ÑπÔ∏è"

                            st.write(f"{status_icon} **{get_text('step', lang)} {i}: {step['step']}**")

                            # Display step details based on what's available
                            if "result" in step:
                                # Check if result is a dict (tool execution result)
                                if isinstance(step["result"], dict):
                                    st.json(step["result"])
                                else:
                                    st.caption(step["result"])
                            elif "tool" in step:
                                st.caption(f"üîß Tool: **{step['tool']}**")
                                if "arguments" in step:
                                    # Show arguments inline with a toggle
                                    with st.container():
                                        st.caption("Arguments:")
                                        st.json(step["arguments"])
                            elif "error" in step:
                                st.error(f"‚ùå Error: {step['error']}")

                    # Show MCP Usage Information
                    mcp_usage = result.get("metadata", {}).get("mcp_usage", {})
                    if mcp_usage and any([mcp_usage.get("tools_used"), mcp_usage.get("resources_accessed")]):
                        with st.expander(get_text("mcp_usage", lang), expanded=False):
                            # Tools Used
                            if mcp_usage.get("tools_used"):
                                st.markdown(f"**{get_text('tools_used', lang)}** ({len(mcp_usage['tools_used'])})")
                                for idx, tool in enumerate(mcp_usage["tools_used"], 1):
                                    st.markdown(f"**{idx}. {tool['name']}**")
                                    with st.container():
                                        st.caption(f"üì• {get_text('arguments', lang)}:")
                                        st.json(tool.get("arguments", {}))
                                        if tool.get("result_summary"):
                                            st.caption(f"üì§ {get_text('result', lang)}: {tool['result_summary']}")
                                st.divider()

                            # Resources Accessed
                            if mcp_usage.get("resources_accessed"):
                                st.markdown(f"**{get_text('resources_accessed', lang)}** ({len(mcp_usage['resources_accessed'])})")
                                for resource in mcp_usage["resources_accessed"]:
                                    if resource["type"] == "document":
                                        st.caption(f"üìÑ Document ID: {resource['id']}")
                                    elif resource["type"] == "search":
                                        st.caption(f"üîç Search: {resource['query']}")
                                st.divider()

                            # Sampling Parameters
                            if mcp_usage.get("sampling_parameters"):
                                st.markdown(f"**{get_text('sampling_params', lang)}**")
                                params = mcp_usage["sampling_parameters"]
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Temperature", f"{params.get('temperature', 0.7):.2f}")
                                with col2:
                                    st.metric("Top-P", f"{params.get('top_p', 0.9):.2f}")
                                with col3:
                                    st.metric("Top-K", params.get('top_k', 40))
                                st.divider()

                            # System Prompt
                            if mcp_usage.get("system_prompt"):
                                st.markdown(f"**{get_text('system_prompt', lang)}**")
                                with st.container():
                                    st.text_area(
                                        label="",
                                        value=mcp_usage["system_prompt"],
                                        height=150,
                                        disabled=True,
                                        label_visibility="collapsed"
                                    )

                    # Show metadata
                    with st.expander(get_text("task_details", lang)):
                        metadata_display = result["metadata"].copy()
                        if needs_more_info:
                            metadata_display["conversation_status"] = "waiting_for_info"
                        # Remove mcp_usage from metadata display (shown above)
                        if "mcp_usage" in metadata_display:
                            del metadata_display["mcp_usage"]
                        st.json(metadata_display)

                    # Rerun to show updated conversation history
                    st.rerun()
                else:
                    st.error(f"{get_text('task_failed', lang)}: {response.text}")
                    # Clear conversation on error
                    st.session_state.agent_conversation_history = []

            except requests.exceptions.Timeout:
                st.error(get_text("task_timeout", lang))
                st.session_state.agent_conversation_history = []
            except Exception as e:
                st.error(f"{get_text('execution_failed', lang)}: {str(e)}")
                st.session_state.agent_conversation_history = []

    elif execute_button:
        st.warning(get_text("enter_task", lang))

    # Example tasks
    st.divider()

    col_header, col_button = st.columns([3, 1])
    with col_header:
        st.subheader(get_text("example_tasks", lang))
    with col_button:
        if st.button("üé≤ " + get_text("generate_example", lang), use_container_width=True):
            # Define a pool of example tasks for different languages
            example_pool = {
                "zh-TW": [
                    "ÁôºÈÄÅÈÉµ‰ª∂Áµ¶ team@example.comÔºå‰∏ªÊó®ÔºöÊØèÈÄ±ÊúÉË≠∞ÔºåÂÖßÂÆπÔºöÊèêÈÜíÂ§ßÂÆ∂Êú¨ÈÄ±‰∫î‰∏ãÂçà3ÈªûÈñãÊúÉ",
                    "ÂàÜÊûê‰∏äÂÄãÊúàÁöÑÈä∑ÂîÆÊï∏Êìö‰∏¶ÁîüÊàêÂ†±Âëä",
                    "ÊêúÁ¥¢ÈóúÊñº‰∫∫Â∑•Êô∫ËÉΩÊúÄÊñ∞Ë∂®Âã¢ÁöÑÊñáÁ´†",
                    "ÂâµÂª∫‰∏ÄÂÄã‰ªªÂãôÔºöÂÆåÊàêQ1Ë≤°ÂãôÂ†±Ë°®ÔºåÊà™Ê≠¢Êó•ÊúüÔºö‰∏ãÈÄ±‰∫î",
                    "Á∏ΩÁµêÈÄô‰ªΩÊñá‰ª∂ÁöÑ‰∏ªË¶ÅÂÖßÂÆπ",
                    "Ë®àÁÆóROIÔºöÂàùÂßãÊäïË≥á10Ëê¨ÔºåÂπ¥Êî∂Áõä3Ëê¨ÔºåÊúüÈôê5Âπ¥",
                    "ÁøªË≠ØÈÄôÊÆµÊñáÂ≠óÂà∞Ëã±ÊñáÔºöÊàëÂÄëÁöÑÁî¢ÂìÅÂú®Â∏ÇÂ†¥‰∏äË°®ÁèæÂÑ™Áï∞",
                    "‰ΩøÁî®Ë™ûÁæ©ÊêúÁ¥¢ÊâæÂà∞Ëàá'Ê©üÂô®Â≠∏Áøí'Áõ∏ÈóúÁöÑÊñáÊ™î"
                ],
                "zh-CN": [
                    "ÂèëÈÄÅÈÇÆ‰ª∂Áªô team@example.comÔºå‰∏ªÈ¢òÔºöÊØèÂë®‰ºöËÆÆÔºåÂÜÖÂÆπÔºöÊèêÈÜíÂ§ßÂÆ∂Êú¨Âë®‰∫î‰∏ãÂçà3ÁÇπÂºÄ‰ºö",
                    "ÂàÜÊûê‰∏ä‰∏™ÊúàÁöÑÈîÄÂîÆÊï∞ÊçÆÂπ∂ÁîüÊàêÊä•Âëä",
                    "ÊêúÁ¥¢ÂÖ≥‰∫é‰∫∫Â∑•Êô∫ËÉΩÊúÄÊñ∞Ë∂ãÂäøÁöÑÊñáÁ´†",
                    "ÂàõÂª∫‰∏Ä‰∏™‰ªªÂä°ÔºöÂÆåÊàêQ1Ë¥¢Âä°Êä•Ë°®ÔºåÊà™Ê≠¢Êó•ÊúüÔºö‰∏ãÂë®‰∫î",
                    "ÊÄªÁªìËøô‰ªΩÊñá‰ª∂ÁöÑ‰∏ªË¶ÅÂÜÖÂÆπ",
                    "ËÆ°ÁÆóROIÔºöÂàùÂßãÊäïËµÑ10‰∏áÔºåÂπ¥Êî∂Áõä3‰∏áÔºåÊúüÈôê5Âπ¥",
                    "ÁøªËØëËøôÊÆµÊñáÂ≠óÂà∞Ëã±ÊñáÔºöÊàë‰ª¨ÁöÑ‰∫ßÂìÅÂú®Â∏ÇÂú∫‰∏äË°®Áé∞‰ºòÂºÇ",
                    "‰ΩøÁî®ËØ≠‰πâÊêúÁ¥¢ÊâæÂà∞‰∏é'Êú∫Âô®Â≠¶‰π†'Áõ∏ÂÖ≥ÁöÑÊñáÊ°£"
                ],
                "en": [
                    "Send email to team@example.com, subject: Weekly Meeting, body: Reminder for Friday 3pm meeting",
                    "Analyze last month's sales data and generate a report",
                    "Search for articles about the latest AI trends",
                    "Create a task: Complete Q1 financial report, deadline: next Friday",
                    "Summarize the main points of this document",
                    "Calculate ROI: Initial investment $100k, annual return $30k, period 5 years",
                    "Translate this text to Chinese: Our product performs exceptionally well in the market",
                    "Use semantic search to find documents related to 'machine learning'"
                ],
                "vi": [
                    "G·ª≠i email ƒë·∫øn team@example.com, ch·ªß ƒë·ªÅ: Cu·ªôc h·ªçp h√†ng tu·∫ßn, n·ªôi dung: Nh·∫Øc nh·ªü cu·ªôc h·ªçp Th·ª© S√°u 3 gi·ªù chi·ªÅu",
                    "Ph√¢n t√≠ch d·ªØ li·ªáu b√°n h√†ng th√°ng tr∆∞·ªõc v√† t·∫°o b√°o c√°o",
                    "T√¨m ki·∫øm b√†i vi·∫øt v·ªÅ xu h∆∞·ªõng AI m·ªõi nh·∫•t",
                    "T·∫°o nhi·ªám v·ª•: Ho√†n th√†nh b√°o c√°o t√†i ch√≠nh Q1, h·∫°n ch√≥t: Th·ª© S√°u tu·∫ßn sau",
                    "T√≥m t·∫Øt c√°c ƒëi·ªÉm ch√≠nh c·ªßa t√†i li·ªáu n√†y",
                    "T√≠nh ROI: ƒê·∫ßu t∆∞ ban ƒë·∫ßu $100k, l·ª£i nhu·∫≠n h√†ng nƒÉm $30k, th·ªùi h·∫°n 5 nƒÉm",
                    "D·ªãch vƒÉn b·∫£n n√†y sang ti·∫øng Trung: S·∫£n ph·∫©m c·ªßa ch√∫ng t√¥i ho·∫°t ƒë·ªông r·∫•t t·ªët tr√™n th·ªã tr∆∞·ªùng",
                    "S·ª≠ d·ª•ng t√¨m ki·∫øm ng·ªØ nghƒ©a ƒë·ªÉ t√¨m t√†i li·ªáu li√™n quan ƒë·∫øn 'machine learning'"
                ]
            }

            # Get pool for current language
            pool = example_pool.get(lang, example_pool["en"])
            # Pick a random example
            random_example = random.choice(pool)
            st.session_state.selected_example = random_example
            st.rerun()

    examples = [
        get_text("example_1", lang),
        get_text("example_2", lang),
        get_text("example_3", lang),
        get_text("example_4", lang)
    ]

    cols = st.columns(2)
    for i, example in enumerate(examples):
        with cols[i % 2]:
            if st.button(f"üìã {example}", key=f"example_{i}"):
                st.session_state.selected_example = example
                st.rerun()

with tab3:
    st.header(get_text("agents_catalog_header", lang))
    st.caption(get_text("agents_catalog_caption", lang))

    # Agent Types Section
    st.subheader(get_text("agent_types", lang))

    agent_configs = {
        "general": {
            "name": get_text("agent_general", lang),
            "icon": "ü§ñ",
            "description": get_text("agent_general_desc", lang),
            "use_cases": get_text("agent_general_uses", lang),
            "prompt": """‰Ω†ÊòØ‰∏ÄÂÄã‰ºÅÊ•≠AIÂä©ÊâãÔºåÂèØ‰ª•Áõ¥Êé•ÂõûÁ≠îÂïèÈ°åÊàñ‰ΩøÁî®ÂêÑÁ®ÆÂ∑•ÂÖ∑‰æÜÂπ´Âä©Áî®Êà∂ÂÆåÊàê‰ªªÂãô„ÄÇ

ÈáçË¶ÅÊåáÂçóÔºö

üìÑ **Êñá‰ª∂ÂàÜÊûêÊ®°Âºè**Ôºö
- Â¶ÇÊûúÁî®Êà∂‰∏äÂÇ≥‰∫ÜÊñá‰ª∂ÔºàPDF„ÄÅÊñáÊú¨Á≠âÔºâ‰∏¶Ë©¢ÂïèÂÖßÂÆπÔºåÁõ¥Êé•ÂàÜÊûêÊñá‰ª∂‰∏¶ÂõûÁ≠îÂïèÈ°å
- ‰∏çÈúÄË¶Å‰ΩøÁî®Â∑•ÂÖ∑ÔºåÁõ¥Êé•Èñ±ËÆÄÊèê‰æõÁöÑÊñá‰ª∂ÂÖßÂÆπ‰∏¶ÈÄ≤Ë°åÂàÜÊûê
- Á§∫‰æãÔºöÁî®Êà∂‰∏äÂÇ≥PDF‰∏¶Âïè"ÊèèËø∞ÈÄô‰ªΩÊñá‰ª∂" ‚Üí Áõ¥Êé•ÂàÜÊûêÊñá‰ª∂ÂÖßÂÆπ‰∏¶Ë©≥Á¥∞ÊèèËø∞

üõ†Ô∏è **Â∑•ÂÖ∑‰ΩøÁî®Ê®°Âºè**Ôºö
1. Áï∂Áî®Êà∂Ë¶ÅÊ±ÇÂü∑Ë°åÊüêÂÄãÊìç‰ΩúÊôÇÔºàÂ¶ÇÁôºÈÄÅÈÉµ‰ª∂„ÄÅÂâµÂª∫‰ªªÂãô„ÄÅÊêúÁ¥¢Á≠âÔºâÔºåË´ãË™øÁî®Áõ∏ÊáâÁöÑÂ∑•ÂÖ∑
2. Âú®Ë™øÁî®Â∑•ÂÖ∑‰πãÂâçÔºåÊ™¢Êü•ÊòØÂê¶ÊúâÊâÄÊúâÂøÖÈúÄÁöÑÂèÉÊï∏
3. Â¶ÇÊûúÁº∫Â∞ëÂøÖÈúÄÂèÉÊï∏ÔºàÂ¶ÇemailÂú∞ÂùÄ„ÄÅsubject„ÄÅbodyÁ≠âÔºâÔºå‰∏çË¶ÅÁåúÊ∏¨Êàñ‰ΩøÁî®ÈªòË™çÂÄº
4. Â¶ÇÊûú‰ø°ÊÅØ‰∏çË∂≥ÔºåË´ãÁ¶ÆË≤åÂú∞Ë©¢ÂïèÁî®Êà∂Êèê‰æõÁº∫Â∞ëÁöÑ‰ø°ÊÅØ
5. ‰∏ÄÊ¨°Âè™Ë©¢ÂïèÁº∫Â∞ëÁöÑ‰ø°ÊÅØÔºå‰∏çË¶ÅÂïè‰∏çÂøÖË¶ÅÁöÑÂïèÈ°å
6. Êî∂ÈõÜÂà∞ÊâÄÊúâÂøÖÈúÄ‰ø°ÊÅØÂæåÔºåÁ´ãÂç≥Âü∑Ë°åÊìç‰Ωú

Á§∫‰æãÔºö
- Áî®Êà∂Ë™™"send email"‰ΩÜÊ≤íÊúâÊèê‰æõÊî∂‰ª∂‰∫∫ ‚Üí Ë©¢ÂïèÊî∂‰ª∂‰∫∫emailÂú∞ÂùÄ
- Áî®Êà∂Ë™™"send email to john@example.com"‰ΩÜÊ≤íÊúâ‰∏ªÊó®ÂíåÂÖßÂÆπ ‚Üí Ë©¢ÂïèÈÉµ‰ª∂‰∏ªÊó®ÂíåÂÖßÂÆπ
- Áî®Êà∂Êèê‰æõ‰∫ÜÊâÄÊúâ‰ø°ÊÅØ ‚Üí Áõ¥Êé•Âü∑Ë°åÁôºÈÄÅÈÉµ‰ª∂"""
        },
        "research": {
            "name": get_text("agent_research", lang),
            "icon": "üî¨",
            "description": get_text("agent_research_desc", lang),
            "use_cases": get_text("agent_research_uses", lang),
            "prompt": """‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÁ†îÁ©∂Âä©ÊâãÔºåÊìÖÈï∑‰ø°ÊÅØÊî∂ÈõÜ„ÄÅÂàÜÊûêÂíåÊï¥ÁêÜ„ÄÇ

‰Ω†ÁöÑÂ∞àÈï∑Ôºö
1. ‰ΩøÁî®ÊêúÁ¥¢Â∑•ÂÖ∑Ôºàsearch_knowledge_base, web_search, semantic_searchÔºâÊ∑±ÂÖ•Á†îÁ©∂‰∏ªÈ°å
2. ÊâæÂà∞Áõ∏ÈóúÊñáÊ™î‰∏¶ÊèêÂèñÈóúÈçµ‰ø°ÊÅØ
3. Êï¥ÂêàÂ§öÂÄã‰æÜÊ∫êÁöÑ‰ø°ÊÅØÔºåÊèê‰æõÂÖ®Èù¢ÁöÑÁ†îÁ©∂Â†±Âëä
4. È©óË≠â‰ø°ÊÅØÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁõ∏ÈóúÊÄß
5. Êèê‰æõÂºïÁî®Âíå‰æÜÊ∫ê

Â∑•‰ΩúÊñπÂºèÔºö
- Êî∂Âà∞Á†îÁ©∂‰ªªÂãôÊôÇÔºåÂÖàË¶èÂäÉÊêúÁ¥¢Á≠ñÁï•
- ‰ΩøÁî®Â§öÂÄãÊêúÁ¥¢Â∑•ÂÖ∑‰∫§ÂèâÈ©óË≠â‰ø°ÊÅØ
- Êï¥ÁêÜÁôºÁèæÁöÑ‰ø°ÊÅØÔºå‰ª•ÁµêÊßãÂåñÊñπÂºèÂëàÁèæ
- ÂøÖË¶ÅÊôÇ‰ΩøÁî® summarize_document Â∑•ÂÖ∑Á∏ΩÁµêÈï∑ÊñáÊ™î
- Êèê‰æõÊ∏ÖÊô∞ÁöÑÁ†îÁ©∂ÁµêË´ñÂíåÂª∫Ë≠∞

ÈáçÈªûÔºöÊ∑±Â∫¶„ÄÅÊ∫ñÁ¢∫ÊÄß„ÄÅ‰æÜÊ∫êÂèØÈù†ÊÄß"""
        },
        "analysis": {
            "name": get_text("agent_analysis", lang),
            "icon": "üìä",
            "description": get_text("agent_analysis_desc", lang),
            "use_cases": get_text("agent_analysis_uses", lang),
            "prompt": """‰Ω†ÊòØ‰∏ÄÂÄãÊï∏ÊìöÂàÜÊûêÂ∞àÂÆ∂ÔºåÂ∞àÊ≥®ÊñºÊï∏ÊìöËôïÁêÜ„ÄÅÂàÜÊûêÂíåÂèØË¶ñÂåñ„ÄÇ

‰Ω†ÁöÑÂ∞àÈï∑Ôºö
1. ‰ΩøÁî® analyze_data Â∑•ÂÖ∑ÈÄ≤Ë°åÁµ±Ë®àÂàÜÊûê
2. ‰ΩøÁî® process_csv ËôïÁêÜÂíåÊ∏ÖÁêÜÊï∏Êìö
3. ‰ΩøÁî® generate_chart ÂâµÂª∫Êï∏ÊìöÂèØË¶ñÂåñ
4. ‰ΩøÁî® calculate_metrics Ë®àÁÆóÊ•≠ÂãôÊåáÊ®ô
5. ‰ΩøÁî® financial_calculator ÈÄ≤Ë°åË≤°ÂãôÂàÜÊûê

Â∑•‰ΩúÊµÅÁ®ãÔºö
- ÁêÜËß£Êï∏ÊìöÂàÜÊûêÈúÄÊ±Ç
- Ê™¢Êü•Êï∏ÊìöË≥™ÈáèÂíåÂÆåÊï¥ÊÄß
- ÈÅ∏ÊìáÈÅ©Áï∂ÁöÑÂàÜÊûêÊñπÊ≥ï
- ÁîüÊàêÊ∏ÖÊô∞ÁöÑÂúñË°®ÂíåÂ†±Ë°®
- Êèê‰æõÊï∏ÊìöÈ©ÖÂãïÁöÑË¶ãËß£ÂíåÂª∫Ë≠∞

ÈáçÈªûÔºöÊï∏ÊìöÊ∫ñÁ¢∫ÊÄß„ÄÅÂàÜÊûêÊ∑±Â∫¶„ÄÅÂèØË¶ñÂåñÊ∏ÖÊô∞Â∫¶„ÄÅactionable insights"""
        }
    }

    cols = st.columns(3)
    for idx, (agent_id, config) in enumerate(agent_configs.items()):
        with cols[idx]:
            st.markdown(f"### {config['icon']} {config['name']}")
            st.caption(config['description'])
            st.markdown(f"**{get_text('use_cases', lang)}:**")
            st.markdown(config['use_cases'])

            with st.expander(get_text("view_system_prompt", lang)):
                st.text_area(
                    label="",
                    value=config['prompt'],
                    height=200,
                    disabled=True,
                    label_visibility="collapsed",
                    key=f"prompt_{agent_id}"
                )

    st.divider()

    # Default Sampling Parameters
    st.subheader(get_text("default_sampling", lang))

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Temperature", "0.7", help=get_text("default_temp_help", lang))
    with col2:
        st.metric("Top-P", "0.9", help=get_text("default_topp_help", lang))
    with col3:
        st.metric("Top-K", "40", help=get_text("default_topk_help", lang))

    st.info(get_text("sampling_info", lang))

    st.divider()

    # MCP Tools Section
    st.subheader(get_text("available_tools", lang))

    # Fetch tools from MCP server
    try:
        mcp_url = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")
        response = requests.get(f"{mcp_url}/tools/list", timeout=5)
        if response.ok:
            tools_data = response.json()
            tools = tools_data.get("tools", [])

            st.success(f"{get_text('tools_loaded', lang)}: {len(tools)} {get_text('tools', lang)}")

            # Group tools by category
            categorized_tools = {}
            for tool in tools:
                category = tool.get("category", "other")
                if category not in categorized_tools:
                    categorized_tools[category] = []
                categorized_tools[category].append(tool)

            # Display tools by category
            for category, category_tools in sorted(categorized_tools.items()):
                with st.expander(f"üìÇ {category.title()} ({len(category_tools)})"):
                    for tool in category_tools:
                        st.markdown(f"**{tool['name']}**")
                        st.caption(f"üìù {tool['description']}")
                        if tool.get("parameters"):
                            params_str = ", ".join([f"`{k}`: {v}" for k, v in tool["parameters"].items()])
                            st.caption(f"‚öôÔ∏è {get_text('parameters', lang)}: {params_str}")
                        st.markdown("---")
        else:
            st.warning(get_text("tools_load_failed", lang))
            st.caption(f"Status: {response.status_code}")
    except Exception as e:
        st.error(f"{get_text('tools_load_error', lang)}: {str(e)}")

    st.divider()

    # Resources Section
    st.subheader(get_text("available_resources", lang))

    resource_types = {
        get_text("resource_documents", lang): {
            "icon": "üìÑ",
            "description": get_text("resource_documents_desc", lang),
            "access": get_text("resource_documents_access", lang)
        },
        get_text("resource_knowledge_base", lang): {
            "icon": "üìö",
            "description": get_text("resource_knowledge_desc", lang),
            "access": get_text("resource_knowledge_access", lang)
        },
        get_text("resource_web", lang): {
            "icon": "üåê",
            "description": get_text("resource_web_desc", lang),
            "access": get_text("resource_web_access", lang)
        },
        get_text("resource_databases", lang): {
            "icon": "üóÑÔ∏è",
            "description": get_text("resource_databases_desc", lang),
            "access": get_text("resource_databases_access", lang)
        }
    }

    for resource_name, resource_info in resource_types.items():
        with st.container():
            col_icon, col_content = st.columns([1, 9])
            with col_icon:
                st.markdown(f"<div style='font-size: 2rem; text-align: center;'>{resource_info['icon']}</div>", unsafe_allow_html=True)
            with col_content:
                st.markdown(f"**{resource_name}**")
                st.caption(resource_info['description'])
                st.caption(f"üîë {get_text('access_via', lang)}: {resource_info['access']}")
            st.markdown("---")

with tab4:
    st.header(get_text("models_config_header", lang))
    st.caption(get_text("models_config_caption", lang))

    # Load litellm config
    import yaml
    import os

    config_path = "/app/config/litellm-config.yaml"

    def load_litellm_config():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            st.error(f"Error loading config: {str(e)}")
            return None

    def save_litellm_config(config_data):
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
            return True
        except Exception as e:
            st.error(f"Error saving config: {str(e)}")
            return False

    # Load config
    litellm_config = load_litellm_config()

    if litellm_config:
        # Initialize session state for editing
        if 'editing_model' not in st.session_state:
            st.session_state.editing_model = None
        if 'adding_new_model' not in st.session_state:
            st.session_state.adding_new_model = False

        # Action buttons
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            if st.button(get_text("add_new_model", lang), type="primary"):
                st.session_state.adding_new_model = True
                st.session_state.editing_model = None
                st.rerun()
        with col2:
            if st.button(get_text("reload_config", lang)):
                st.rerun()

        st.divider()

        # Add new model form
        if st.session_state.adding_new_model:
            with st.form("add_model_form"):
                st.subheader(get_text("add_new_model", lang))

                col1, col2 = st.columns(2)
                with col1:
                    new_model_name = st.text_input(get_text("model_name", lang), placeholder="gpt-4")
                    new_display_name = st.text_input(get_text("display_name", lang), placeholder="GPT-4 (OpenAI)")
                    new_model_provider = st.selectbox(
                        get_text("provider_type", lang),
                        ["openai", "anthropic", "ollama", "gemini", "custom"]
                    )
                    new_model_id = st.text_input(get_text("model_id", lang), placeholder="openai/gpt-4")

                with col2:
                    new_api_base = st.text_input(get_text("api_base", lang), placeholder="https://api.openai.com/v1")
                    new_api_key = st.text_input(
                        get_text("api_key", lang),
                        placeholder="os.environ/OPENAI_API_KEY or actual key",
                        type="password"
                    )
                    new_visible = st.checkbox(
                        get_text("visible_in_selection", lang),
                        value=True,
                        help=get_text("visible_help", lang)
                    )

                col_submit, col_cancel = st.columns([1, 5])
                with col_submit:
                    submitted = st.form_submit_button(get_text("save", lang), type="primary")
                with col_cancel:
                    cancelled = st.form_submit_button(get_text("cancel", lang))

                if submitted and new_model_name and new_model_id:
                    # Create new model entry
                    new_model = {
                        "model_name": new_model_name,
                        "visible": new_visible,
                        "litellm_params": {
                            "model": new_model_id
                        }
                    }

                    # Add display_name if provided
                    if new_display_name:
                        new_model["display_name"] = new_display_name

                    if new_api_base:
                        new_model["litellm_params"]["api_base"] = new_api_base
                    if new_api_key:
                        new_model["litellm_params"]["api_key"] = new_api_key

                    # Add to config
                    if "model_list" not in litellm_config:
                        litellm_config["model_list"] = []
                    litellm_config["model_list"].append(new_model)

                    if save_litellm_config(litellm_config):
                        st.success(get_text("model_added_success", lang))
                        st.session_state.adding_new_model = False
                        st.rerun()

                if cancelled:
                    st.session_state.adding_new_model = False
                    st.rerun()

        # Display models
        st.subheader(get_text("available_models", lang))

        if "model_list" in litellm_config:
            for idx, model in enumerate(litellm_config["model_list"]):
                model_name = model.get("model_name", "Unknown")
                display_name = model.get("display_name", model_name)
                litellm_params = model.get("litellm_params", {})
                model_id = litellm_params.get("model", "")
                api_base = litellm_params.get("api_base", "")
                api_key = litellm_params.get("api_key", "")

                # Determine provider
                provider = "Unknown"
                if "openai" in model_id.lower():
                    provider = "OpenAI"
                elif "anthropic" in model_id.lower() or "claude" in model_id.lower():
                    provider = "Anthropic"
                elif "ollama" in model_id.lower():
                    provider = "Ollama"
                elif "gemini" in model_id.lower():
                    provider = "Google"
                elif "llama" in model_name.lower():
                    provider = "Taiwan Gov"

                # Check if this model is being edited
                is_editing = st.session_state.editing_model == idx

                with st.expander(f"**{display_name}** ({model_name}) - {provider}", expanded=is_editing):
                    if is_editing:
                        # Edit mode
                        with st.form(f"edit_model_form_{idx}"):
                            col1, col2 = st.columns(2)
                            with col1:
                                edit_model_name = st.text_input(
                                    get_text("model_name", lang),
                                    value=model_name,
                                    key=f"edit_name_{idx}"
                                )
                                edit_display_name = st.text_input(
                                    get_text("display_name", lang),
                                    value=display_name,
                                    key=f"edit_display_{idx}"
                                )
                                edit_model_id = st.text_input(
                                    get_text("model_id", lang),
                                    value=model_id,
                                    key=f"edit_id_{idx}"
                                )

                            with col2:
                                edit_api_base = st.text_input(
                                    get_text("api_base", lang),
                                    value=api_base,
                                    key=f"edit_base_{idx}"
                                )
                                # Mask API key display
                                display_key = api_key if len(api_key) < 20 else f"{api_key[:10]}...{api_key[-10:]}"
                                edit_api_key = st.text_input(
                                    get_text("api_key", lang),
                                    value=api_key,
                                    type="password",
                                    key=f"edit_key_{idx}"
                                )
                                edit_visible = st.checkbox(
                                    get_text("visible_in_selection", lang),
                                    value=model.get("visible", True),
                                    key=f"edit_visible_{idx}",
                                    help=get_text("visible_help", lang)
                                )

                            col_save, col_cancel, col_delete = st.columns([1, 1, 4])
                            with col_save:
                                save_clicked = st.form_submit_button(get_text("save", lang), type="primary")
                            with col_cancel:
                                cancel_clicked = st.form_submit_button(get_text("cancel", lang))
                            with col_delete:
                                delete_clicked = st.form_submit_button(get_text("delete", lang), type="secondary")

                            if save_clicked:
                                # Update model
                                litellm_config["model_list"][idx]["model_name"] = edit_model_name
                                litellm_config["model_list"][idx]["visible"] = edit_visible
                                if edit_display_name:
                                    litellm_config["model_list"][idx]["display_name"] = edit_display_name
                                litellm_config["model_list"][idx]["litellm_params"]["model"] = edit_model_id
                                if edit_api_base:
                                    litellm_config["model_list"][idx]["litellm_params"]["api_base"] = edit_api_base
                                else:
                                    litellm_config["model_list"][idx]["litellm_params"].pop("api_base", None)
                                if edit_api_key:
                                    litellm_config["model_list"][idx]["litellm_params"]["api_key"] = edit_api_key
                                else:
                                    litellm_config["model_list"][idx]["litellm_params"].pop("api_key", None)

                                if save_litellm_config(litellm_config):
                                    st.success(get_text("model_updated_success", lang))
                                    st.session_state.editing_model = None
                                    st.rerun()

                            if cancel_clicked:
                                st.session_state.editing_model = None
                                st.rerun()

                            if delete_clicked:
                                # Delete model
                                litellm_config["model_list"].pop(idx)
                                if save_litellm_config(litellm_config):
                                    st.success(get_text("model_deleted_success", lang))
                                    st.session_state.editing_model = None
                                    st.rerun()
                    else:
                        # View mode
                        col1, col2, col3 = st.columns([3, 3, 1])
                        with col1:
                            st.markdown(f"**{get_text('model_name', lang)}:** `{model_name}`")
                            st.markdown(f"**{get_text('model_id', lang)}:** `{model_id}`")
                            # Show visibility status
                            is_visible = model.get("visible", True)
                            visibility_icon = "‚úÖ" if is_visible else "‚ùå"
                            visibility_text = get_text("visible", lang) if is_visible else get_text("hidden", lang)
                            st.markdown(f"**{get_text('visibility', lang)}:** {visibility_icon} {visibility_text}")
                        with col2:
                            st.markdown(f"**{get_text('provider', lang)}:** {provider}")
                            if api_base:
                                st.markdown(f"**{get_text('api_base', lang)}:** `{api_base}`")
                            if api_key:
                                masked_key = f"{api_key[:10]}..." if len(api_key) > 10 else "***"
                                st.markdown(f"**{get_text('api_key', lang)}:** `{masked_key}`")
                        with col3:
                            if st.button(get_text("edit", lang), key=f"edit_btn_{idx}"):
                                st.session_state.editing_model = idx
                                st.session_state.adding_new_model = False
                                st.rerun()

        # Config file info
        st.divider()
        st.info(f"{get_text('config_file_location', lang)}: `{config_path}`")

        # Raw config viewer
        with st.expander(get_text("view_raw_config", lang)):
            st.code(yaml.dump(litellm_config, default_flow_style=False, allow_unicode=True), language="yaml")

with tab5:
    st.header(get_text("monitor_header", lang))
    st.caption(get_text("monitor_caption", lang))

    col1, col2, col3 = st.columns(3)

    # Simulated metrics (should be fetched from Prometheus)
    with col1:
        st.metric(
            label=get_text("agent_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    with col2:
        st.metric(
            label=get_text("llm_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    with col3:
        st.metric(
            label=get_text("mcp_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    st.divider()

    # Monitoring links
    st.subheader(get_text("monitor_tools", lang))

    col1, col2 = st.columns(2)

    with col1:
        st.markdown(f"""
        **{get_text("grafana_dashboard", lang)}**
        - {get_text("grafana_url", lang)}: http://localhost:3000
        - {get_text("grafana_account", lang)}: admin
        - {get_text("grafana_password", lang)}: admin
        - {get_text("grafana_features", lang)}
        """)

    with col2:
        st.markdown(f"""
        **{get_text("prometheus", lang)}**
        - {get_text("grafana_url", lang)}: http://localhost:9090
        - {get_text("prometheus_features", lang)}
        """)

    st.info(get_text("monitor_tip", lang))

with tab6:
    st.header(get_text("rag_header", lang))
    st.caption(get_text("rag_caption", lang))

    # Create columns for layout
    col1, col2 = st.columns([1, 1])

    with col1:
        # Document Upload Section
        st.subheader(get_text("rag_upload_section", lang))

        uploaded_file = st.file_uploader(
            get_text("rag_upload_file", lang),
            type=['pdf', 'docx', 'txt'],
            help=get_text("rag_upload_help", lang)
        )

        doc_title = st.text_input(get_text("rag_doc_title", lang))
        doc_category = st.text_input(get_text("rag_doc_category", lang), value="General")
        doc_tags = st.text_input(get_text("rag_doc_tags", lang), placeholder="AI, Documentation, Enterprise")

        if st.button(get_text("rag_upload_button", lang)):
            if uploaded_file and doc_title:
                with st.spinner(get_text("rag_uploading", lang)):
                    try:
                        # Prepare file upload
                        files = {"file": (uploaded_file.name, uploaded_file.getvalue(), uploaded_file.type)}
                        data = {
                            "title": doc_title,
                            "category": doc_category,
                            "tags": doc_tags
                        }

                        # Upload to MCP server
                        response = requests.post(
                            f"{MCP_SERVER_URL}/rag/documents/upload",
                            files=files,
                            data=data,
                            timeout=30
                        )

                        if response.status_code == 200:
                            result = response.json()
                            st.success(get_text("rag_upload_success", lang).format(
                                doc_id=result.get('doc_id'),
                                chunks=result.get('chunks_count')
                            ))
                        else:
                            st.error(f"{get_text('rag_upload_error', lang)}: {response.text}")
                    except Exception as e:
                        st.error(f"{get_text('rag_upload_error', lang)}: {str(e)}")
            else:
                st.warning("Please provide both file and title!")

        st.divider()

        # Manual Text Input Section (for scanned PDFs or direct input)
        st.subheader("üìù " + ("Áõ¥Êé•Ëº∏ÂÖ•ÊñáÊú¨" if lang == "zh-TW" else "Direct Text Input"))
        st.caption("Alternative: Create document from text (useful for scanned PDFs)")

        with st.expander("Create document from text"):
            text_doc_title = st.text_input("Document Title", key="text_title")
            text_doc_content = st.text_area("Content", height=200, key="text_content",
                                           placeholder="Paste or type your document content here...")
            text_doc_category = st.text_input("Category", value="General", key="text_category")
            text_doc_tags = st.text_input("Tags (comma-separated)", key="text_tags")

            if st.button("Create from Text", key="create_text_doc"):
                if text_doc_title and text_doc_content:
                    with st.spinner("Creating document..."):
                        try:
                            response = requests.post(
                                f"{MCP_SERVER_URL}/rag/documents/text",
                                json={
                                    "title": text_doc_title,
                                    "content": text_doc_content,
                                    "category": text_doc_category,
                                    "tags": [t.strip() for t in text_doc_tags.split(",")] if text_doc_tags else []
                                },
                                timeout=30
                            )

                            if response.status_code == 200:
                                result = response.json()
                                st.success(f"‚úÖ Document created! ID: {result.get('doc_id')}, Chunks: {result.get('chunks_count')}")
                            else:
                                st.error(f"‚ùå Error: {response.text}")
                        except Exception as e:
                            st.error(f"‚ùå Error: {str(e)}")
                else:
                    st.warning("Please provide both title and content!")

        st.divider()

        # Semantic Search Section
        st.subheader(get_text("rag_search_section", lang))

        search_query = st.text_input(
            get_text("rag_search_query", lang),
            placeholder=get_text("rag_search_placeholder", lang)
        )

        col_search1, col_search2 = st.columns(2)
        with col_search1:
            top_k = st.slider(get_text("rag_search_topk", lang), 1, 10, 5)
        with col_search2:
            threshold = st.slider(get_text("rag_search_threshold", lang), 0.0, 1.0, 0.5, 0.05)

        if st.button(get_text("rag_search_button", lang)):
            if search_query:
                with st.spinner(get_text("rag_searching", lang)):
                    try:
                        response = requests.post(
                            f"{MCP_SERVER_URL}/rag/search",
                            json={
                                "query": search_query,
                                "top_k": top_k,
                                "similarity_threshold": threshold
                            },
                            timeout=15
                        )

                        if response.status_code == 200:
                            results = response.json()

                            st.subheader(get_text("rag_search_results", lang))

                            if results.get('count', 0) > 0:
                                for i, result in enumerate(results['results'], 1):
                                    with st.expander(f"{i}. {result['title']} - {get_text('rag_result_score', lang)}: {result['score']:.3f}"):
                                        st.write(f"**{get_text('rag_doc_id', lang)}:** {result['doc_id']}")
                                        st.write(f"**{get_text('rag_result_content', lang)}:**")
                                        st.write(result['content'])
                            else:
                                st.info(get_text("rag_no_results", lang))
                        else:
                            st.error(f"Search failed: {response.text}")
                    except Exception as e:
                        st.error(f"Search error: {str(e)}")

    with col2:
        # Document List Section
        st.subheader(get_text("rag_docs_section", lang))

        try:
            response = requests.get(f"{MCP_SERVER_URL}/rag/documents?limit=10", timeout=5)

            if response.status_code == 200:
                docs_data = response.json()
                total = docs_data.get('total', 0)
                st.caption(get_text("rag_docs_total", lang).format(total=total))

                for doc in docs_data.get('documents', []):
                    with st.expander(f"üìÑ {doc['title']}"):
                        st.write(f"**{get_text('rag_doc_id', lang)}:** {doc['id']}")
                        st.write(f"**{get_text('rag_doc_category', lang)}:** {doc.get('category', 'N/A')}")
                        st.write(f"**{get_text('rag_doc_created', lang)}:** {doc.get('created_at', 'N/A')}")

                        if st.button(f"üóëÔ∏è {get_text('rag_doc_delete', lang)}", key=f"delete_{doc['id']}"):
                            try:
                                del_response = requests.delete(
                                    f"{MCP_SERVER_URL}/rag/documents/{doc['id']}",
                                    timeout=5
                                )
                                if del_response.status_code == 200:
                                    st.success("Document deleted!")
                                    st.rerun()
                                else:
                                    st.error(f"Delete failed: {del_response.text}")
                            except Exception as e:
                                st.error(f"Delete error: {str(e)}")
            else:
                st.error(f"Failed to load documents: {response.text}")
        except Exception as e:
            st.error(f"Error loading documents: {str(e)}")

        st.divider()

        # Stats Section
        st.subheader(get_text("rag_stats_section", lang))

        try:
            with st.spinner(get_text("rag_stats_loading", lang)):
                response = requests.get(f"{MCP_SERVER_URL}/rag/stats", timeout=5)

                if response.status_code == 200:
                    stats = response.json()

                    col_stat1, col_stat2 = st.columns(2)
                    with col_stat1:
                        st.metric(
                            get_text("rag_stats_total_docs", lang),
                            stats.get('documents', {}).get('total', 0)
                        )
                    with col_stat2:
                        st.metric(
                            get_text("rag_stats_total_vectors", lang),
                            stats.get('vectors', {}).get('points_count', 0)
                        )

                    st.caption(f"{get_text('rag_stats_collection', lang)}: {stats.get('vectors', {}).get('collection_name', 'N/A')}")
                else:
                    st.error(f"Failed to load stats: {response.text}")
        except Exception as e:
            st.error(f"Error loading stats: {str(e)}")

with tab7:
    st.header("üìö Project Documentation")
    st.caption("Complete documentation for the AI Platform")

    # Documentation navigation
    doc_sections = {
        "üìñ Quick Start": "README.md",
        "üóÑÔ∏è Database Schema": "DATABASE_SCHEMA.md",
        "üîß Troubleshooting Guide": "TROUBLESHOOTING_GUIDE.md",
        "üß† Context-Aware Agent": "CONTEXT_AWARE_AGENT_GUIDE.md",
        "üìß SMTP Configuration": "SMTP_CONFIGURATION_GUIDE.md",
        "üì± LINE Messaging Setup": "LINE_SETUP_GUIDE.md",
        "‚úÖ Test Results": "TEST_RESULTS.md",
        "üöÄ Deployment Guide": "DEPLOYMENT_GUIDE.md",
        "üìù Changelog": "CHANGELOG.md",
        "üìä Project Summary": "PROJECT_SUMMARY.md"
    }

    # Create columns for documentation cards
    cols = st.columns(2)

    for idx, (title, filename) in enumerate(doc_sections.items()):
        with cols[idx % 2]:
            with st.container():
                st.subheader(title)

                # Read documentation file
                doc_path = f"/app/{filename}"  # In Docker container
                try:
                    if os.path.exists(doc_path):
                        with open(doc_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # Show preview
                        preview = content[:200] + "..." if len(content) > 200 else content
                        st.text(preview)

                        # View button
                        if st.button(f"üìÑ View {title}", key=f"view_{filename}"):
                            st.session_state['current_doc'] = filename
                            st.session_state['current_doc_title'] = title
                    else:
                        st.warning(f"Document not found: {filename}")
                except Exception as e:
                    st.error(f"Error loading {filename}: {str(e)}")

    st.divider()

    # Display selected document
    if 'current_doc' in st.session_state:
        doc_file = st.session_state['current_doc']
        doc_title = st.session_state.get('current_doc_title', doc_file)

        st.markdown(f"### üìñ {doc_title}")

        # Back button
        if st.button("‚¨ÖÔ∏è Back to Documentation List"):
            del st.session_state['current_doc']
            del st.session_state['current_doc_title']
            st.rerun()

        # Read and display full document
        doc_path = f"/app/{doc_file}"
        try:
            if os.path.exists(doc_path):
                with open(doc_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Display markdown content
                st.markdown(content)

                # Download button
                st.download_button(
                    label=f"‚¨áÔ∏è Download {doc_file}",
                    data=content,
                    file_name=doc_file,
                    mime="text/markdown"
                )
            else:
                st.error(f"Document not found: {doc_file}")
        except Exception as e:
            st.error(f"Error reading document: {str(e)}")
    else:
        # Show quick links when no document is selected
        st.markdown("""
        ### Quick Links

        - **Getting Started**: View README.md for quickstart guide
        - **Troubleshooting**: TROUBLESHOOTING_GUIDE.md - Complete problem-solving guide
        - **Context-Aware Agent**: CONTEXT_AWARE_AGENT_GUIDE.md - Natural language understanding
        - **Email Setup**: SMTP_CONFIGURATION_GUIDE.md - Configure real email sending
        - **LINE Messaging**: LINE_SETUP_GUIDE.md - Smart group/personal messaging with auto-detection
        - **Database**: See DATABASE_SCHEMA.md for schema details
        - **Testing**: Check TEST_RESULTS.md for 100% test coverage
        - **Deployment**: Follow DEPLOYMENT_GUIDE.md for production setup
        - **Changes**: Review CHANGELOG.md for version history
        - **Overview**: Read PROJECT_SUMMARY.md for executive summary

        ### External Documentation

        - [LiteLLM Docs](https://docs.litellm.ai/)
        - [FastAPI Docs](https://fastapi.tiangolo.com/)
        - [Streamlit Docs](https://docs.streamlit.io/)
        - [Qdrant Docs](https://qdrant.tech/documentation/)
        - [PostgreSQL Docs](https://www.postgresql.org/docs/)
        """)

        # Tools reference
        st.divider()
        st.subheader("üõ†Ô∏è Available Tools (28 Total)")

        tool_categories = {
            "Data Analysis & Processing (3)": [
                "analyze_data - Statistical analysis",
                "process_csv - CSV file processing",
                "generate_chart - Data visualization"
            ],
            "Search & Retrieval (3)": [
                "semantic_search - AI-driven search",
                "web_search - Web search integration",
                "find_similar_documents - Document similarity"
            ],
            "Content Generation (3)": [
                "summarize_document - Text summarization",
                "translate_text - Multi-language translation",
                "generate_report - Report generation"
            ],
            "Security & Compliance (3)": [
                "check_permissions - Access control",
                "audit_log - Audit logging",
                "scan_sensitive_data - PII detection"
            ],
            "Business Process (3)": [
                "create_task - Task management",
                "send_notification - Notifications",
                "schedule_meeting - Meeting scheduling"
            ],
            "System Integration (3)": [
                "call_api - External API calls",
                "execute_sql - SQL queries",
                "run_script - Script execution"
            ],
            "Communication (2)": [
                "send_email - Email sending",
                "create_slack_message - Slack integration"
            ],
            "File Management (3)": [
                "upload_file - File uploads",
                "download_file - File downloads",
                "list_files - File listing"
            ],
            "Calculation (2)": [
                "calculate_metrics - Business KPIs",
                "financial_calculator - ROI/NPV/IRR"
            ]
        }

        for category, tools in tool_categories.items():
            with st.expander(f"üìÇ {category}"):
                for tool in tools:
                    st.markdown(f"- `{tool}`")

with tab7:
    st.header(get_text("about_header", lang))

    st.markdown(f"""
    ### {get_text("about_title", lang)}

    {get_text("about_intro", lang)}

    #### {get_text("core_features", lang)}
    - üîÑ {get_text("feature_hybrid", lang)}
    - ü§ñ {get_text("feature_agent", lang)}
    - üìä {get_text("feature_monitor", lang)}
    - üîí {get_text("feature_security", lang)}

    #### {get_text("tech_stack", lang)}
    - {get_text("frontend", lang)}
    - {get_text("backend", lang)}
    - {get_text("llm_gateway", lang)}
    - {get_text("local_inference", lang)}
    - {get_text("database", lang)}
    - {get_text("monitoring", lang)}

    #### {get_text("supported_models", lang)}
    - {get_text("models_openai", lang)}
    - {get_text("models_anthropic", lang)}
    - {get_text("models_local", lang)}
    - {get_text("models_others", lang)}

    #### {get_text("usage_tips", lang)}
    1. {get_text("tip_1", lang)}
    2. {get_text("tip_2", lang)}
    3. {get_text("tip_3", lang)}

    #### {get_text("version_info", lang)}
    - {get_text("version", lang)}
    - {get_text("update_date", lang)}
    - {get_text("license", lang)}
    """)

    st.divider()

    st.markdown(f"""
    ### {get_text("tech_support", lang)}

    {get_text("support_intro", lang)}
    - {get_text("support_docs", lang)}
    - {get_text("support_troubleshoot", lang)}
    - {get_text("support_logs", lang)}
    """)
