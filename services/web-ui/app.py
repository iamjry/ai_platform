import streamlit as st
import requests
import os
import time
import base64
import random
import yaml
from io import BytesIO
from PyPDF2 import PdfReader
from i18n import LANGUAGES, get_text

# Helper functions for agent prompts configuration
def load_agent_prompts():
    """Load agent system prompts from config/agent_prompts.yaml"""
    prompts_path = "/app/config/agent_prompts.yaml"
    default_prompts = {
        "general": "ä½ æ˜¯ä¸€å€‹ä¼æ¥­AIåŠ©æ‰‹ï¼Œå¯ä»¥ç›´æ¥å›ç­”å•é¡Œæˆ–ä½¿ç”¨å„ç¨®å·¥å…·ä¾†å¹«åŠ©ç”¨æˆ¶å®Œæˆä»»å‹™ã€‚",
        "research": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•·ä¿¡æ¯æ”¶é›†ã€åˆ†æå’Œæ•´ç†ã€‚",
        "analysis": "ä½ æ˜¯ä¸€å€‹æ•¸æ“šåˆ†æå°ˆå®¶ï¼Œå°ˆæ³¨æ–¼æ•¸æ“šè™•ç†ã€åˆ†æå’Œå¯è¦–åŒ–ã€‚",
        "contract_review": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¥‘ç´„å¯©æŸ¥åŠ©æ‰‹ï¼Œå°ˆæ³¨æ–¼å¥‘ç´„åˆ†æã€é¢¨éšªè©•ä¼°å’Œåˆè¦æª¢æŸ¥ã€‚"
    }

    try:
        if os.path.exists(prompts_path):
            with open(prompts_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                if data and 'agent_prompts' in data:
                    return data['agent_prompts']
        return default_prompts
    except Exception as e:
        st.error(f"Error loading agent prompts: {e}")
        return default_prompts

def save_agent_prompts(prompts):
    """Save agent system prompts to config/agent_prompts.yaml"""
    prompts_path = "/app/config/agent_prompts.yaml"

    try:
        data = {
            "agent_prompts": prompts
        }
        with open(prompts_path, 'w', encoding='utf-8') as f:
            f.write("# Agent System Prompts Configuration\n")
            f.write("# Last Updated: " + time.strftime("%Y-%m-%d %H:%M:%S") + "\n")
            f.write("#\n")
            f.write("# This file contains the system prompts for different agent types.\n")
            f.write("# Edit these prompts to customize agent behavior.\n")
            f.write("#\n")
            f.write("# NOTE: After modifying this file, the changes take effect immediately.\n")
            f.write("#       Both web-ui and agent-service will reload prompts on next request.\n\n")
            yaml.safe_dump(data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        return True
    except Exception as e:
        st.error(f"Error saving agent prompts: {e}")
        return False

# Initialize session state for language
if "language" not in st.session_state:
    st.session_state.language = "zh-TW"  # Default to Traditional Chinese

lang = st.session_state.language

st.set_page_config(
    page_title=get_text("page_title", lang),
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

AGENT_SERVICE_URL = os.getenv("AGENT_SERVICE_URL", "http://agent-service:8000")
LITELLM_URL = os.getenv("LITELLM_URL", "http://litellm:4000")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")

# Model context limits (conservative estimates to account for system prompt and tools)
MODEL_CONTEXT_LIMITS = {
    "qwen2.5": {
        "total_tokens": 32000,
        "safe_limit": 24000,
        "avg_tokens_per_message": 150
    },
    "qwen2.5-7b": {
        "total_tokens": 32000,
        "safe_limit": 24000,
        "avg_tokens_per_message": 150
    },
    "gpt-3.5-turbo": {
        "total_tokens": 16385,
        "safe_limit": 12000,
        "avg_tokens_per_message": 100
    },
    "gpt-4": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "gpt-4o": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "gpt-4o-mini": {
        "total_tokens": 128000,
        "safe_limit": 96000,
        "avg_tokens_per_message": 150
    },
    "claude-3-opus": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-sonnet": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-5-sonnet": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "claude-3-haiku": {
        "total_tokens": 200000,
        "safe_limit": 150000,
        "avg_tokens_per_message": 200
    },
    "gemini-1.5-pro": {
        "total_tokens": 1000000,
        "safe_limit": 750000,
        "avg_tokens_per_message": 200
    },
    "gemini-1.5-flash": {
        "total_tokens": 1000000,
        "safe_limit": 750000,
        "avg_tokens_per_message": 150
    }
}

def estimate_tokens(text):
    """Rough estimate: ~4 characters per token"""
    return len(text) // 4

def truncate_conversation_history(history, model, max_messages=None):
    """Truncate conversation history to fit within model context limits"""
    if not history:
        return history

    limits = MODEL_CONTEXT_LIMITS.get(model, MODEL_CONTEXT_LIMITS["gpt-3.5-turbo"])

    # If max_messages specified, use that
    if max_messages:
        return history[-max_messages * 2:]  # Keep last N exchanges (user + assistant pairs)

    # Otherwise, estimate tokens and truncate
    estimated_tokens = sum(estimate_tokens(msg["content"]) for msg in history)

    if estimated_tokens <= limits["safe_limit"]:
        return history

    # Truncate from the beginning, keeping most recent messages
    # Always keep at least the last 4 messages (2 exchanges)
    min_messages = 4
    truncated = history[-min_messages:]

    # Add more messages if we have room
    for i in range(len(history) - min_messages - 1, -1, -1):
        msg = history[i]
        msg_tokens = estimate_tokens(msg["content"])
        current_tokens = sum(estimate_tokens(m["content"]) for m in truncated)

        if current_tokens + msg_tokens < limits["safe_limit"]:
            truncated.insert(0, msg)
        else:
            break

    return truncated

def get_context_usage_info(history, model):
    """Get context usage information"""
    if not history:
        return {"messages": 0, "estimated_tokens": 0, "percentage": 0}

    limits = MODEL_CONTEXT_LIMITS.get(model, MODEL_CONTEXT_LIMITS["gpt-3.5-turbo"])
    estimated_tokens = sum(estimate_tokens(msg["content"]) for msg in history)
    percentage = (estimated_tokens / limits["safe_limit"]) * 100

    return {
        "messages": len(history),
        "estimated_tokens": estimated_tokens,
        "safe_limit": limits["safe_limit"],
        "percentage": round(percentage, 1)
    }

# Custom CSS
st.markdown("""
<style>
    /* Main content area padding */
    .main > div {
        padding-top: 2rem !important;
    }

    .block-container {
        padding-top: 2rem !important;
        padding-bottom: 1rem !important;
    }

    /* Reduce sidebar padding aggressively */
    section[data-testid="stSidebar"] {
        padding-top: 0 !important;
    }

    section[data-testid="stSidebar"] > div {
        padding-top: 0.3rem !important;
        padding-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] > div > div {
        padding-top: 0 !important;
    }

    /* Ensure first element in sidebar has no extra margin */
    section[data-testid="stSidebar"] .element-container:first-child {
        margin-top: 0 !important;
        padding-top: 0 !important;
    }

    section[data-testid="stSidebar"] .element-container {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] hr {
        margin-top: 0.5rem !important;
        margin-bottom: 0.5rem !important;
    }

    section[data-testid="stSidebar"] .stSelectbox {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] [data-testid="stMarkdownContainer"] {
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] .stAlert {
        padding: 0.5rem !important;
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] .stColumns {
        gap: 0.3rem !important;
    }

    /* Reduce header spacing */
    h1, h2, h3 {
        margin-top: 0.3rem !important;
        margin-bottom: 0.3rem !important;
    }

    section[data-testid="stSidebar"] h1,
    section[data-testid="stSidebar"] h2,
    section[data-testid="stSidebar"] h3 {
        margin-top: 0.2rem !important;
        margin-bottom: 0.2rem !important;
        padding-top: 0 !important;
        padding-bottom: 0 !important;
    }

    /* Reduce caption spacing */
    .caption {
        margin-top: 0.2rem !important;
        margin-bottom: 0.2rem !important;
    }

    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 0.3rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .status-healthy {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
    }
    .status-unhealthy {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    # Logo and Title at top of sidebar
    st.markdown("""
    <div style="text-align: center; margin: 0; padding: 0;">
        <div style="font-size: 2rem; line-height: 1; margin: 0;">ğŸ¤–</div>
        <h3 style="margin: 0; padding: 0; color: #1f77b4; font-weight: 700; font-size: 1rem; line-height: 1;">AI Agents Platform</h3>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("---")

    st.markdown(f"### {get_text('settings', lang)}")

    # Language selector
    selected_lang = st.selectbox(
        get_text("language", lang),
        options=list(LANGUAGES.keys()),
        format_func=lambda x: LANGUAGES[x],
        index=list(LANGUAGES.keys()).index(st.session_state.language),
        key="lang_selector"
    )

    if selected_lang != st.session_state.language:
        st.session_state.language = selected_lang
        st.rerun()

    # Model options with display names
    # Load model options from litellm config
    import yaml

    def load_model_options():
        try:
            with open("/app/config/litellm-config.yaml", 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                model_options = {}
                if "model_list" in config:
                    for model in config["model_list"]:
                        # Only include models where visible is True (default to True if not specified)
                        is_visible = model.get("visible", True)
                        if not is_visible:
                            continue

                        model_name = model.get("model_name", "")
                        display_name = model.get("display_name", model_name)
                        if model_name and display_name:
                            model_options[display_name] = model_name
                return model_options
        except Exception as e:
            # Fallback to default if config can't be loaded
            return {
                "qwen2.5:7b (local - better for PDFs)": "qwen2.5-7b",
                "gpt-4o (OpenAI)": "gpt-4o"
            }

    model_options = load_model_options()

    model_display = st.selectbox(
        get_text("select_model", lang),
        options=list(model_options.keys()),
        help=get_text("model_help", lang)
    )

    model_choice = model_options[model_display]

    # Show model information
    st.markdown("---")
    st.markdown("### ğŸ“‹ " + ("æ¨¡å‹è³‡è¨Š" if lang == "zh-TW" else "Model Information"))

    # Get model info
    model_info = MODEL_CONTEXT_LIMITS.get(model_choice, {})

    # Display model details in a nice format
    col_info1, col_info2 = st.columns(2)

    with col_info1:
        # Model name and provider
        if model_choice.startswith("qwen"):
            st.info(f"**Provider:** Local (Ollama)\n\n**Status:** âœ… No API key needed")
        elif model_choice.startswith("gpt"):
            st.info(f"**Provider:** OpenAI\n\n**Status:** âš ï¸ API key required")
        elif model_choice.startswith("claude"):
            st.info(f"**Provider:** Anthropic\n\n**Status:** âš ï¸ API key required")
        elif model_choice.startswith("gemini"):
            st.info(f"**Provider:** Google\n\n**Status:** âš ï¸ API key required")
        elif model_choice.startswith("llama"):
            st.info(f"**Provider:** Taiwan Gov (AFSPOD)\n\n**Status:** âœ… API key configured")

    with col_info2:
        # Context window info
        if model_info:
            st.info(f"**Context Window:** {model_info.get('total_tokens', 'N/A'):,} tokens\n\n**Safe Limit:** {model_info.get('safe_limit', 'N/A'):,} tokens")

    # Model capabilities
    vision_models = ["gpt-4o", "gpt-4o-mini", "claude-3-opus", "claude-3-5-sonnet", "claude-3-sonnet", "gemini-1.5-pro", "gemini-1.5-flash"]
    capabilities = []
    if model_choice in vision_models:
        capabilities.append("ğŸ–¼ï¸ Vision")
    if model_choice.startswith("qwen") or model_choice.startswith("claude"):
        capabilities.append("ğŸ“„ PDF Analysis")
    if model_choice in ["gpt-4o", "gpt-4", "claude-3-opus", "claude-3-5-sonnet", "gemini-1.5-pro"]:
        capabilities.append("ğŸ§  Advanced Reasoning")

    if capabilities:
        st.caption("**Capabilities:** " + " â€¢ ".join(capabilities))

    st.markdown("---")

    # Sampling parameters
    st.subheader("ğŸ›ï¸ " + ("æ¡æ¨£åƒæ•¸" if lang == "zh-TW" else "Sampling Parameters"))

    temperature = st.slider(
        get_text("temperature", lang),
        0.0, 1.0, 0.7,
        help=get_text("temperature_help", lang)
    )

    top_p = st.slider(
        "Top-P (nucleus sampling)",
        0.0, 1.0, 0.9,
        help="Controls diversity via nucleus sampling. Lower values make output more focused, higher values more diverse."
    )

    top_k = st.slider(
        "Top-K",
        0, 100, 40,
        help="Limits sampling to top K tokens. 0 means no limit. Lower values make output more focused."
    )

    st.divider()

    # Always show Context Information section
    st.subheader("ğŸ’¬ " + ("å°è©±ä¸Šä¸‹æ–‡" if lang == "zh-TW" else "Context Info"))

    # Show model context limits
    if model_choice in MODEL_CONTEXT_LIMITS:
        limits = MODEL_CONTEXT_LIMITS[model_choice]
        st.info(f"ğŸ“Š **{model_choice}**\n\nMax Context: {limits['total_tokens']:,} tokens\nSafe Limit: {limits['safe_limit']:,} tokens")

    # Show context usage if there's conversation history
    if "conversation_history" in st.session_state and st.session_state.conversation_history:
        usage = get_context_usage_info(st.session_state.conversation_history, model_choice)

        # Color based on usage
        if usage["percentage"] < 50:
            color = "ğŸŸ¢"
        elif usage["percentage"] < 80:
            color = "ğŸŸ¡"
        else:
            color = "ğŸ”´"

        st.metric(
            label="Current Usage",
            value=f"{usage['percentage']}%",
            delta=f"{usage['messages']} messages"
        )
        st.caption(f"{color} {usage['estimated_tokens']:,} / {usage['safe_limit']:,} tokens")

        if usage["percentage"] > 80:
            st.warning("âš ï¸ " + ("æ¥è¿‘ä¸Šä¸‹æ–‡é™åˆ¶ï¼" if lang == "zh-TW" else "Near context limit!"))
    else:
        st.caption("ğŸ’­ " + ("é–‹å§‹å°è©±å¾Œæœƒé¡¯ç¤ºä½¿ç”¨æƒ…æ³" if lang == "zh-TW" else "Start a conversation to see usage"))

    st.divider()

    # System Status
    st.header(get_text("system_status", lang))

    with st.spinner(get_text("checking_status", lang)):
        # Check Agent service
        try:
            resp = requests.get(f"{AGENT_SERVICE_URL}/health", timeout=3)
            if resp.ok:
                st.success(get_text("agent_service_ok", lang))
                health_data = resp.json()
                if "services" in health_data:
                    for service, status in health_data["services"].items():
                        if "connected" in str(status):
                            st.text(f"  â””â”€ {service}: âœ“")
                        else:
                            st.text(f"  â””â”€ {service}: âœ—")
            else:
                st.error(get_text("agent_service_error", lang))
        except Exception as e:
            st.error(get_text("agent_service_offline", lang))
            st.caption(f"{get_text('error', lang)}: {str(e)}")

    st.divider()

    # Quick Actions
    st.header(get_text("quick_actions", lang))
    if st.button(get_text("clear_chat", lang)):
        st.session_state.messages = []
        st.session_state.conversation_history = []  # Also clear conversation history
        st.rerun()

    if st.button(get_text("export_chat", lang)):
        if "messages" in st.session_state and st.session_state.messages:
            conversation = "\n\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in st.session_state.messages
            ])
            st.download_button(
                get_text("download_chat", lang),
                conversation,
                file_name="conversation.txt",
                mime="text/plain"
            )
        else:
            st.info(get_text("no_chat_history", lang))

# Main Content
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
    get_text("tab_chat", lang),
    get_text("tab_agent", lang),
    get_text("tab_agents_catalog", lang),
    get_text("tab_models_config", lang),
    get_text("tab_monitor", lang),
    get_text("tab_rag", lang),
    "ğŸ“š Documentation",
    get_text("tab_about", lang)
])

with tab1:
    st.header(get_text("chat_header", lang))
    st.caption(get_text("chat_caption", lang))

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Initialize conversation history for multi-stage conversations
    if "conversation_history" not in st.session_state:
        st.session_state.conversation_history = []

    # Initialize uploaded files
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []

    # File upload and web search options (Claude.ai style)
    with st.expander("ğŸ“ Attachments & Options", expanded=False):
        col1, col2, col3 = st.columns([2, 1, 1])

        with col1:
            uploaded_files = st.file_uploader(
                "Upload files (images, documents, etc.)",
                accept_multiple_files=True,
                type=['png', 'jpg', 'jpeg', 'gif', 'pdf', 'txt', 'md', 'csv', 'json', 'xml'],
                key="file_uploader"
            )

        with col2:
            web_search_enabled = st.checkbox(
                "ğŸŒ Web Search",
                value=False,
                help="Enable web search for real-time information"
            )

        with col3:
            rag_search_enabled = st.checkbox(
                "ğŸ“š RAG Knowledge",
                value=False,
                help="Search knowledge base for relevant context"
            )

        # Document selection for RAG
        if rag_search_enabled:
            st.write("**Select documents to search:**")
            try:
                response = requests.get(f"{MCP_SERVER_URL}/rag/documents?limit=100", timeout=5)
                if response.status_code == 200:
                    docs_data = response.json()
                    documents = docs_data.get('documents', [])

                    if documents:
                        doc_options = {f"{doc['id']} - {doc['title']}": doc['id'] for doc in documents}
                        selected_docs = st.multiselect(
                            "Choose specific documents (optional - leave empty to search all)",
                            options=list(doc_options.keys()),
                            default=[],
                            key="rag_doc_selection"
                        )
                        # Store selected doc IDs in session state
                        if "selected_doc_ids" not in st.session_state:
                            st.session_state.selected_doc_ids = []
                        st.session_state.selected_doc_ids = [doc_options[doc] for doc in selected_docs]
                    else:
                        st.info("No documents available. Upload documents in the Knowledge Base tab.")
                else:
                    st.warning("Could not load documents list")
            except Exception as e:
                st.warning(f"Error loading documents: {str(e)}")

        # Display uploaded files
        if uploaded_files:
            st.write("**Attached files:**")
            for file in uploaded_files:
                file_size = len(file.getvalue()) / 1024  # KB
                st.caption(f"ğŸ“„ {file.name} ({file_size:.1f} KB)")

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            # Show attached files if any
            if message.get("files"):
                with st.expander("ğŸ“ Attachments"):
                    for file_info in message["files"]:
                        st.caption(f"ğŸ“„ {file_info['name']}")

            # Show conversation indicator if this was part of a multi-stage conversation
            if message.get("needs_more_info"):
                st.caption("ğŸ’¬ Waiting for more information...")

    # Chat input
    if prompt := st.chat_input(get_text("chat_input", lang)):
        # Process uploaded files
        file_contents = []
        file_info_list = []
        if uploaded_files:
            for file in uploaded_files:
                file_bytes = file.getvalue()
                file_name = file.name
                file_type = file.type

                # Handle different file types
                if file_type.startswith('image/'):
                    # Encode image as base64
                    base64_image = base64.b64encode(file_bytes).decode('utf-8')
                    file_contents.append({
                        "type": "image",
                        "name": file_name,
                        "data": base64_image,
                        "mime_type": file_type
                    })
                elif file_type == 'application/pdf':
                    # Extract PDF text content
                    try:
                        pdf_reader = PdfReader(BytesIO(file_bytes))
                        text_content = ""
                        for page in pdf_reader.pages:
                            text_content += page.extract_text() + "\n"
                        file_contents.append({
                            "type": "text",
                            "name": file_name,
                            "content": text_content,
                            "pages": len(pdf_reader.pages)
                        })
                    except Exception as e:
                        # If PDF extraction fails, provide error info
                        file_contents.append({
                            "type": "file",
                            "name": file_name,
                            "size": len(file_bytes),
                            "error": f"Failed to extract PDF: {str(e)}"
                        })
                elif file_type in ['text/plain', 'text/markdown', 'text/csv', 'application/json']:
                    # Extract text content
                    text_content = file_bytes.decode('utf-8')
                    file_contents.append({
                        "type": "text",
                        "name": file_name,
                        "content": text_content
                    })
                else:
                    # For other files, provide basic info
                    file_contents.append({
                        "type": "file",
                        "name": file_name,
                        "size": len(file_bytes)
                    })

                file_info_list.append({"name": file_name, "type": file_type})

        # Build enhanced prompt with file context
        enhanced_prompt = prompt
        has_images = False

        if file_contents:
            # Check if we have images
            has_images = any(fc["type"] == "image" for fc in file_contents)

            # Add text file contexts
            text_files = [fc for fc in file_contents if fc["type"] == "text"]
            if text_files:
                # Add explicit instruction for document analysis
                files_context = "\n\n===== IMPORTANT: DOCUMENT ANALYSIS REQUIRED =====\n"
                files_context += "User has uploaded document(s). Please analyze the content below and respond to the user's question based on this document.\n\n"

                for fc in text_files:
                    # For PDF files, include more content (up to 10000 chars for better understanding)
                    # For other text files, include full content or 5000 chars
                    if fc['name'].endswith('.pdf'):
                        max_chars = 10000
                        content_preview = fc['content'][:max_chars]
                        truncated = len(fc['content']) > max_chars
                        if truncated:
                            files_context += f"\n[DOCUMENT: {fc['name']} - {fc.get('pages', '?')} pages PDF - First {max_chars} characters shown]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT (TRUNCATED)---\n"
                        else:
                            files_context += f"\n[DOCUMENT: {fc['name']} - {fc.get('pages', '?')} pages PDF - Complete Content]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT---\n"
                    else:
                        max_chars = 5000
                        content_preview = fc['content'][:max_chars]
                        truncated = len(fc['content']) > max_chars
                        if truncated:
                            files_context += f"\n[DOCUMENT: {fc['name']} - First {max_chars} characters shown]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT (TRUNCATED)---\n"
                        else:
                            files_context += f"\n[DOCUMENT: {fc['name']} - Complete Content]\n"
                            files_context += f"---BEGIN DOCUMENT CONTENT---\n{content_preview}\n---END DOCUMENT CONTENT---\n"

                files_context += "\n===== END OF DOCUMENT(S) =====\n\nUser's Question: "
                enhanced_prompt = files_context + prompt

            # For images, we'll add them separately to the API call
            vision_models = ["gpt-4o", "gpt-4o-mini", "claude-3-opus", "claude-3-5-sonnet", "claude-3-sonnet", "gemini-1.5-pro", "gemini-1.5-flash"]
            if has_images and model_choice not in vision_models:
                # Show error message with available vision models
                if lang == "zh-TW":
                    error_msg = """
                    âš ï¸ **æ‰€é¸æ¨¡å‹ä¸æ”¯æ´åœ–åƒè¼¸å…¥**

                    æ‚¨ä¸Šå‚³äº†åœ–ç‰‡ï¼Œä½†ç•¶å‰é¸æ“‡çš„æ¨¡å‹ `{}` ç„¡æ³•è™•ç†åœ–åƒã€‚

                    **è«‹é¸æ“‡ä»¥ä¸‹æ”¯æ´è¦–è¦ºåŠŸèƒ½çš„æ¨¡å‹ä¹‹ä¸€ï¼š**
                    - gpt-4o (OpenAI)
                    - gpt-4o-mini (OpenAI)
                    - claude-3-5-sonnet (Anthropic)
                    - claude-3-opus (Anthropic)
                    - claude-3-sonnet (Anthropic)
                    - gemini-1.5-pro (Google)
                    - gemini-1.5-flash (Google)

                    è«‹å¾å·¦å´æ¬„æ›´æ›æ¨¡å‹å¾Œå†è©¦ã€‚
                    """.format(model_choice)
                else:
                    error_msg = """
                    âš ï¸ **Selected model doesn't support vision**

                    You have uploaded image(s), but the current model `{}` cannot process images.

                    **Please select one of these vision-capable models:**
                    - gpt-4o (OpenAI)
                    - gpt-4o-mini (OpenAI)
                    - claude-3-5-sonnet (Anthropic)
                    - claude-3-opus (Anthropic)
                    - claude-3-sonnet (Anthropic)
                    - gemini-1.5-pro (Google)
                    - gemini-1.5-flash (Google)

                    Please change your model selection in the sidebar and try again.
                    """.format(model_choice)

                st.error(error_msg)
                st.stop()  # Prevent submission

        # RAG Knowledge Search
        rag_context = ""
        if rag_search_enabled:
            try:
                # Get selected doc IDs if any
                selected_doc_ids = st.session_state.get("selected_doc_ids", [])

                search_payload = {
                    "query": prompt,
                    "top_k": 3,
                    "similarity_threshold": 0.5
                }

                # Add doc_ids filter if specific documents are selected
                if selected_doc_ids:
                    search_payload["doc_ids"] = selected_doc_ids

                rag_response = requests.post(
                    f"{MCP_SERVER_URL}/rag/search",
                    json=search_payload,
                    timeout=10
                )

                if rag_response.status_code == 200:
                    rag_results = rag_response.json()
                    if rag_results.get('count', 0) > 0:
                        rag_context = "\n\n**[RAG Knowledge Context]**\n"
                        for result in rag_results['results']:
                            rag_context += f"\n- {result['title']} (similarity: {result['score']:.2f})\n"
                            rag_context += f"  {result['content'][:200]}...\n"

                        enhanced_prompt = f"{rag_context}\n\n**[User Question]**\n{enhanced_prompt}"
            except Exception as e:
                st.warning(f"RAG search failed: {str(e)}")

        if web_search_enabled:
            enhanced_prompt = f"[WEB_SEARCH_ENABLED] {enhanced_prompt}"

        # Add user message
        st.session_state.messages.append({
            "role": "user",
            "content": prompt,
            "files": file_info_list if file_info_list else None
        })

        with st.chat_message("user"):
            st.markdown(prompt)
            if file_info_list:
                with st.expander("ğŸ“ Attachments"):
                    for file_info in file_info_list:
                        st.caption(f"ğŸ“„ {file_info['name']}")
            if rag_context:
                with st.expander("ğŸ“š RAG Knowledge Context"):
                    st.markdown(rag_context)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner(get_text("thinking", lang)):
                try:
                    start_time = time.time()

                    # Truncate conversation history to fit within context limits
                    truncated_history = truncate_conversation_history(
                        st.session_state.conversation_history,
                        model_choice
                    )

                    # Show truncation warning if needed
                    if len(truncated_history) < len(st.session_state.conversation_history):
                        removed = len(st.session_state.conversation_history) - len(truncated_history)
                        st.caption(f"â„¹ï¸ {removed} " + ("æ¢èˆŠæ¶ˆæ¯å·²ç§»é™¤ä»¥ç¯€çœä¸Šä¸‹æ–‡ç©ºé–“" if lang == "zh-TW" else "old messages removed to save context"))

                    # Build request payload
                    request_payload = {
                        "task": enhanced_prompt,
                        "model": model_choice,
                        "conversation_history": truncated_history,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k
                    }

                    # Add image data for vision models
                    if has_images and file_contents:
                        image_files = [fc for fc in file_contents if fc["type"] == "image"]
                        if image_files:
                            # Add images to context
                            request_payload["images"] = []
                            for img in image_files:
                                request_payload["images"].append({
                                    "name": img["name"],
                                    "data": img["data"],
                                    "mime_type": img["mime_type"]
                                })

                    # Use /agent/execute endpoint with conversation history support
                    # Longer timeout for PDF processing with qwen2.5:7b (180 seconds)
                    response = requests.post(
                        f"{AGENT_SERVICE_URL}/agent/execute",
                        json=request_payload,
                        timeout=180
                    )

                    elapsed_time = time.time() - start_time

                    if response.ok:
                        result = response.json()
                        answer = result["result"]
                        needs_more_info = result.get("needs_more_info", False)
                        conversation_active = result.get("metadata", {}).get("conversation_active", False)

                        # Display the response
                        st.markdown(answer)

                        # Show conversation status
                        if needs_more_info or conversation_active:
                            st.info("ğŸ’¬ " + ("è«‹ç¹¼çºŒæä¾›è³‡è¨Š..." if lang == "zh-TW" else "Please provide more information..."))

                        # Show metadata
                        with st.expander(get_text("view_details", lang)):
                            metadata_display = {
                                get_text("model", lang): result.get("metadata", {}).get("model_used", model_choice),
                                get_text("response_time", lang): f"{elapsed_time:.2f}{get_text('seconds', lang)}",
                                "Tokens Used": result.get("metadata", {}).get("tokens_used", "N/A")
                            }

                            if needs_more_info:
                                metadata_display["Status"] = "Waiting for more info" if lang == "en" else "ç­‰å¾…æ›´å¤šè³‡è¨Š"

                            st.json(metadata_display)

                            # Show execution steps if available
                            if result.get("steps"):
                                st.markdown("**Execution Steps:**")
                                for step in result["steps"]:
                                    st.text(f"â€¢ {step.get('step', 'Unknown')}: {step.get('status', 'unknown')}")

                        # Update conversation history for multi-stage conversations
                        if needs_more_info or conversation_active:
                            st.session_state.conversation_history.append({
                                "role": "user",
                                "content": prompt
                            })
                            st.session_state.conversation_history.append({
                                "role": "assistant",
                                "content": answer
                            })
                        else:
                            # Task completed, clear conversation history
                            st.session_state.conversation_history = []

                        # Add to display messages
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": answer,
                            "needs_more_info": needs_more_info
                        })
                    else:
                        error_msg = f"âŒ {get_text('error', lang)}: {response.text}"
                        st.error(error_msg)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": error_msg
                        })
                        # Clear conversation history on error
                        st.session_state.conversation_history = []

                except requests.exceptions.Timeout:
                    error_msg = get_text("request_timeout", lang)
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
                    st.session_state.conversation_history = []
                except Exception as e:
                    error_msg = f"{get_text('request_failed', lang)}: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
                    st.session_state.conversation_history = []

with tab2:
    st.header(get_text("agent_header", lang))
    st.caption(get_text("agent_caption", lang))

    # Initialize selected_example in session state
    if "selected_example" not in st.session_state:
        st.session_state.selected_example = ""

    # Initialize agent tab conversation history
    if "agent_conversation_history" not in st.session_state:
        st.session_state.agent_conversation_history = []

    # Initialize agent conversation messages for display
    if "agent_messages" not in st.session_state:
        st.session_state.agent_messages = []

    # Initialize task input value
    if "task_input_value" not in st.session_state:
        st.session_state.task_input_value = ""

    # Show conversation history
    if st.session_state.agent_messages:
        st.subheader("ğŸ’¬ " + ("å°è©±æ­·å²" if lang == "zh-TW" else "Conversation History"))
        for msg in st.session_state.agent_messages:
            if msg["role"] == "user":
                st.info(f"**You:** {msg['content']}")
            else:
                st.success(f"**Agent:** {msg['content']}")

        st.divider()

    col1, col2 = st.columns([3, 1])

    with col1:
        # Use task_input_value to control the text_area
        if st.session_state.selected_example:
            st.session_state.task_input_value = st.session_state.selected_example
            st.session_state.selected_example = ""

        task = st.text_area(
            get_text("describe_task", lang),
            height=150,
            placeholder=get_text("task_placeholder", lang),
            value=st.session_state.task_input_value,
            key="task_input"
        )

    with col2:
        # Agent type mapping with icons and names
        agent_options = {
            "general": f"ğŸ¤– {get_text('agent_general', lang)}",
            "research": f"ğŸ”¬ {get_text('agent_research', lang)}",
            "analysis": f"ğŸ“Š {get_text('agent_analysis', lang)}",
            "contract_review": f"ğŸ“‹ {get_text('agent_contract_review', lang)}"
        }

        agent_type_display = st.selectbox(
            get_text("agent_type", lang),
            options=list(agent_options.values()),
            help=get_text("agent_type_help", lang)
        )

        # Reverse mapping to get the agent_type ID
        agent_type = [k for k, v in agent_options.items() if v == agent_type_display][0]

        # File upload for contracts and documents
        st.markdown("---")
        uploaded_file = st.file_uploader(
            "ğŸ“ " + get_text("upload_file", lang),
            type=['pdf', 'docx', 'txt'],
            help=get_text("upload_file_help", lang),
            key="agent_file_upload"
        )

        execute_button = st.button(get_text("execute_task", lang), use_container_width=True)

        # Add clear conversation button
        if st.session_state.agent_conversation_history:
            if st.button("ğŸ”„ " + ("é‡ç½®å°è©±" if lang == "zh-TW" else "Reset Conversation"), use_container_width=True):
                st.session_state.agent_conversation_history = []
                st.session_state.agent_messages = []
                st.rerun()

    # Initialize session state for file content
    if 'uploaded_file_content' not in st.session_state:
        st.session_state.uploaded_file_content = ""
    if 'uploaded_file_name' not in st.session_state:
        st.session_state.uploaded_file_name = ""

    # Process uploaded file if present
    if uploaded_file is not None:
        # Only process if it's a new file or different file
        if uploaded_file.name != st.session_state.uploaded_file_name:
            try:
                file_content = ""
                if uploaded_file.type == "application/pdf":
                    # Extract text from PDF
                    import PyPDF2
                    import io
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.read()))
                    file_content = "\n\n".join([page.extract_text() for page in pdf_reader.pages])
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    # Extract text from DOCX
                    import docx
                    import io
                    doc = docx.Document(io.BytesIO(uploaded_file.read()))
                    file_content = "\n\n".join([paragraph.text for paragraph in doc.paragraphs])
                elif uploaded_file.type == "text/plain":
                    # Extract text from TXT
                    file_content = uploaded_file.read().decode('utf-8')

                # Store in session state
                st.session_state.uploaded_file_content = file_content
                st.session_state.uploaded_file_name = uploaded_file.name

                if file_content:
                    st.success(f"âœ… {get_text('file_loaded', lang)}: {uploaded_file.name} ({len(file_content)} {get_text('characters', lang)})")
                else:
                    st.warning(f"âš ï¸ æ–‡ä»¶å·²ä¸Šå‚³ä½†æœªèƒ½æå–å…§å®¹: {uploaded_file.name}")
            except Exception as e:
                st.error(f"âŒ {get_text('file_load_error', lang)}: {str(e)}")
                st.session_state.uploaded_file_content = ""
    elif st.session_state.uploaded_file_content:
        # File was uploaded before, show status
        st.info(f"ğŸ“ {get_text('file_loaded', lang)}: {st.session_state.uploaded_file_name} ({len(st.session_state.uploaded_file_content)} {get_text('characters', lang)})")

    # Get file content from session state
    file_content = st.session_state.uploaded_file_content

    if execute_button and (task or file_content):
        # Combine task description with file content
        if file_content and not task.strip():
            # If only file uploaded without task description, provide default instruction
            if agent_type == "contract_review":
                default_instruction = {
                    "zh-TW": "è«‹å¯©æŸ¥ä»¥ä¸‹å¥‘ç´„å…§å®¹ï¼Œæä¾›é¢¨éšªè©•ä¼°ã€æ¢æ¬¾åˆ†æå’Œå»ºè­°ï¼š",
                    "zh-CN": "è¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒå†…å®¹ï¼Œæä¾›é£é™©è¯„ä¼°ã€æ¡æ¬¾åˆ†æå’Œå»ºè®®ï¼š",
                    "en": "Please review the following contract, provide risk assessment, clause analysis and recommendations:",
                    "vi": "Vui lÃ²ng xem xÃ©t há»£p Ä‘á»“ng sau, cung cáº¥p Ä‘Ã¡nh giÃ¡ rá»§i ro, phÃ¢n tÃ­ch Ä‘iá»u khoáº£n vÃ  khuyáº¿n nghá»‹:"
                }
                combined_task = f"{default_instruction.get(lang, default_instruction['en'])}\n\n{file_content}"
            else:
                # For other agent types, just use the file content
                combined_task = f"{get_text('file_content', lang)}:\n\n{file_content}"
        elif file_content:
            # Both task and file content present
            combined_task = f"{task}\n\n{get_text('file_content', lang)}:\n\n{file_content}"
        else:
            # Only task, no file
            combined_task = task

        # Clear the task input after execution starts
        st.session_state.selected_example = ""
        st.session_state.task_input_value = ""

        with st.spinner(get_text("executing", lang)):
            try:
                start_time = time.time()

                response = requests.post(
                    f"{AGENT_SERVICE_URL}/agent/execute",
                    json={
                        "task": combined_task,
                        "agent_type": agent_type,
                        "model": model_choice,
                        "conversation_history": st.session_state.agent_conversation_history,
                        "temperature": temperature,
                        "top_p": top_p,
                        "top_k": top_k
                    },
                    timeout=180
                )

                elapsed_time = time.time() - start_time

                if response.ok:
                    result = response.json()
                    needs_more_info = result.get("needs_more_info", False)
                    conversation_active = result.get("metadata", {}).get("conversation_active", False)

                    # Add user message to conversation display
                    st.session_state.agent_messages.append({
                        "role": "user",
                        "content": task
                    })

                    # Add agent response to conversation display
                    st.session_state.agent_messages.append({
                        "role": "assistant",
                        "content": result["result"]
                    })

                    # Update conversation history if more info needed
                    if needs_more_info or conversation_active:
                        st.session_state.agent_conversation_history.append({
                            "role": "user",
                            "content": task
                        })
                        st.session_state.agent_conversation_history.append({
                            "role": "assistant",
                            "content": result["result"]
                        })

                        # Show that conversation is active
                        st.info("ğŸ’¬ " + ("è«‹åœ¨ä¸Šæ–¹æ–‡å­—æ¡†ç¹¼çºŒæä¾›è³‡è¨Šï¼Œç„¶å¾Œé»æ“Šã€ŒåŸ·è¡Œä»»å‹™ã€" if lang == "zh-TW" else "Please provide more information in the text box above and click 'Execute Task'"))
                    else:
                        # Task completed, clear conversation history
                        st.session_state.agent_conversation_history = []
                        st.success(get_text("task_complete", lang, time=f"{elapsed_time:.2f}"))

                    # Show result
                    st.subheader(get_text("execution_result", lang))
                    st.write(result["result"])

                    # Show execution steps
                    with st.expander(get_text("view_steps", lang), expanded=True):
                        for i, step in enumerate(result["steps"], 1):
                            # Map status to icon
                            status = step.get("status", "unknown")
                            if status == "success":
                                status_icon = "âœ…"
                            elif status == "failed":
                                status_icon = "âŒ"
                            elif status in ["detected", "executing"]:
                                status_icon = "ğŸ”"
                            else:
                                status_icon = "â„¹ï¸"

                            st.write(f"{status_icon} **{get_text('step', lang)} {i}: {step['step']}**")

                            # Display step details based on what's available
                            if "result" in step:
                                # Check if result is a dict (tool execution result)
                                if isinstance(step["result"], dict):
                                    st.json(step["result"])
                                else:
                                    st.caption(step["result"])
                            elif "tool" in step:
                                st.caption(f"ğŸ”§ Tool: **{step['tool']}**")
                                if "arguments" in step:
                                    # Show arguments inline with a toggle
                                    with st.container():
                                        st.caption("Arguments:")
                                        st.json(step["arguments"])
                            elif "error" in step:
                                st.error(f"âŒ Error: {step['error']}")

                    # Show MCP Usage Information
                    mcp_usage = result.get("metadata", {}).get("mcp_usage", {})
                    if mcp_usage and any([mcp_usage.get("tools_used"), mcp_usage.get("resources_accessed")]):
                        with st.expander(get_text("mcp_usage", lang), expanded=False):
                            # Tools Used
                            if mcp_usage.get("tools_used"):
                                st.markdown(f"**{get_text('tools_used', lang)}** ({len(mcp_usage['tools_used'])})")
                                for idx, tool in enumerate(mcp_usage["tools_used"], 1):
                                    st.markdown(f"**{idx}. {tool['name']}**")
                                    with st.container():
                                        st.caption(f"ğŸ“¥ {get_text('arguments', lang)}:")
                                        st.json(tool.get("arguments", {}))
                                        if tool.get("result_summary"):
                                            st.caption(f"ğŸ“¤ {get_text('result', lang)}: {tool['result_summary']}")
                                st.divider()

                            # Resources Accessed
                            if mcp_usage.get("resources_accessed"):
                                st.markdown(f"**{get_text('resources_accessed', lang)}** ({len(mcp_usage['resources_accessed'])})")
                                for resource in mcp_usage["resources_accessed"]:
                                    if resource["type"] == "document":
                                        st.caption(f"ğŸ“„ Document ID: {resource['id']}")
                                    elif resource["type"] == "search":
                                        st.caption(f"ğŸ” Search: {resource['query']}")
                                st.divider()

                            # Sampling Parameters
                            if mcp_usage.get("sampling_parameters"):
                                st.markdown(f"**{get_text('sampling_params', lang)}**")
                                params = mcp_usage["sampling_parameters"]
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Temperature", f"{params.get('temperature', 0.7):.2f}")
                                with col2:
                                    st.metric("Top-P", f"{params.get('top_p', 0.9):.2f}")
                                with col3:
                                    st.metric("Top-K", params.get('top_k', 40))
                                st.divider()

                            # System Prompt
                            if mcp_usage.get("system_prompt"):
                                st.markdown(f"**{get_text('system_prompt', lang)}**")
                                with st.container():
                                    st.text_area(
                                        label="",
                                        value=mcp_usage["system_prompt"],
                                        height=150,
                                        disabled=True,
                                        label_visibility="collapsed"
                                    )

                    # Show metadata
                    with st.expander(get_text("task_details", lang)):
                        metadata_display = result["metadata"].copy()
                        if needs_more_info:
                            metadata_display["conversation_status"] = "waiting_for_info"
                        # Remove mcp_usage from metadata display (shown above)
                        if "mcp_usage" in metadata_display:
                            del metadata_display["mcp_usage"]
                        st.json(metadata_display)

                    # Rerun to show updated conversation history
                    st.rerun()
                else:
                    st.error(f"{get_text('task_failed', lang)}: {response.text}")
                    # Clear conversation on error
                    st.session_state.agent_conversation_history = []

            except requests.exceptions.Timeout:
                st.error(get_text("task_timeout", lang))
                st.session_state.agent_conversation_history = []
            except Exception as e:
                st.error(f"{get_text('execution_failed', lang)}: {str(e)}")
                st.session_state.agent_conversation_history = []

    elif execute_button:
        st.warning(get_text("enter_task", lang))

    # Example tasks
    st.divider()

    col_header, col_button = st.columns([3, 1])
    with col_header:
        st.subheader(get_text("example_tasks", lang))
    with col_button:
        if st.button("ğŸ² " + get_text("generate_example", lang), use_container_width=True):
            # Define a pool of example tasks for different languages
            example_pool = {
                "zh-TW": [
                    "ç™¼é€éƒµä»¶çµ¦ team@example.comï¼Œä¸»æ—¨ï¼šæ¯é€±æœƒè­°ï¼Œå…§å®¹ï¼šæé†’å¤§å®¶æœ¬é€±äº”ä¸‹åˆ3é»é–‹æœƒ",
                    "åˆ†æä¸Šå€‹æœˆçš„éŠ·å”®æ•¸æ“šä¸¦ç”Ÿæˆå ±å‘Š",
                    "æœç´¢é—œæ–¼äººå·¥æ™ºèƒ½æœ€æ–°è¶¨å‹¢çš„æ–‡ç« ",
                    "å‰µå»ºä¸€å€‹ä»»å‹™ï¼šå®ŒæˆQ1è²¡å‹™å ±è¡¨ï¼Œæˆªæ­¢æ—¥æœŸï¼šä¸‹é€±äº”",
                    "ç¸½çµé€™ä»½æ–‡ä»¶çš„ä¸»è¦å…§å®¹",
                    "è¨ˆç®—ROIï¼šåˆå§‹æŠ•è³‡10è¬ï¼Œå¹´æ”¶ç›Š3è¬ï¼ŒæœŸé™5å¹´",
                    "ç¿»è­¯é€™æ®µæ–‡å­—åˆ°è‹±æ–‡ï¼šæˆ‘å€‘çš„ç”¢å“åœ¨å¸‚å ´ä¸Šè¡¨ç¾å„ªç•°",
                    "ä½¿ç”¨èªç¾©æœç´¢æ‰¾åˆ°èˆ‡'æ©Ÿå™¨å­¸ç¿’'ç›¸é—œçš„æ–‡æª”"
                ],
                "zh-CN": [
                    "å‘é€é‚®ä»¶ç»™ team@example.comï¼Œä¸»é¢˜ï¼šæ¯å‘¨ä¼šè®®ï¼Œå†…å®¹ï¼šæé†’å¤§å®¶æœ¬å‘¨äº”ä¸‹åˆ3ç‚¹å¼€ä¼š",
                    "åˆ†æä¸Šä¸ªæœˆçš„é”€å”®æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š",
                    "æœç´¢å…³äºäººå·¥æ™ºèƒ½æœ€æ–°è¶‹åŠ¿çš„æ–‡ç« ",
                    "åˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼šå®ŒæˆQ1è´¢åŠ¡æŠ¥è¡¨ï¼Œæˆªæ­¢æ—¥æœŸï¼šä¸‹å‘¨äº”",
                    "æ€»ç»“è¿™ä»½æ–‡ä»¶çš„ä¸»è¦å†…å®¹",
                    "è®¡ç®—ROIï¼šåˆå§‹æŠ•èµ„10ä¸‡ï¼Œå¹´æ”¶ç›Š3ä¸‡ï¼ŒæœŸé™5å¹´",
                    "ç¿»è¯‘è¿™æ®µæ–‡å­—åˆ°è‹±æ–‡ï¼šæˆ‘ä»¬çš„äº§å“åœ¨å¸‚åœºä¸Šè¡¨ç°ä¼˜å¼‚",
                    "ä½¿ç”¨è¯­ä¹‰æœç´¢æ‰¾åˆ°ä¸'æœºå™¨å­¦ä¹ 'ç›¸å…³çš„æ–‡æ¡£"
                ],
                "en": [
                    "Send email to team@example.com, subject: Weekly Meeting, body: Reminder for Friday 3pm meeting",
                    "Analyze last month's sales data and generate a report",
                    "Search for articles about the latest AI trends",
                    "Create a task: Complete Q1 financial report, deadline: next Friday",
                    "Summarize the main points of this document",
                    "Calculate ROI: Initial investment $100k, annual return $30k, period 5 years",
                    "Translate this text to Chinese: Our product performs exceptionally well in the market",
                    "Use semantic search to find documents related to 'machine learning'"
                ],
                "vi": [
                    "Gá»­i email Ä‘áº¿n team@example.com, chá»§ Ä‘á»: Cuá»™c há»p hÃ ng tuáº§n, ná»™i dung: Nháº¯c nhá»Ÿ cuá»™c há»p Thá»© SÃ¡u 3 giá» chiá»u",
                    "PhÃ¢n tÃ­ch dá»¯ liá»‡u bÃ¡n hÃ ng thÃ¡ng trÆ°á»›c vÃ  táº¡o bÃ¡o cÃ¡o",
                    "TÃ¬m kiáº¿m bÃ i viáº¿t vá» xu hÆ°á»›ng AI má»›i nháº¥t",
                    "Táº¡o nhiá»‡m vá»¥: HoÃ n thÃ nh bÃ¡o cÃ¡o tÃ i chÃ­nh Q1, háº¡n chÃ³t: Thá»© SÃ¡u tuáº§n sau",
                    "TÃ³m táº¯t cÃ¡c Ä‘iá»ƒm chÃ­nh cá»§a tÃ i liá»‡u nÃ y",
                    "TÃ­nh ROI: Äáº§u tÆ° ban Ä‘áº§u $100k, lá»£i nhuáº­n hÃ ng nÄƒm $30k, thá»i háº¡n 5 nÄƒm",
                    "Dá»‹ch vÄƒn báº£n nÃ y sang tiáº¿ng Trung: Sáº£n pháº©m cá»§a chÃºng tÃ´i hoáº¡t Ä‘á»™ng ráº¥t tá»‘t trÃªn thá»‹ trÆ°á»ng",
                    "Sá»­ dá»¥ng tÃ¬m kiáº¿m ngá»¯ nghÄ©a Ä‘á»ƒ tÃ¬m tÃ i liá»‡u liÃªn quan Ä‘áº¿n 'machine learning'"
                ]
            }

            # Get pool for current language
            pool = example_pool.get(lang, example_pool["en"])
            # Pick a random example
            random_example = random.choice(pool)
            st.session_state.selected_example = random_example
            st.rerun()

    examples = [
        get_text("example_1", lang),
        get_text("example_2", lang),
        get_text("example_3", lang),
        get_text("example_4", lang)
    ]

    cols = st.columns(2)
    for i, example in enumerate(examples):
        with cols[i % 2]:
            if st.button(f"ğŸ“‹ {example}", key=f"example_{i}"):
                st.session_state.selected_example = example
                st.rerun()

with tab3:
    st.header(get_text("agents_catalog_header", lang))
    st.caption(get_text("agents_catalog_caption", lang))

    # Agent Types Section
    st.subheader(get_text("agent_types", lang))

    agent_configs = {
        "general": {
            "name": get_text("agent_general", lang),
            "icon": "ğŸ¤–",
            "description": get_text("agent_general_desc", lang),
            "use_cases": get_text("agent_general_uses", lang),
            "prompt": """ä½ æ˜¯ä¸€å€‹ä¼æ¥­AIåŠ©æ‰‹ï¼Œå¯ä»¥ç›´æ¥å›ç­”å•é¡Œæˆ–ä½¿ç”¨å„ç¨®å·¥å…·ä¾†å¹«åŠ©ç”¨æˆ¶å®Œæˆä»»å‹™ã€‚

é‡è¦æŒ‡å—ï¼š

ğŸ“„ **æ–‡ä»¶åˆ†ææ¨¡å¼**ï¼š
- å¦‚æœç”¨æˆ¶ä¸Šå‚³äº†æ–‡ä»¶ï¼ˆPDFã€æ–‡æœ¬ç­‰ï¼‰ä¸¦è©¢å•å…§å®¹ï¼Œç›´æ¥åˆ†ææ–‡ä»¶ä¸¦å›ç­”å•é¡Œ
- ä¸éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥é–±è®€æä¾›çš„æ–‡ä»¶å…§å®¹ä¸¦é€²è¡Œåˆ†æ
- ç¤ºä¾‹ï¼šç”¨æˆ¶ä¸Šå‚³PDFä¸¦å•"æè¿°é€™ä»½æ–‡ä»¶" â†’ ç›´æ¥åˆ†ææ–‡ä»¶å…§å®¹ä¸¦è©³ç´°æè¿°

ğŸ› ï¸ **å·¥å…·ä½¿ç”¨æ¨¡å¼**ï¼š
1. ç•¶ç”¨æˆ¶è¦æ±‚åŸ·è¡ŒæŸå€‹æ“ä½œæ™‚ï¼ˆå¦‚ç™¼é€éƒµä»¶ã€å‰µå»ºä»»å‹™ã€æœç´¢ç­‰ï¼‰ï¼Œè«‹èª¿ç”¨ç›¸æ‡‰çš„å·¥å…·
2. åœ¨èª¿ç”¨å·¥å…·ä¹‹å‰ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰æ‰€æœ‰å¿…éœ€çš„åƒæ•¸
3. å¦‚æœç¼ºå°‘å¿…éœ€åƒæ•¸ï¼ˆå¦‚emailåœ°å€ã€subjectã€bodyç­‰ï¼‰ï¼Œä¸è¦çŒœæ¸¬æˆ–ä½¿ç”¨é»˜èªå€¼
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹ç¦®è²Œåœ°è©¢å•ç”¨æˆ¶æä¾›ç¼ºå°‘çš„ä¿¡æ¯
5. ä¸€æ¬¡åªè©¢å•ç¼ºå°‘çš„ä¿¡æ¯ï¼Œä¸è¦å•ä¸å¿…è¦çš„å•é¡Œ
6. æ”¶é›†åˆ°æ‰€æœ‰å¿…éœ€ä¿¡æ¯å¾Œï¼Œç«‹å³åŸ·è¡Œæ“ä½œ

ç¤ºä¾‹ï¼š
- ç”¨æˆ¶èªª"send email"ä½†æ²’æœ‰æä¾›æ”¶ä»¶äºº â†’ è©¢å•æ”¶ä»¶äººemailåœ°å€
- ç”¨æˆ¶èªª"send email to john@example.com"ä½†æ²’æœ‰ä¸»æ—¨å’Œå…§å®¹ â†’ è©¢å•éƒµä»¶ä¸»æ—¨å’Œå…§å®¹
- ç”¨æˆ¶æä¾›äº†æ‰€æœ‰ä¿¡æ¯ â†’ ç›´æ¥åŸ·è¡Œç™¼é€éƒµä»¶"""
        },
        "research": {
            "name": get_text("agent_research", lang),
            "icon": "ğŸ”¬",
            "description": get_text("agent_research_desc", lang),
            "use_cases": get_text("agent_research_uses", lang),
            "prompt": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•·ä¿¡æ¯æ”¶é›†ã€åˆ†æå’Œæ•´ç†ã€‚

ä½ çš„å°ˆé•·ï¼š
1. ä½¿ç”¨æœç´¢å·¥å…·ï¼ˆsearch_knowledge_base, web_search, semantic_searchï¼‰æ·±å…¥ç ”ç©¶ä¸»é¡Œ
2. æ‰¾åˆ°ç›¸é—œæ–‡æª”ä¸¦æå–é—œéµä¿¡æ¯
3. æ•´åˆå¤šå€‹ä¾†æºçš„ä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„ç ”ç©¶å ±å‘Š
4. é©—è­‰ä¿¡æ¯çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§
5. æä¾›å¼•ç”¨å’Œä¾†æº

å·¥ä½œæ–¹å¼ï¼š
- æ”¶åˆ°ç ”ç©¶ä»»å‹™æ™‚ï¼Œå…ˆè¦åŠƒæœç´¢ç­–ç•¥
- ä½¿ç”¨å¤šå€‹æœç´¢å·¥å…·äº¤å‰é©—è­‰ä¿¡æ¯
- æ•´ç†ç™¼ç¾çš„ä¿¡æ¯ï¼Œä»¥çµæ§‹åŒ–æ–¹å¼å‘ˆç¾
- å¿…è¦æ™‚ä½¿ç”¨ summarize_document å·¥å…·ç¸½çµé•·æ–‡æª”
- æä¾›æ¸…æ™°çš„ç ”ç©¶çµè«–å’Œå»ºè­°

é‡é»ï¼šæ·±åº¦ã€æº–ç¢ºæ€§ã€ä¾†æºå¯é æ€§"""
        },
        "analysis": {
            "name": get_text("agent_analysis", lang),
            "icon": "ğŸ“Š",
            "description": get_text("agent_analysis_desc", lang),
            "use_cases": get_text("agent_analysis_uses", lang),
            "prompt": """ä½ æ˜¯ä¸€å€‹æ•¸æ“šåˆ†æå°ˆå®¶ï¼Œå°ˆæ³¨æ–¼æ•¸æ“šè™•ç†ã€åˆ†æå’Œå¯è¦–åŒ–ã€‚

ä½ çš„å°ˆé•·ï¼š
1. ä½¿ç”¨ analyze_data å·¥å…·é€²è¡Œçµ±è¨ˆåˆ†æ
2. ä½¿ç”¨ process_csv è™•ç†å’Œæ¸…ç†æ•¸æ“š
3. ä½¿ç”¨ generate_chart å‰µå»ºæ•¸æ“šå¯è¦–åŒ–
4. ä½¿ç”¨ calculate_metrics è¨ˆç®—æ¥­å‹™æŒ‡æ¨™
5. ä½¿ç”¨ financial_calculator é€²è¡Œè²¡å‹™åˆ†æ

å·¥ä½œæµç¨‹ï¼š
- ç†è§£æ•¸æ“šåˆ†æéœ€æ±‚
- æª¢æŸ¥æ•¸æ“šè³ªé‡å’Œå®Œæ•´æ€§
- é¸æ“‡é©ç•¶çš„åˆ†ææ–¹æ³•
- ç”Ÿæˆæ¸…æ™°çš„åœ–è¡¨å’Œå ±è¡¨
- æä¾›æ•¸æ“šé©…å‹•çš„è¦‹è§£å’Œå»ºè­°

é‡é»ï¼šæ•¸æ“šæº–ç¢ºæ€§ã€åˆ†ææ·±åº¦ã€å¯è¦–åŒ–æ¸…æ™°åº¦ã€actionable insights"""
        },
        "contract_review": {
            "name": get_text("agent_contract_review", lang),
            "icon": "ğŸ“‹",
            "description": get_text("agent_contract_review_desc", lang),
            "use_cases": get_text("agent_contract_review_uses", lang),
            "prompt": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¥‘ç´„å¯©æŸ¥åŠ©æ‰‹ï¼Œå°ˆæ³¨æ–¼å¥‘ç´„åˆ†æã€é¢¨éšªè©•ä¼°å’Œåˆè¦æª¢æŸ¥ã€‚

ä½ çš„å°ˆé•·ï¼š
1. ä½¿ç”¨ review_contract å·¥å…·é€²è¡Œå…¨é¢çš„å¥‘ç´„å¯©æŸ¥
2. ä½¿ç”¨ analyze_clause å·¥å…·åˆ†æç‰¹å®šæ¢æ¬¾
3. ä½¿ç”¨ compare_contracts å·¥å…·æ¯”è¼ƒå¤šä»½å¥‘ç´„
4. è­˜åˆ¥é«˜é¢¨éšªæ¢æ¬¾å’Œä¸å…¬å¹³æ¢ä»¶
5. è©•ä¼°å¥‘ç´„åˆè¦æ€§å’Œå®Œæ•´æ€§

å·¥ä½œæµç¨‹ï¼š
1. æ¥æ”¶å¥‘ç´„å…§å®¹å¾Œï¼Œå…ˆåˆ¤æ–·å¥‘ç´„é¡å‹ï¼ˆemployment/nda/service/lease/salesï¼‰
2. ä½¿ç”¨ review_contract å·¥å…·é€²è¡Œå®Œæ•´åˆ†æ
3. è­˜åˆ¥é—œéµé¢¨éšªé»ä¸¦è¨ˆç®—é¢¨éšªè©•åˆ†ï¼ˆ0-100ï¼‰
4. æª¢æŸ¥ç¼ºå¤±çš„å¿…è¦æ¢æ¬¾
5. æä¾›å…·é«”çš„ä¿®æ”¹å»ºè­°å’Œå„ªå…ˆç´šæ’åº

é¢¨éšªè©•ä¼°æ¨™æº–ï¼š
- 0-24åˆ†ï¼šä½é¢¨éšª âœ… ï¼ˆå¯ä»¥æ¥å—ï¼‰
- 25-49åˆ†ï¼šä¸­ç­‰é¢¨éšª âš ï¸ ï¼ˆéœ€è¦ä»”ç´°å¯©æŸ¥ï¼‰
- 50-74åˆ†ï¼šé«˜é¢¨éšª ğŸ”´ ï¼ˆå»ºè­°å”å•†ä¿®æ”¹ï¼‰
- 75-100åˆ†ï¼šæ¥µé«˜é¢¨éšª ğŸš¨ ï¼ˆå­˜åœ¨é‡å¤§å•é¡Œï¼‰

é‡é»é—œæ³¨ï¼š
- ç„¡é™è²¬ä»»æ¢æ¬¾
- æ°¸ä¹…æ€§ç¾©å‹™
- æ”¾æ£„æ¬Šåˆ©æ¢æ¬¾
- å–®æ–¹é¢æ±ºå®šæ¬Š
- éæ–¼å¯¬æ³›çš„ç«¶æ¥­ç¦æ­¢
- è‡ªå‹•çºŒç´„æ¢æ¬¾
- æ¨¡ç³Šä¸æ¸…çš„ç”¨èª

è¼¸å‡ºæ ¼å¼ï¼š
1. å¥‘ç´„æ‘˜è¦ï¼ˆé¡å‹ã€ç•¶äº‹äººã€é—œéµæ¢æ¬¾ï¼‰
2. é¢¨éšªè©•åˆ†åŠç­‰ç´š
3. å…·é«”é¢¨éšªåˆ†æï¼ˆæŒ‰åš´é‡ç¨‹åº¦åˆ†é¡ï¼‰
4. ç¼ºå¤±æ¢æ¬¾æ¸…å–®
5. å„ªå…ˆç´šæ’åºçš„å»ºè­°äº‹é …

é‡è¦æé†’ï¼š
- æ­¤å·¥å…·æä¾›åˆæ­¥åˆ†æï¼Œä¸èƒ½æ›¿ä»£å°ˆæ¥­æ³•å¾‹è«®è©¢
- å°æ–¼é«˜åƒ¹å€¼æˆ–è¤‡é›œå¥‘ç´„ï¼Œå»ºè­°è«®è©¢å°ˆæ¥­å¾‹å¸«
- åˆ†æåŸºæ–¼ä¸€èˆ¬æ³•å¾‹åŸå‰‡ï¼Œä¸é‡å°ç‰¹å®šå¸æ³•ç®¡è½„å€

è«‹ä½¿ç”¨æ¸…æ™°ã€å°ˆæ¥­çš„èªè¨€ï¼Œç¢ºä¿ç”¨æˆ¶èƒ½å¤ ç†è§£é¢¨éšªä¸¦æ¡å–è¡Œå‹•ã€‚"""
        }
    }

    # Display agents in 2x2 grid layout
    agent_items = list(agent_configs.items())

    # First row: General and Research
    cols_row1 = st.columns(2)
    for idx, (agent_id, config) in enumerate(agent_items[:2]):
        with cols_row1[idx]:
            st.markdown(f"### {config['icon']} {config['name']}")
            st.caption(config['description'])
            st.markdown(f"**{get_text('use_cases', lang)}:**")
            st.markdown(config['use_cases'])

            with st.expander(get_text("view_system_prompt", lang)):
                st.text_area(
                    label="",
                    value=config['prompt'],
                    height=200,
                    disabled=True,
                    label_visibility="collapsed",
                    key=f"prompt_{agent_id}"
                )

    # Second row: Analysis and Contract Review
    cols_row2 = st.columns(2)
    for idx, (agent_id, config) in enumerate(agent_items[2:]):
        with cols_row2[idx]:
            st.markdown(f"### {config['icon']} {config['name']}")
            st.caption(config['description'])
            st.markdown(f"**{get_text('use_cases', lang)}:**")
            st.markdown(config['use_cases'])

            with st.expander(get_text("view_system_prompt", lang)):
                st.text_area(
                    label="",
                    value=config['prompt'],
                    height=200,
                    disabled=True,
                    label_visibility="collapsed",
                    key=f"prompt_{agent_id}"
                )

    st.divider()

    # Default Sampling Parameters
    st.subheader(get_text("default_sampling", lang))

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Temperature", "0.7", help=get_text("default_temp_help", lang))
    with col2:
        st.metric("Top-P", "0.9", help=get_text("default_topp_help", lang))
    with col3:
        st.metric("Top-K", "40", help=get_text("default_topk_help", lang))

    st.info(get_text("sampling_info", lang))

    st.divider()

    # MCP Tools Section
    st.subheader(get_text("available_tools", lang))

    # Fetch tools from MCP server
    try:
        mcp_url = os.getenv("MCP_SERVER_URL", "http://mcp-server:8000")
        response = requests.get(f"{mcp_url}/tools/list", timeout=5)
        if response.ok:
            tools_data = response.json()
            tools = tools_data.get("tools", [])

            st.success(f"{get_text('tools_loaded', lang)}: {len(tools)} {get_text('tools', lang)}")

            # Group tools by category
            categorized_tools = {}
            for tool in tools:
                category = tool.get("category", "other")
                if category not in categorized_tools:
                    categorized_tools[category] = []
                categorized_tools[category].append(tool)

            # Display tools by category
            for category, category_tools in sorted(categorized_tools.items()):
                with st.expander(f"ğŸ“‚ {category.title()} ({len(category_tools)})"):
                    for tool in category_tools:
                        st.markdown(f"**{tool['name']}**")
                        st.caption(f"ğŸ“ {tool['description']}")
                        if tool.get("parameters"):
                            params_str = ", ".join([f"`{k}`: {v}" for k, v in tool["parameters"].items()])
                            st.caption(f"âš™ï¸ {get_text('parameters', lang)}: {params_str}")
                        st.markdown("---")
        else:
            st.warning(get_text("tools_load_failed", lang))
            st.caption(f"Status: {response.status_code}")
    except Exception as e:
        st.error(f"{get_text('tools_load_error', lang)}: {str(e)}")

    st.divider()

    # Resources Section
    st.subheader(get_text("available_resources", lang))

    resource_types = {
        get_text("resource_documents", lang): {
            "icon": "ğŸ“„",
            "description": get_text("resource_documents_desc", lang),
            "access": get_text("resource_documents_access", lang)
        },
        get_text("resource_knowledge_base", lang): {
            "icon": "ğŸ“š",
            "description": get_text("resource_knowledge_desc", lang),
            "access": get_text("resource_knowledge_access", lang)
        },
        get_text("resource_web", lang): {
            "icon": "ğŸŒ",
            "description": get_text("resource_web_desc", lang),
            "access": get_text("resource_web_access", lang)
        },
        get_text("resource_databases", lang): {
            "icon": "ğŸ—„ï¸",
            "description": get_text("resource_databases_desc", lang),
            "access": get_text("resource_databases_access", lang)
        }
    }

    for resource_name, resource_info in resource_types.items():
        with st.container():
            col_icon, col_content = st.columns([1, 9])
            with col_icon:
                st.markdown(f"<div style='font-size: 2rem; text-align: center;'>{resource_info['icon']}</div>", unsafe_allow_html=True)
            with col_content:
                st.markdown(f"**{resource_name}**")
                st.caption(resource_info['description'])
                st.caption(f"ğŸ”‘ {get_text('access_via', lang)}: {resource_info['access']}")
            st.markdown("---")

with tab4:
    st.header(get_text("models_config_header", lang))
    st.caption(get_text("models_config_caption", lang))

    # Load litellm config
    import yaml
    import os

    config_path = "/app/config/litellm-config.yaml"

    def load_litellm_config():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            st.error(f"Error loading config: {str(e)}")
            return None

    def save_litellm_config(config_data):
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
            return True
        except Exception as e:
            st.error(f"Error saving config: {str(e)}")
            return False

    # Load config
    litellm_config = load_litellm_config()

    if litellm_config:
        # Initialize session state for editing
        if 'editing_model' not in st.session_state:
            st.session_state.editing_model = None
        if 'adding_new_model' not in st.session_state:
            st.session_state.adding_new_model = False

        # Action buttons
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            if st.button(get_text("add_new_model", lang), type="primary"):
                st.session_state.adding_new_model = True
                st.session_state.editing_model = None
                st.rerun()
        with col2:
            if st.button(get_text("reload_config", lang)):
                st.rerun()

        st.divider()

        # Add new model form
        if st.session_state.adding_new_model:
            with st.form("add_model_form"):
                st.subheader(get_text("add_new_model", lang))

                col1, col2 = st.columns(2)
                with col1:
                    new_model_name = st.text_input(get_text("model_name", lang), placeholder="gpt-4")
                    new_display_name = st.text_input(get_text("display_name", lang), placeholder="GPT-4 (OpenAI)")
                    new_model_provider = st.selectbox(
                        get_text("provider_type", lang),
                        ["openai", "anthropic", "ollama", "gemini", "custom"]
                    )
                    new_model_id = st.text_input(get_text("model_id", lang), placeholder="openai/gpt-4")

                with col2:
                    new_api_base = st.text_input(get_text("api_base", lang), placeholder="https://api.openai.com/v1")
                    new_api_key = st.text_input(
                        get_text("api_key", lang),
                        placeholder="os.environ/OPENAI_API_KEY or actual key",
                        type="password"
                    )
                    new_visible = st.checkbox(
                        get_text("visible_in_selection", lang),
                        value=True,
                        help=get_text("visible_help", lang)
                    )

                col_submit, col_cancel = st.columns([1, 5])
                with col_submit:
                    submitted = st.form_submit_button(get_text("save", lang), type="primary")
                with col_cancel:
                    cancelled = st.form_submit_button(get_text("cancel", lang))

                if submitted and new_model_name and new_model_id:
                    # Create new model entry
                    new_model = {
                        "model_name": new_model_name,
                        "visible": new_visible,
                        "litellm_params": {
                            "model": new_model_id
                        }
                    }

                    # Add display_name if provided
                    if new_display_name:
                        new_model["display_name"] = new_display_name

                    if new_api_base:
                        new_model["litellm_params"]["api_base"] = new_api_base
                    if new_api_key:
                        new_model["litellm_params"]["api_key"] = new_api_key

                    # Add to config
                    if "model_list" not in litellm_config:
                        litellm_config["model_list"] = []
                    litellm_config["model_list"].append(new_model)

                    if save_litellm_config(litellm_config):
                        st.success(get_text("model_added_success", lang))
                        st.session_state.adding_new_model = False
                        st.rerun()

                if cancelled:
                    st.session_state.adding_new_model = False
                    st.rerun()

        # Display models
        st.subheader(get_text("available_models", lang))

        if "model_list" in litellm_config:
            for idx, model in enumerate(litellm_config["model_list"]):
                model_name = model.get("model_name", "Unknown")
                display_name = model.get("display_name", model_name)
                litellm_params = model.get("litellm_params", {})
                model_id = litellm_params.get("model", "")
                api_base = litellm_params.get("api_base", "")
                api_key = litellm_params.get("api_key", "")

                # Determine provider
                provider = "Unknown"
                if "openai" in model_id.lower():
                    provider = "OpenAI"
                elif "anthropic" in model_id.lower() or "claude" in model_id.lower():
                    provider = "Anthropic"
                elif "ollama" in model_id.lower():
                    provider = "Ollama"
                elif "gemini" in model_id.lower():
                    provider = "Google"
                elif "llama" in model_name.lower():
                    provider = "Taiwan Gov"

                # Check if this model is being edited
                is_editing = st.session_state.editing_model == idx

                with st.expander(f"**{display_name}** ({model_name}) - {provider}", expanded=is_editing):
                    if is_editing:
                        # Edit mode
                        with st.form(f"edit_model_form_{idx}"):
                            col1, col2 = st.columns(2)
                            with col1:
                                edit_model_name = st.text_input(
                                    get_text("model_name", lang),
                                    value=model_name,
                                    key=f"edit_name_{idx}"
                                )
                                edit_display_name = st.text_input(
                                    get_text("display_name", lang),
                                    value=display_name,
                                    key=f"edit_display_{idx}"
                                )
                                edit_model_id = st.text_input(
                                    get_text("model_id", lang),
                                    value=model_id,
                                    key=f"edit_id_{idx}"
                                )

                            with col2:
                                edit_api_base = st.text_input(
                                    get_text("api_base", lang),
                                    value=api_base,
                                    key=f"edit_base_{idx}"
                                )
                                # Mask API key display
                                display_key = api_key if len(api_key) < 20 else f"{api_key[:10]}...{api_key[-10:]}"
                                edit_api_key = st.text_input(
                                    get_text("api_key", lang),
                                    value=api_key,
                                    type="password",
                                    key=f"edit_key_{idx}"
                                )
                                edit_visible = st.checkbox(
                                    get_text("visible_in_selection", lang),
                                    value=model.get("visible", True),
                                    key=f"edit_visible_{idx}",
                                    help=get_text("visible_help", lang)
                                )

                            col_save, col_cancel, col_delete = st.columns([1, 1, 4])
                            with col_save:
                                save_clicked = st.form_submit_button(get_text("save", lang), type="primary")
                            with col_cancel:
                                cancel_clicked = st.form_submit_button(get_text("cancel", lang))
                            with col_delete:
                                delete_clicked = st.form_submit_button(get_text("delete", lang), type="secondary")

                            if save_clicked:
                                # Update model
                                litellm_config["model_list"][idx]["model_name"] = edit_model_name
                                litellm_config["model_list"][idx]["visible"] = edit_visible
                                if edit_display_name:
                                    litellm_config["model_list"][idx]["display_name"] = edit_display_name
                                litellm_config["model_list"][idx]["litellm_params"]["model"] = edit_model_id
                                if edit_api_base:
                                    litellm_config["model_list"][idx]["litellm_params"]["api_base"] = edit_api_base
                                else:
                                    litellm_config["model_list"][idx]["litellm_params"].pop("api_base", None)
                                if edit_api_key:
                                    litellm_config["model_list"][idx]["litellm_params"]["api_key"] = edit_api_key
                                else:
                                    litellm_config["model_list"][idx]["litellm_params"].pop("api_key", None)

                                if save_litellm_config(litellm_config):
                                    st.success(get_text("model_updated_success", lang))
                                    st.session_state.editing_model = None
                                    st.rerun()

                            if cancel_clicked:
                                st.session_state.editing_model = None
                                st.rerun()

                            if delete_clicked:
                                # Delete model
                                litellm_config["model_list"].pop(idx)
                                if save_litellm_config(litellm_config):
                                    st.success(get_text("model_deleted_success", lang))
                                    st.session_state.editing_model = None
                                    st.rerun()
                    else:
                        # View mode
                        col1, col2, col3 = st.columns([3, 3, 1])
                        with col1:
                            st.markdown(f"**{get_text('model_name', lang)}:** `{model_name}`")
                            st.markdown(f"**{get_text('model_id', lang)}:** `{model_id}`")
                            # Show visibility status
                            is_visible = model.get("visible", True)
                            visibility_icon = "âœ…" if is_visible else "âŒ"
                            visibility_text = get_text("visible", lang) if is_visible else get_text("hidden", lang)
                            st.markdown(f"**{get_text('visibility', lang)}:** {visibility_icon} {visibility_text}")
                        with col2:
                            st.markdown(f"**{get_text('provider', lang)}:** {provider}")
                            if api_base:
                                st.markdown(f"**{get_text('api_base', lang)}:** `{api_base}`")
                            if api_key:
                                masked_key = f"{api_key[:10]}..." if len(api_key) > 10 else "***"
                                st.markdown(f"**{get_text('api_key', lang)}:** `{masked_key}`")
                        with col3:
                            if st.button(get_text("edit", lang), key=f"edit_btn_{idx}"):
                                st.session_state.editing_model = idx
                                st.session_state.adding_new_model = False
                                st.rerun()

        # Config file info
        st.divider()
        st.info(f"{get_text('config_file_location', lang)}: `{config_path}`")

        # Raw config viewer
        with st.expander(get_text("view_raw_config", lang)):
            st.code(yaml.dump(litellm_config, default_flow_style=False, allow_unicode=True), language="yaml")

with tab5:
    st.header(get_text("monitor_header", lang))
    st.caption(get_text("monitor_caption", lang))

    col1, col2, col3 = st.columns(3)

    # Simulated metrics (should be fetched from Prometheus)
    with col1:
        st.metric(
            label=get_text("agent_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    with col2:
        st.metric(
            label=get_text("llm_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    with col3:
        st.metric(
            label=get_text("mcp_service", lang),
            value=get_text("running", lang),
            delta=get_text("normal", lang)
        )

    st.divider()

    # Monitoring links
    st.subheader(get_text("monitor_tools", lang))

    col1, col2 = st.columns(2)

    with col1:
        st.markdown(f"""
        **{get_text("grafana_dashboard", lang)}**
        - {get_text("grafana_url", lang)}: http://localhost:3000
        - {get_text("grafana_account", lang)}: admin
        - {get_text("grafana_password", lang)}: admin
        - {get_text("grafana_features", lang)}
        """)

    with col2:
        st.markdown(f"""
        **{get_text("prometheus", lang)}**
        - {get_text("grafana_url", lang)}: http://localhost:9090
        - {get_text("prometheus_features", lang)}
        """)

    st.info(get_text("monitor_tip", lang))

with tab6:
    st.header(get_text("rag_header", lang))
    st.caption(get_text("rag_caption", lang))

    # Create columns for layout
    col1, col2 = st.columns([1, 1])

    with col1:
        # Document Upload Section
        st.subheader(get_text("rag_upload_section", lang))

        uploaded_file = st.file_uploader(
            get_text("rag_upload_file", lang),
            type=['pdf', 'docx', 'txt'],
            help=get_text("rag_upload_help", lang)
        )

        doc_title = st.text_input(get_text("rag_doc_title", lang))
        doc_category = st.text_input(get_text("rag_doc_category", lang), value="General")
        doc_tags = st.text_input(get_text("rag_doc_tags", lang), placeholder="AI, Documentation, Enterprise")

        if st.button(get_text("rag_upload_button", lang)):
            if uploaded_file and doc_title:
                with st.spinner(get_text("rag_uploading", lang)):
                    try:
                        # Prepare file upload
                        files = {"file": (uploaded_file.name, uploaded_file.getvalue(), uploaded_file.type)}
                        data = {
                            "title": doc_title,
                            "category": doc_category,
                            "tags": doc_tags
                        }

                        # Upload to MCP server
                        response = requests.post(
                            f"{MCP_SERVER_URL}/rag/documents/upload",
                            files=files,
                            data=data,
                            timeout=30
                        )

                        if response.status_code == 200:
                            result = response.json()
                            st.success(get_text("rag_upload_success", lang).format(
                                doc_id=result.get('doc_id'),
                                chunks=result.get('chunks_count')
                            ))
                        else:
                            st.error(f"{get_text('rag_upload_error', lang)}: {response.text}")
                    except Exception as e:
                        st.error(f"{get_text('rag_upload_error', lang)}: {str(e)}")
            else:
                st.warning("Please provide both file and title!")

        st.divider()

        # Manual Text Input Section (for scanned PDFs or direct input)
        st.subheader("ğŸ“ " + ("ç›´æ¥è¼¸å…¥æ–‡æœ¬" if lang == "zh-TW" else "Direct Text Input"))
        st.caption("Alternative: Create document from text (useful for scanned PDFs)")

        with st.expander("Create document from text"):
            text_doc_title = st.text_input("Document Title", key="text_title")
            text_doc_content = st.text_area("Content", height=200, key="text_content",
                                           placeholder="Paste or type your document content here...")
            text_doc_category = st.text_input("Category", value="General", key="text_category")
            text_doc_tags = st.text_input("Tags (comma-separated)", key="text_tags")

            if st.button("Create from Text", key="create_text_doc"):
                if text_doc_title and text_doc_content:
                    with st.spinner("Creating document..."):
                        try:
                            response = requests.post(
                                f"{MCP_SERVER_URL}/rag/documents/text",
                                json={
                                    "title": text_doc_title,
                                    "content": text_doc_content,
                                    "category": text_doc_category,
                                    "tags": [t.strip() for t in text_doc_tags.split(",")] if text_doc_tags else []
                                },
                                timeout=30
                            )

                            if response.status_code == 200:
                                result = response.json()
                                st.success(f"âœ… Document created! ID: {result.get('doc_id')}, Chunks: {result.get('chunks_count')}")
                            else:
                                st.error(f"âŒ Error: {response.text}")
                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")
                else:
                    st.warning("Please provide both title and content!")

        st.divider()

        # Semantic Search Section
        st.subheader(get_text("rag_search_section", lang))

        search_query = st.text_input(
            get_text("rag_search_query", lang),
            placeholder=get_text("rag_search_placeholder", lang)
        )

        col_search1, col_search2 = st.columns(2)
        with col_search1:
            top_k = st.slider(get_text("rag_search_topk", lang), 1, 10, 5)
        with col_search2:
            threshold = st.slider(get_text("rag_search_threshold", lang), 0.0, 1.0, 0.5, 0.05)

        if st.button(get_text("rag_search_button", lang)):
            if search_query:
                with st.spinner(get_text("rag_searching", lang)):
                    try:
                        response = requests.post(
                            f"{MCP_SERVER_URL}/rag/search",
                            json={
                                "query": search_query,
                                "top_k": top_k,
                                "similarity_threshold": threshold
                            },
                            timeout=15
                        )

                        if response.status_code == 200:
                            results = response.json()

                            st.subheader(get_text("rag_search_results", lang))

                            if results.get('count', 0) > 0:
                                for i, result in enumerate(results['results'], 1):
                                    with st.expander(f"{i}. {result['title']} - {get_text('rag_result_score', lang)}: {result['score']:.3f}"):
                                        st.write(f"**{get_text('rag_doc_id', lang)}:** {result['doc_id']}")
                                        st.write(f"**{get_text('rag_result_content', lang)}:**")
                                        st.write(result['content'])
                            else:
                                st.info(get_text("rag_no_results", lang))
                        else:
                            st.error(f"Search failed: {response.text}")
                    except Exception as e:
                        st.error(f"Search error: {str(e)}")

    with col2:
        # Document List Section
        st.subheader(get_text("rag_docs_section", lang))

        try:
            response = requests.get(f"{MCP_SERVER_URL}/rag/documents?limit=10", timeout=5)

            if response.status_code == 200:
                docs_data = response.json()
                total = docs_data.get('total', 0)
                st.caption(get_text("rag_docs_total", lang).format(total=total))

                for doc in docs_data.get('documents', []):
                    with st.expander(f"ğŸ“„ {doc['title']}"):
                        st.write(f"**{get_text('rag_doc_id', lang)}:** {doc['id']}")
                        st.write(f"**{get_text('rag_doc_category', lang)}:** {doc.get('category', 'N/A')}")
                        st.write(f"**{get_text('rag_doc_created', lang)}:** {doc.get('created_at', 'N/A')}")

                        if st.button(f"ğŸ—‘ï¸ {get_text('rag_doc_delete', lang)}", key=f"delete_{doc['id']}"):
                            try:
                                del_response = requests.delete(
                                    f"{MCP_SERVER_URL}/rag/documents/{doc['id']}",
                                    timeout=5
                                )
                                if del_response.status_code == 200:
                                    st.success("Document deleted!")
                                    st.rerun()
                                else:
                                    st.error(f"Delete failed: {del_response.text}")
                            except Exception as e:
                                st.error(f"Delete error: {str(e)}")
            else:
                st.error(f"Failed to load documents: {response.text}")
        except Exception as e:
            st.error(f"Error loading documents: {str(e)}")

        st.divider()

        # Stats Section
        st.subheader(get_text("rag_stats_section", lang))

        try:
            with st.spinner(get_text("rag_stats_loading", lang)):
                response = requests.get(f"{MCP_SERVER_URL}/rag/stats", timeout=5)

                if response.status_code == 200:
                    stats = response.json()

                    col_stat1, col_stat2 = st.columns(2)
                    with col_stat1:
                        st.metric(
                            get_text("rag_stats_total_docs", lang),
                            stats.get('documents', {}).get('total', 0)
                        )
                    with col_stat2:
                        st.metric(
                            get_text("rag_stats_total_vectors", lang),
                            stats.get('vectors', {}).get('points_count', 0)
                        )

                    st.caption(f"{get_text('rag_stats_collection', lang)}: {stats.get('vectors', {}).get('collection_name', 'N/A')}")
                else:
                    st.error(f"Failed to load stats: {response.text}")
        except Exception as e:
            st.error(f"Error loading stats: {str(e)}")

with tab7:
    st.header("ğŸ“š Project Documentation")
    st.caption("Complete documentation for the AI Platform")

    # Documentation navigation
    doc_sections = {
        "ğŸ“– Quick Start": "README.md",
        "ğŸ—„ï¸ Database Schema": "DATABASE_SCHEMA.md",
        "ğŸ”§ Troubleshooting Guide": "TROUBLESHOOTING_GUIDE.md",
        "ğŸ§  Context-Aware Agent": "CONTEXT_AWARE_AGENT_GUIDE.md",
        "ğŸ“§ SMTP Configuration": "SMTP_CONFIGURATION_GUIDE.md",
        "ğŸ“± LINE Messaging Setup": "LINE_SETUP_GUIDE.md",
        "âœ… Test Results": "TEST_RESULTS.md",
        "ğŸš€ Deployment Guide": "DEPLOYMENT_GUIDE.md",
        "ğŸ“ Changelog": "CHANGELOG.md",
        "ğŸ“Š Project Summary": "PROJECT_SUMMARY.md"
    }

    # Create columns for documentation cards
    cols = st.columns(2)

    for idx, (title, filename) in enumerate(doc_sections.items()):
        with cols[idx % 2]:
            with st.container():
                st.subheader(title)

                # Read documentation file
                doc_path = f"/app/{filename}"  # In Docker container
                try:
                    if os.path.exists(doc_path):
                        with open(doc_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # Show preview
                        preview = content[:200] + "..." if len(content) > 200 else content
                        st.text(preview)

                        # View button
                        if st.button(f"ğŸ“„ View {title}", key=f"view_{filename}"):
                            st.session_state['current_doc'] = filename
                            st.session_state['current_doc_title'] = title
                    else:
                        st.warning(f"Document not found: {filename}")
                except Exception as e:
                    st.error(f"Error loading {filename}: {str(e)}")

    st.divider()

    # Display selected document
    if 'current_doc' in st.session_state:
        doc_file = st.session_state['current_doc']
        doc_title = st.session_state.get('current_doc_title', doc_file)

        st.markdown(f"### ğŸ“– {doc_title}")

        # Back button
        if st.button("â¬…ï¸ Back to Documentation List"):
            del st.session_state['current_doc']
            del st.session_state['current_doc_title']
            st.rerun()

        # Read and display full document
        doc_path = f"/app/{doc_file}"
        try:
            if os.path.exists(doc_path):
                with open(doc_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Display markdown content
                st.markdown(content)

                # Download button
                st.download_button(
                    label=f"â¬‡ï¸ Download {doc_file}",
                    data=content,
                    file_name=doc_file,
                    mime="text/markdown"
                )
            else:
                st.error(f"Document not found: {doc_file}")
        except Exception as e:
            st.error(f"Error reading document: {str(e)}")
    else:
        # Show quick links when no document is selected
        st.markdown("""
        ### Quick Links

        - **Getting Started**: View README.md for quickstart guide
        - **Troubleshooting**: TROUBLESHOOTING_GUIDE.md - Complete problem-solving guide
        - **Context-Aware Agent**: CONTEXT_AWARE_AGENT_GUIDE.md - Natural language understanding
        - **Email Setup**: SMTP_CONFIGURATION_GUIDE.md - Configure real email sending
        - **LINE Messaging**: LINE_SETUP_GUIDE.md - Smart group/personal messaging with auto-detection
        - **Database**: See DATABASE_SCHEMA.md for schema details
        - **Testing**: Check TEST_RESULTS.md for 100% test coverage
        - **Deployment**: Follow DEPLOYMENT_GUIDE.md for production setup
        - **Changes**: Review CHANGELOG.md for version history
        - **Overview**: Read PROJECT_SUMMARY.md for executive summary

        ### External Documentation

        - [LiteLLM Docs](https://docs.litellm.ai/)
        - [FastAPI Docs](https://fastapi.tiangolo.com/)
        - [Streamlit Docs](https://docs.streamlit.io/)
        - [Qdrant Docs](https://qdrant.tech/documentation/)
        - [PostgreSQL Docs](https://www.postgresql.org/docs/)
        """)

        # Tools reference
        st.divider()
        st.subheader("ğŸ› ï¸ Available Tools (28 Total)")

        tool_categories = {
            "Data Analysis & Processing (3)": [
                "analyze_data - Statistical analysis",
                "process_csv - CSV file processing",
                "generate_chart - Data visualization"
            ],
            "Search & Retrieval (3)": [
                "semantic_search - AI-driven search",
                "web_search - Web search integration",
                "find_similar_documents - Document similarity"
            ],
            "Content Generation (3)": [
                "summarize_document - Text summarization",
                "translate_text - Multi-language translation",
                "generate_report - Report generation"
            ],
            "Security & Compliance (3)": [
                "check_permissions - Access control",
                "audit_log - Audit logging",
                "scan_sensitive_data - PII detection"
            ],
            "Business Process (3)": [
                "create_task - Task management",
                "send_notification - Notifications",
                "schedule_meeting - Meeting scheduling"
            ],
            "System Integration (3)": [
                "call_api - External API calls",
                "execute_sql - SQL queries",
                "run_script - Script execution"
            ],
            "Communication (2)": [
                "send_email - Email sending",
                "create_slack_message - Slack integration"
            ],
            "File Management (3)": [
                "upload_file - File uploads",
                "download_file - File downloads",
                "list_files - File listing"
            ],
            "Calculation (2)": [
                "calculate_metrics - Business KPIs",
                "financial_calculator - ROI/NPV/IRR"
            ]
        }

        for category, tools in tool_categories.items():
            with st.expander(f"ğŸ“‚ {category}"):
                for tool in tools:
                    st.markdown(f"- `{tool}`")

with tab7:
    st.header(get_text("about_header", lang))

    st.markdown(f"""
    ### {get_text("about_title", lang)}

    {get_text("about_intro", lang)}

    #### {get_text("core_features", lang)}
    - ğŸ”„ {get_text("feature_hybrid", lang)}
    - ğŸ¤– {get_text("feature_agent", lang)}
    - ğŸ“Š {get_text("feature_monitor", lang)}
    - ğŸ”’ {get_text("feature_security", lang)}

    #### {get_text("tech_stack", lang)}
    - {get_text("frontend", lang)}
    - {get_text("backend", lang)}
    - {get_text("llm_gateway", lang)}
    - {get_text("local_inference", lang)}
    - {get_text("database", lang)}
    - {get_text("monitoring", lang)}

    #### {get_text("supported_models", lang)}
    - {get_text("models_openai", lang)}
    - {get_text("models_anthropic", lang)}
    - {get_text("models_local", lang)}
    - {get_text("models_others", lang)}

    #### {get_text("usage_tips", lang)}
    1. {get_text("tip_1", lang)}
    2. {get_text("tip_2", lang)}
    3. {get_text("tip_3", lang)}

    #### {get_text("version_info", lang)}
    - {get_text("version", lang)}
    - {get_text("update_date", lang)}
    - {get_text("license", lang)}
    """)

    st.divider()

    st.markdown(f"""
    ### {get_text("tech_support", lang)}

    {get_text("support_intro", lang)}
    - {get_text("support_docs", lang)}
    - {get_text("support_troubleshoot", lang)}
    - {get_text("support_logs", lang)}
    """)
